---
phase: 05-run-analytics
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/core/src/datasets/run/analytics/types.ts
  - packages/core/src/datasets/run/analytics/aggregate.ts
  - packages/core/src/datasets/run/analytics/compare.ts
  - packages/core/src/datasets/run/analytics/index.ts
  - packages/core/src/datasets/run/index.ts
  - packages/core/src/datasets/index.ts
autonomous: true

must_haves:
  truths:
    - "User can compare two runs and see score deltas per scorer"
    - "Comparison returns hasRegression flag for CI quick check"
    - "Comparison proceeds with overlapping items when versions differ, with versionMismatch warning in result"
    - "Stats include error rate, pass rate, avg score per scorer"
  deferred:
    - "ANAL-03 (latency distribution) deferred per CONTEXT.md — latency stored but percentiles not computed for v1"
    - "Scorer-as-target runs (targetType: 'scorer') use same comparison logic — scorer output is the 'output' field, scores still recorded normally"
  artifacts:
    - path: "packages/core/src/datasets/run/analytics/types.ts"
      provides: "ComparisonResult, ScorerComparison, ScorerStats, ItemComparison types"
      exports: ["ComparisonResult", "ScorerComparison", "ScorerStats", "ItemComparison"]
    - path: "packages/core/src/datasets/run/analytics/compare.ts"
      provides: "compareRuns function for explicit pair comparison"
      exports: ["compareRuns"]
    - path: "packages/core/src/datasets/run/analytics/aggregate.ts"
      provides: "Aggregation helpers for stats computation"
      exports: ["computeScorerStats", "isRegression"]
  key_links:
    - from: "packages/core/src/datasets/run/analytics/compare.ts"
      to: "RunsStorage.listResults, ScoresStorage.listScoresByRunId"
      via: "storage.getStore() calls"
      pattern: "getStore\\('runs'\\)|getStore\\('scores'\\)"
    - from: "packages/core/src/datasets/index.ts"
      to: "packages/core/src/datasets/run/analytics/index.ts"
      via: "export * from './run/analytics'"
      pattern: "export.*from.*analytics"
---

<objective>
Implement run comparison and analytics for regression detection.

Purpose: Users need to compare two runs side-by-side with score deltas to detect quality regressions when prompts or models change.

Output: `compareRuns()` function returning nested-by-scorer structure with regression flags, plus aggregation helpers for computing error rate, pass rate, and avg score.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase context (locked decisions)
@.planning/phases/05-run-analytics/05-CONTEXT.md
@.planning/phases/05-run-analytics/05-RESEARCH.md

# Existing types and patterns
@packages/core/src/storage/types.ts (Run, RunResult, ListRunResultsOutput)
@packages/core/src/storage/domains/runs/base.ts (RunsStorage.listResults)
@packages/core/src/storage/domains/scores/base.ts (ScoresStorage.listScoresByRunId)
@packages/core/src/evals/types.ts (ScoreRowData)
@packages/core/src/datasets/run/index.ts (existing runDataset structure)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create analytics types</name>
  <files>packages/core/src/datasets/run/analytics/types.ts</files>
  <action>
    Create TypeScript types for comparison and aggregation results.

    From CONTEXT.md decisions:
    - Nested-by-scorer structure for scalability
    - Three metrics per scorer: error rate, pass rate, avg score
    - Regression flags at both levels (top-level hasRegression, per-scorer regressed)

    Types to create:

    1. **ScorerStats** - Per-scorer aggregation
       - errorRate: number (items with errors / total)
       - errorCount: number
       - passRate: number (items >= threshold / items with scores)
       - passCount: number
       - avgScore: number (mean of scores, errors excluded)
       - scoreCount: number
       - totalItems: number

    2. **ScorerComparison** - Per-scorer comparison
       - statsA: ScorerStats
       - statsB: ScorerStats
       - delta: number (avgB - avgA)
       - regressed: boolean (delta below threshold)
       - threshold: number

    3. **ItemComparison** - Per-item detail
       - itemId: string
       - inBothRuns: boolean
       - scoresA: Record<scorerId, number | null>
       - scoresB: Record<scorerId, number | null>

    4. **ComparisonResult** - Top-level return
       - runA: { id: string; datasetVersion: Date }
       - runB: { id: string; datasetVersion: Date }
       - versionMismatch: boolean
       - hasRegression: boolean (any scorer regressed)
       - scorers: Record<string, ScorerComparison>
       - items: ItemComparison[]
       - warnings: string[] (e.g., "Runs have different dataset versions")

    5. **CompareRunsConfig** - Function input
       - runIdA: string
       - runIdB: string
       - thresholds?: Record<scorerId, { value: number; direction?: 'higher-is-better' | 'lower-is-better' }>

    Note on thresholds: For v1, thresholds are runtime-only (passed via config).
    Dataset-level threshold storage deferred — can add later as convenience layer.
    Default threshold when not specified: 0 (any negative delta = regression for higher-is-better).
  </action>
  <verify>pnpm typecheck (no errors in packages/core)</verify>
  <done>All analytics types exported from types.ts</done>
</task>

<task type="auto">
  <name>Task 2: Create aggregation helpers</name>
  <files>packages/core/src/datasets/run/analytics/aggregate.ts</files>
  <action>
    Create pure functions for computing stats from raw data.

    From CONTEXT.md:
    - Basic stats only: mean score, count (no percentiles)
    - Filter nulls before computing avgScore
    - Track error count separately

    Functions to create:

    1. **computeMean(values: number[]): number**
       - Return 0 for empty array
       - Standard mean calculation

    2. **computeScorerStats(scores: ScoreRowData[], passThreshold: number): ScorerStats**
       - `passThreshold` is absolute: a score of 0.7 with threshold 0.7 passes (score >= passThreshold)
       - Filter out null scores for avgScore
       - errorCount = scores where score is null
       - errorRate = errorCount / totalItems
       - passCount = scores >= passThreshold
       - passRate = passCount / (totalItems - errorCount)
       - avgScore = mean of non-null scores
       - Default passThreshold when not provided: 0.5 (50% pass mark, common for binary scorers)

    3. **isRegression(delta: number, threshold: number, direction: 'higher-is-better' | 'lower-is-better'): boolean**
       - higher-is-better: negative delta is regression (delta < -threshold)
       - lower-is-better: positive delta is regression (delta > threshold)
       - Default direction: 'higher-is-better'

    All functions should be pure (no side effects) and well-typed.
  </action>
  <verify>pnpm typecheck (no errors in packages/core)</verify>
  <done>Aggregation helpers exported from aggregate.ts</done>
</task>

<task type="auto">
  <name>Task 3A: Create compareRuns function</name>
  <files>packages/core/src/datasets/run/analytics/compare.ts</files>
  <action>
    Implement the main comparison function.

    From CONTEXT.md:
    - Explicit pair comparison: user provides both run IDs
    - When versions differ: warn and proceed, compare only overlapping items
    - Return both item-level diffs AND aggregate summary

    **Signature (follows runDataset pattern):**

    ```typescript
    import { Mastra } from '../../../mastra';

    export async function compareRuns(
      mastra: Mastra,
      config: CompareRunsConfig,
    ): Promise<ComparisonResult>
    ```

    Implementation steps:
    1. Get storage via `mastra.getStorage()`
    2. Get runs store via `storage.getStore('runs')`
    3. Get scores store via `storage.getStore('scores')`
    4. Load both runs via runsStore.getRunById
    5. Handle edge cases:
       - Either run not found -> throw Error with clear message
       - Empty runs -> return comparison with empty arrays + warning
    6. Load results for both runs via runsStore.listResults (perPage: false for all)
    7. Load scores for both runs via scoresStore.listScoresByRunId (perPage: false)
    8. Check version mismatch: runA.datasetVersion !== runB.datasetVersion
    9. Find overlapping items by itemId (present in both runs)
    10. Group scores by scorerId and itemId for both runs
    11. For each unique scorer found:
        - Get threshold from config.thresholds[scorerId] or use default { value: 0, direction: 'higher-is-better' }
        - Compute ScorerStats for each run using computeScorerStats
        - Compute delta = statsB.avgScore - statsA.avgScore
        - Check regression using isRegression with threshold
        - Build ScorerComparison
    12. Build ItemComparison array for overlapping items
    13. Set hasRegression = any scorer has regressed
    14. Build warnings array (version mismatch, no overlapping items, etc.)
    15. Return ComparisonResult
  </action>
  <verify>pnpm typecheck (no errors in packages/core)</verify>
  <done>compareRuns function implemented in compare.ts</done>
</task>

<task type="auto">
  <name>Task 3B: Wire exports and verify build</name>
  <files>packages/core/src/datasets/run/analytics/index.ts, packages/core/src/datasets/run/index.ts, packages/core/src/datasets/index.ts</files>
  <action>
    Wire up exports so compareRuns is accessible from package root.

    **analytics/index.ts:**
    ```typescript
    export * from './types';
    export * from './aggregate';
    export * from './compare';
    ```

    **run/index.ts:**
    Add export at bottom:
    ```typescript
    export * from './analytics';
    ```

    **datasets/index.ts:**
    Verify run/index.ts already exports analytics (chain complete).
    No changes needed if run/index.ts is already exported.

    **Build verification:**
    Run `pnpm build:core` to ensure exports compile correctly.
  </action>
  <verify>pnpm typecheck && pnpm build:core</verify>
  <done>compareRuns exported from packages/core/src/datasets, build passes</done>
</task>

</tasks>

<verification>
Run these commands to verify the phase is complete:

```bash
# Type check passes
pnpm typecheck

# Core package builds
pnpm build:core

# Verify exports (grep for key exports)
grep -r "compareRuns" packages/core/src/datasets/
grep -r "ComparisonResult" packages/core/src/datasets/
```
</verification>

<success_criteria>
1. Types compile without errors
2. compareRuns function exported from packages/core/src/datasets
3. No storage schema changes (pure computation layer)
4. All CONTEXT.md decisions implemented:
   - Nested-by-scorer structure
   - Three metrics per scorer (error rate, pass rate, avg score)
   - hasRegression flag at top level
   - versionMismatch warning when versions differ
</success_criteria>

<output>
After completion, create `.planning/phases/05-run-analytics/05-01-SUMMARY.md`
</output>
