---
phase: 04-scorer-targets
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - packages/core/src/storage/types.ts
  - packages/core/src/datasets/run/executor.ts
  - packages/core/src/datasets/run/__tests__/executor.test.ts
  - packages/core/src/datasets/run/__tests__/runDataset.test.ts
autonomous: true

must_haves:
  truths:
    - "User can run dataset against scorer to test scoring logic"
    - "Scorer receives item.input, item.output, item.expectedOutput mapped to scorer.run()"
    - "Scorer result (score/reason) stored in ItemResult.output"
    - "Invalid score (NaN, wrong type) becomes null with console.warn"
    - "Scorer error caught and stored, run continues"
  artifacts:
    - path: "packages/core/src/storage/types.ts"
      provides: "DatasetItem.output field"
      contains: "output?: unknown"
    - path: "packages/core/src/datasets/run/executor.ts"
      provides: "executeScorer function"
      exports: ["executeTarget"]
      contains: "executeScorer"
    - path: "packages/core/src/datasets/run/__tests__/executor.test.ts"
      provides: "Scorer target unit tests"
      contains: "scorer target"
  key_links:
    - from: "packages/core/src/datasets/run/executor.ts"
      to: "scorer.run"
      via: "executeScorer calls scorer.run({ input, output, groundTruth })"
      pattern: "scorer\\.run\\("
    - from: "packages/core/src/datasets/run/executor.ts"
      to: "case 'scorer'"
      via: "executeTarget switch statement"
      pattern: "case 'scorer'"
---

<objective>
Implement scorer-as-target execution for LLM-as-judge calibration testing.

Purpose: Enable users to run datasets against scorers to verify scoring logic alignment with human-labeled ground truth. This is critical for calibrating LLM-as-judge evaluation before production use.

Output: Working executeScorer function with full test coverage, DatasetItem type updated with output field.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-scorer-targets/04-CONTEXT.md
@.planning/phases/04-scorer-targets/04-RESEARCH.md
@packages/core/src/storage/types.ts
@packages/core/src/datasets/run/executor.ts
@packages/core/src/datasets/run/__tests__/executor.test.ts
@packages/core/src/datasets/run/scorer.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add output field to DatasetItem type</name>
  <files>packages/core/src/storage/types.ts</files>
  <action>
Add `output?: unknown` field to DatasetItem interface after the `input` field:

```typescript
export interface DatasetItem {
  id: string;
  datasetId: string;
  version: Date;
  input: unknown;
  output?: unknown;  // NEW: The response being evaluated (for scorer targets)
  expectedOutput?: unknown;
  context?: Record<string, unknown>;
  createdAt: Date;
  updatedAt: Date;
}
```

Also add to AddDatasetItemInput and UpdateDatasetItemInput interfaces:
- AddDatasetItemInput: add `output?: unknown` after `input`
- UpdateDatasetItemInput: add `output?: unknown` after `input`

No storage schema changes needed - inmemory stores objects directly, and the field is optional.
  </action>
  <verify>TypeScript compiles: `cd packages/core && pnpm tsc --noEmit`</verify>
  <done>DatasetItem, AddDatasetItemInput, UpdateDatasetItemInput all have optional output field</done>
</task>

<task type="auto">
  <name>Task 2: TDD executeScorer implementation</name>
  <files>
    packages/core/src/datasets/run/__tests__/executor.test.ts
    packages/core/src/datasets/run/executor.ts
    packages/core/src/datasets/run/__tests__/runDataset.test.ts
  </files>
  <action>
Follow RED-GREEN-REFACTOR cycle:

**RED - Write failing tests first** (in executor.test.ts):

Add new describe block for scorer target tests:

```typescript
describe('scorer target', () => {
  // Helper to create mock scorer
  const createMockScorer = (score: number, reason?: string, shouldFail = false) => ({
    id: 'test-scorer',
    name: 'Test Scorer',
    run: vi.fn().mockImplementation(async () => {
      if (shouldFail) throw new Error('Scorer error');
      return { score, reason };
    }),
  });

  it('calls scorer.run with input, output, groundTruth from item', async () => {
    const mockScorer = createMockScorer(0.85, 'Good answer');
    const result = await executeTarget(mockScorer as any, 'scorer', {
      id: 'item-1',
      datasetId: 'ds-1',
      input: { question: 'What is 2+2?' },
      output: { response: '4' },
      expectedOutput: { score: 1.0, label: 'correct' },
      version: new Date(),
      createdAt: new Date(),
      updatedAt: new Date(),
    });

    expect(mockScorer.run).toHaveBeenCalledWith({
      input: { question: 'What is 2+2?' },
      output: { response: '4' },
      groundTruth: { score: 1.0, label: 'correct' },
    });
    expect(result.output).toEqual({ score: 0.85, reason: 'Good answer' });
    expect(result.error).toBeNull();
  });

  it('returns null score and warns on NaN score', async () => {
    const consoleSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});
    const mockScorer = createMockScorer(NaN, 'Invalid');

    const result = await executeTarget(mockScorer as any, 'scorer', {
      id: 'item-2',
      datasetId: 'ds-1',
      input: 'test',
      output: 'response',
      expectedOutput: null,
      version: new Date(),
      createdAt: new Date(),
      updatedAt: new Date(),
    });

    expect(result.output).toEqual({ score: null, reason: 'Invalid' });
    expect(result.error).toBeNull();
    expect(consoleSpy).toHaveBeenCalledWith(expect.stringContaining('invalid score'));
    consoleSpy.mockRestore();
  });

  it('returns null score and warns on non-number score', async () => {
    const consoleSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});
    const mockScorer = {
      id: 'test-scorer',
      name: 'Test Scorer',
      run: vi.fn().mockResolvedValue({ score: 'not-a-number', reason: 'Bad type' }),
    };

    const result = await executeTarget(mockScorer as any, 'scorer', {
      id: 'item-3',
      datasetId: 'ds-1',
      input: 'test',
      output: 'response',
      expectedOutput: null,
      version: new Date(),
      createdAt: new Date(),
      updatedAt: new Date(),
    });

    expect(result.output).toEqual({ score: null, reason: 'Bad type' });
    expect(consoleSpy).toHaveBeenCalled();
    consoleSpy.mockRestore();
  });

  it('captures error when scorer throws', async () => {
    const mockScorer = createMockScorer(0, '', true);

    const result = await executeTarget(mockScorer as any, 'scorer', {
      id: 'item-4',
      datasetId: 'ds-1',
      input: 'test',
      output: 'response',
      expectedOutput: null,
      version: new Date(),
      createdAt: new Date(),
      updatedAt: new Date(),
    });

    expect(result.output).toBeNull();
    expect(result.error).toBe('Scorer error');
  });

  it('handles missing item.output (passes undefined to scorer)', async () => {
    const mockScorer = createMockScorer(0.5);

    const result = await executeTarget(mockScorer as any, 'scorer', {
      id: 'item-5',
      datasetId: 'ds-1',
      input: 'test',
      // No output field
      expectedOutput: null,
      version: new Date(),
      createdAt: new Date(),
      updatedAt: new Date(),
    });

    expect(mockScorer.run).toHaveBeenCalledWith({
      input: 'test',
      output: undefined,
      groundTruth: null,
    });
    expect(result.output).toEqual({ score: 0.5, reason: null });
  });

  it('handles null reason in scorer result', async () => {
    const mockScorer = {
      id: 'test-scorer',
      name: 'Test Scorer',
      run: vi.fn().mockResolvedValue({ score: 0.7, reason: null }),
    };

    const result = await executeTarget(mockScorer as any, 'scorer', {
      id: 'item-6',
      datasetId: 'ds-1',
      input: 'test',
      output: 'response',
      expectedOutput: null,
      version: new Date(),
      createdAt: new Date(),
      updatedAt: new Date(),
    });

    expect(result.output).toEqual({ score: 0.7, reason: null });
  });
});
```

Run tests - they MUST fail (scorer case throws "not yet supported").

**GREEN - Implement executeScorer** (in executor.ts):

Add executeScorer function before executeTarget:

```typescript
/**
 * Execute a dataset item against a scorer (LLM-as-judge calibration).
 * Maps item fields to scorer.run({ input, output, groundTruth }).
 */
async function executeScorer(
  scorer: MastraScorer<any, any, any, any>,
  item: DatasetItem,
): Promise<ExecutionResult> {
  try {
    const result = await scorer.run({
      input: item.input,
      output: item.output,
      groundTruth: item.expectedOutput,
    });

    // Validate score is a number
    const score = typeof result.score === 'number' && !isNaN(result.score)
      ? result.score
      : null;

    if (score === null && result.score !== undefined) {
      console.warn(`Scorer ${scorer.id} returned invalid score: ${result.score}`);
    }

    return {
      output: {
        score,
        reason: typeof result.reason === 'string' ? result.reason : null,
      },
      error: null,
    };
  } catch (error) {
    return {
      output: null,
      error: error instanceof Error ? error.message : String(error),
    };
  }
}
```

Update executeTarget switch to call executeScorer:

```typescript
case 'scorer':
  return await executeScorer(target as MastraScorer<any, any, any, any>, item);
```

Remove 'processor' from the scorer case (keep it throwing "not yet supported").

Run tests - they MUST pass.

**Add integration test** (in runDataset.test.ts):

Add test for scorer target in runDataset:

```typescript
it('executes scorer target and applies meta-scorers', async () => {
  // Create dataset and item with output field
  const dataset = await datasetsStorage.createDataset({ name: 'Scorer Test' });
  const item = await datasetsStorage.addItem({
    datasetId: dataset.id,
    input: { question: 'What is AI?' },
    output: { response: 'AI is artificial intelligence.' },
    expectedOutput: { score: 1.0, label: 'good' },
  });

  // Mock scorer as target
  const mockScorer = {
    id: 'target-scorer',
    name: 'Target Scorer',
    run: vi.fn().mockResolvedValue({ score: 0.9, reason: 'Accurate' }),
  };

  const mockMastra = {
    getStorage: vi.fn().mockReturnValue(storage),
    getScorerById: vi.fn().mockImplementation((id: string) => {
      if (id === 'target-scorer') return mockScorer;
      if (id === 'meta-scorer') return {
        id: 'meta-scorer',
        name: 'Meta Scorer',
        run: vi.fn().mockResolvedValue({ score: 0.95, reason: 'Good calibration' }),
      };
      return null;
    }),
    getAgentById: vi.fn().mockReturnValue(null),
    getWorkflowById: vi.fn().mockReturnValue(null),
  } as unknown as Mastra;

  const runResult = await runDataset(mockMastra, {
    datasetId: dataset.id,
    targetId: 'target-scorer',
    targetType: 'scorer',
    scorerIds: ['meta-scorer'],
  });

  expect(runResult.status).toBe('completed');
  expect(runResult.results).toHaveLength(1);
  expect(runResult.results[0].output).toEqual({ score: 0.9, reason: 'Accurate' });
  expect(mockScorer.run).toHaveBeenCalledWith({
    input: { question: 'What is AI?' },
    output: { response: 'AI is artificial intelligence.' },
    groundTruth: { score: 1.0, label: 'good' },
  });
});
```

Run all tests: `pnpm test --filter @mastra/core -- --run`
  </action>
  <verify>
All tests pass:
```bash
cd packages/core && pnpm test -- --run --testPathPattern="executor|runDataset"
```
  </verify>
  <done>
- executeScorer function implemented following existing pattern
- 6 unit tests covering: basic success, NaN score, non-number score, scorer error, missing output, null reason
- 1 integration test verifying runDataset with scorer target and meta-scorers
- Scorer case enabled in executeTarget switch
  </done>
</task>

</tasks>

<verification>
Run full test suite for datasets module:
```bash
cd packages/core && pnpm test -- --run --testPathPattern="datasets"
```

TypeScript compiles without errors:
```bash
cd packages/core && pnpm tsc --noEmit
```

Verify scorer target works end-to-end:
- DatasetItem accepts output field
- executeTarget routes scorer to executeScorer
- executeScorer calls scorer.run with correct arguments
- Invalid scores handled with null + warning
- Errors captured and stored
</verification>

<success_criteria>
1. DatasetItem, AddDatasetItemInput, UpdateDatasetItemInput have optional output field
2. executeScorer function exists and handles all edge cases
3. executeTarget switch routes 'scorer' to executeScorer
4. 6+ scorer unit tests pass
5. 1+ scorer integration test passes
6. TypeScript compiles without errors
7. Existing agent/workflow tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-scorer-targets/04-01-SUMMARY.md` following the standard summary template.
</output>
