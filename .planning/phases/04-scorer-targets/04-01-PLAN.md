---
phase: 04-scorer-targets
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - packages/core/src/datasets/run/executor.ts
  - packages/core/src/datasets/run/__tests__/executor.test.ts
  - packages/core/src/datasets/run/__tests__/runDataset.test.ts
autonomous: true

must_haves:
  truths:
    - 'User can run dataset against scorer to test scoring logic'
    - "Scorer receives item.input directly (user provides scorer's expected input shape)"
    - 'Scorer result (score/reason) stored in ItemResult.output'
    - 'Invalid score (NaN, wrong type) becomes null with console.warn'
    - 'Scorer error caught and stored, run continues'
  artifacts:
    - path: 'packages/core/src/datasets/run/executor.ts'
      provides: 'executeScorer function'
      exports: ['executeTarget']
      contains: 'executeScorer'
    - path: 'packages/core/src/datasets/run/__tests__/executor.test.ts'
      provides: 'Scorer target unit tests'
      contains: 'scorer target'
  key_links:
    - from: 'packages/core/src/datasets/run/executor.ts'
      to: 'scorer.run'
      via: 'executeScorer calls scorer.run(item.input)'
      pattern: "scorer\\.run\\("
    - from: 'packages/core/src/datasets/run/executor.ts'
      to: "case 'scorer'"
      via: 'executeTarget switch statement'
      pattern: "case 'scorer'"
---

<objective>
Implement scorer-as-target execution for LLM-as-judge calibration testing.

Purpose: Enable users to run datasets against scorers to verify scoring logic alignment with human-labeled ground truth. This is critical for calibrating LLM-as-judge evaluation before production use.

Output: Working executeScorer function with full test coverage.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-scorer-targets/04-CONTEXT.md
@.planning/phases/04-scorer-targets/04-RESEARCH.md
@packages/core/src/datasets/run/executor.ts
@packages/core/src/datasets/run/__tests__/executor.test.ts
@packages/core/src/datasets/run/scorer.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: TDD executeScorer implementation</name>
  <files>
    packages/core/src/datasets/run/__tests__/executor.test.ts
    packages/core/src/datasets/run/executor.ts
    packages/core/src/datasets/run/__tests__/runDataset.test.ts
  </files>
  <action>
Follow RED-GREEN-REFACTOR cycle.

**Design decision:** item.input contains exactly what the scorer expects. No field mapping.

- For scorer calibration: `item.input = { input, output, groundTruth }` (user structures it)
- `item.expectedOutput` holds human label for alignment comparison (Phase 5 analytics)
- `executeScorer` just calls `scorer.run(item.input)` directly

**RED - Write failing tests first** (in executor.test.ts):

Add new describe block for scorer target tests:

```typescript
describe('scorer target', () => {
  // Helper to create mock scorer
  const createMockScorer = (score: number, reason?: string, shouldFail = false) => ({
    id: 'test-scorer',
    name: 'Test Scorer',
    run: vi.fn().mockImplementation(async () => {
      if (shouldFail) throw new Error('Scorer error');
      return { score, reason };
    }),
  });

  it('calls scorer.run with item.input directly', async () => {
    const mockScorer = createMockScorer(0.85, 'Good answer');
    const scorerInput = {
      input: { question: 'What is 2+2?' },
      output: { response: '4' },
      groundTruth: { score: 1.0, label: 'correct' },
    };

    const result = await executeTarget(mockScorer as any, 'scorer', {
      id: 'item-1',
      datasetId: 'ds-1',
      input: scorerInput, // Full scorer input in item.input
      expectedOutput: { humanScore: 1.0 }, // Human label for alignment analysis
      version: new Date(),
      createdAt: new Date(),
      updatedAt: new Date(),
    });

    expect(mockScorer.run).toHaveBeenCalledWith(scorerInput);
    expect(result.output).toEqual({ score: 0.85, reason: 'Good answer' });
    expect(result.error).toBeNull();
  });

  it('returns null score and warns on NaN score', async () => {
    const consoleSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});
    const mockScorer = createMockScorer(NaN, 'Invalid');

    const result = await executeTarget(mockScorer as any, 'scorer', {
      id: 'item-2',
      datasetId: 'ds-1',
      input: { output: 'test response' },
      version: new Date(),
      createdAt: new Date(),
      updatedAt: new Date(),
    });

    expect(result.output).toEqual({ score: null, reason: 'Invalid' });
    expect(result.error).toBeNull();
    expect(consoleSpy).toHaveBeenCalledWith(expect.stringContaining('invalid score'));
    consoleSpy.mockRestore();
  });

  it('returns null score and warns on non-number score', async () => {
    const consoleSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});
    const mockScorer = {
      id: 'test-scorer',
      name: 'Test Scorer',
      run: vi.fn().mockResolvedValue({ score: 'not-a-number', reason: 'Bad type' }),
    };

    const result = await executeTarget(mockScorer as any, 'scorer', {
      id: 'item-3',
      datasetId: 'ds-1',
      input: { output: 'test' },
      version: new Date(),
      createdAt: new Date(),
      updatedAt: new Date(),
    });

    expect(result.output).toEqual({ score: null, reason: 'Bad type' });
    expect(consoleSpy).toHaveBeenCalled();
    consoleSpy.mockRestore();
  });

  it('captures error when scorer throws', async () => {
    const mockScorer = createMockScorer(0, '', true);

    const result = await executeTarget(mockScorer as any, 'scorer', {
      id: 'item-4',
      datasetId: 'ds-1',
      input: { output: 'test' },
      version: new Date(),
      createdAt: new Date(),
      updatedAt: new Date(),
    });

    expect(result.output).toBeNull();
    expect(result.error).toBe('Scorer error');
  });

  it('handles null reason in scorer result', async () => {
    const mockScorer = {
      id: 'test-scorer',
      name: 'Test Scorer',
      run: vi.fn().mockResolvedValue({ score: 0.7, reason: null }),
    };

    const result = await executeTarget(mockScorer as any, 'scorer', {
      id: 'item-5',
      datasetId: 'ds-1',
      input: { output: 'response' },
      version: new Date(),
      createdAt: new Date(),
      updatedAt: new Date(),
    });

    expect(result.output).toEqual({ score: 0.7, reason: null });
  });
});
```

Run tests - they MUST fail (scorer case throws "not yet supported").

**GREEN - Implement executeScorer** (in executor.ts):

Add executeScorer function before executeTarget:

```typescript
/**
 * Execute a dataset item against a scorer (LLM-as-judge calibration).
 * item.input should contain exactly what the scorer expects.
 */
async function executeScorer(scorer: MastraScorer<any, any, any, any>, item: DatasetItem): Promise<ExecutionResult> {
  try {
    const result = await scorer.run(item.input);

    // Validate score is a number
    const score = typeof result.score === 'number' && !isNaN(result.score) ? result.score : null;

    if (score === null && result.score !== undefined) {
      console.warn(`Scorer ${scorer.id} returned invalid score: ${result.score}`);
    }

    return {
      output: {
        score,
        reason: typeof result.reason === 'string' ? result.reason : null,
      },
      error: null,
    };
  } catch (error) {
    return {
      output: null,
      error: error instanceof Error ? error.message : String(error),
    };
  }
}
```

Update executeTarget switch to call executeScorer:

```typescript
case 'scorer':
  return await executeScorer(target as MastraScorer<any, any, any, any>, item);
```

Keep 'processor' throwing "not yet supported".

Run tests - they MUST pass.

**Add integration test** (in runDataset.test.ts):

Add test for scorer target in runDataset:

```typescript
it('executes scorer target and applies meta-scorers', async () => {
  // Create dataset with item containing full scorer input
  const dataset = await datasetsStorage.createDataset({ name: 'Scorer Test' });
  const item = await datasetsStorage.addItem({
    datasetId: dataset.id,
    input: {
      input: { question: 'What is AI?' },
      output: { response: 'AI is artificial intelligence.' },
      groundTruth: { label: 'good' },
    },
    expectedOutput: { humanScore: 1.0 }, // Human label for alignment
  });

  // Mock scorer as target
  const mockScorer = {
    id: 'target-scorer',
    name: 'Target Scorer',
    run: vi.fn().mockResolvedValue({ score: 0.9, reason: 'Accurate' }),
  };

  const mockMastra = {
    getStorage: vi.fn().mockReturnValue(storage),
    getScorerById: vi.fn().mockImplementation((id: string) => {
      if (id === 'target-scorer') return mockScorer;
      if (id === 'meta-scorer')
        return {
          id: 'meta-scorer',
          name: 'Meta Scorer',
          run: vi.fn().mockResolvedValue({ score: 0.95, reason: 'Good calibration' }),
        };
      return null;
    }),
    getAgentById: vi.fn().mockReturnValue(null),
    getWorkflowById: vi.fn().mockReturnValue(null),
  } as unknown as Mastra;

  const runResult = await runDataset(mockMastra, {
    datasetId: dataset.id,
    targetId: 'target-scorer',
    targetType: 'scorer',
    scorerIds: ['meta-scorer'],
  });

  expect(runResult.status).toBe('completed');
  expect(runResult.results).toHaveLength(1);
  expect(runResult.results[0].output).toEqual({ score: 0.9, reason: 'Accurate' });
  expect(mockScorer.run).toHaveBeenCalledWith({
    input: { question: 'What is AI?' },
    output: { response: 'AI is artificial intelligence.' },
    groundTruth: { label: 'good' },
  });
});
```

Run all tests: `pnpm test --filter @mastra/core -- --run`
</action>
<verify>
All tests pass:

```bash
cd packages/core && pnpm test -- --run --testPathPattern="executor|runDataset"
```

  </verify>
  <done>
- executeScorer function implemented with direct item.input passthrough
- 5 unit tests covering: basic success, NaN score, non-number score, scorer error, null reason
- 1 integration test verifying runDataset with scorer target and meta-scorers
- Scorer case enabled in executeTarget switch
- No changes to DatasetItem type (item.input already supports any shape)
  </done>
</task>

</tasks>

<verification>
Run full test suite for datasets module:
```bash
cd packages/core && pnpm test -- --run --testPathPattern="datasets"
```

TypeScript compiles without errors:

```bash
cd packages/core && pnpm tsc --noEmit
```

Verify scorer target works end-to-end:

- executeTarget routes scorer to executeScorer
- executeScorer calls scorer.run(item.input) directly
- Invalid scores handled with null + warning
- Errors captured and stored
  </verification>

<success_criteria>

1. executeScorer function exists and handles all edge cases
2. executeTarget switch routes 'scorer' to executeScorer
3. 5+ scorer unit tests pass
4. 1+ scorer integration test passes
5. TypeScript compiles without errors
6. Existing agent/workflow tests still pass
7. No changes to DatasetItem type required
   </success_criteria>

<output>
After completion, create `.planning/phases/04-scorer-targets/04-01-SUMMARY.md` following the standard summary template.
</output>
