---
phase: 03-agent-workflow-targets
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/core/src/datasets/run/__tests__/executor.test.ts
autonomous: true

must_haves:
  truths:
    - "Agent accepts string input and returns full output"
    - "Agent accepts messages array and returns full output"
    - "Workflow success returns output"
    - "Workflow non-success statuses captured as error strings"
    - "Empty input handled gracefully (target decides behavior)"
  artifacts:
    - path: "packages/core/src/datasets/run/__tests__/executor.test.ts"
      provides: "Executor edge case tests"
      min_lines: 100
      contains: "executeTarget"
  key_links:
    - from: "executor.test.ts"
      to: "executor.ts"
      via: "import executeTarget"
      pattern: "import.*executeTarget.*from.*executor"
---

<objective>
Verify Agent and Workflow target execution handles all input variations and edge cases.

Purpose: Phase 2 implemented the executor. RESEARCH.md identified potential gaps: agent input variations, workflow status handling. This plan adds focused tests to verify correct behavior.
Output: New executor.test.ts with comprehensive edge case coverage.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-agent-workflow-targets/03-CONTEXT.md
@.planning/phases/03-agent-workflow-targets/03-RESEARCH.md

@packages/core/src/datasets/run/executor.ts
@packages/core/src/datasets/run/__tests__/runDataset.test.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Agent Execution Tests</name>
  <files>packages/core/src/datasets/run/__tests__/executor.test.ts</files>
  <action>
Create new test file for executor.ts with agent-focused tests.

Test cases to cover:
1. String input - `executeTarget(agent, 'agent', { input: "Hello" })` returns FullOutput
2. Messages array input - `executeTarget(agent, 'agent', { input: [{ role: 'user', content: 'Hi' }] })` returns FullOutput
3. Empty string input - verify target receives it (let agent decide behavior)
4. Agent throws error - error captured as string, output is null
5. Legacy model fallback - mock `isSupportedLanguageModel` returning false, verify generateLegacy called

Use mock pattern from runDataset.test.ts:
```typescript
const createMockAgent = (response: string) => ({
  id: 'test-agent',
  name: 'Test Agent',
  getModel: vi.fn().mockResolvedValue({ specificationVersion: 'v2' }),
  generate: vi.fn().mockImplementation(async (input) => {
    return { text: response, input }; // Include input to verify pass-through
  }),
});
```

For legacy fallback test, mock `specificationVersion: 'v1'` and add `generateLegacy` method.

Import executeTarget directly (not through runDataset) for unit testing:
```typescript
import { executeTarget } from '../executor';
```
  </action>
  <verify>
Run: `cd packages/core && pnpm test executor`
All agent tests pass.
  </verify>
  <done>
5 agent execution tests passing:
- string input
- messages array input
- empty input
- error capture
- legacy fallback
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Workflow Execution Tests</name>
  <files>packages/core/src/datasets/run/__tests__/executor.test.ts</files>
  <action>
Add workflow execution tests to the same file.

Test cases to cover:
1. Success status - `executeTarget(workflow, 'workflow', { input: { data: 'test' } })` returns result
2. Failed status - mock workflow returning `{ status: 'failed', error: { message: 'Failed' } }`, verify error captured
3. Tripwire status - mock `{ status: 'tripwire', tripwire: { reason: 'Limit exceeded' } }`, verify error message
4. Suspended status - mock `{ status: 'suspended' }`, verify error says "not yet supported"
5. Paused status - mock `{ status: 'paused' }`, verify error says "not yet supported"
6. Empty object input - `{ input: {} }` passed through to workflow

Mock workflow pattern:
```typescript
const createMockWorkflow = (result: any) => ({
  id: 'test-workflow',
  name: 'Test Workflow',
  createRun: vi.fn().mockImplementation(async () => ({
    start: vi.fn().mockResolvedValue(result),
  })),
});
```

Verify:
- `disableScorers: true` passed to createRun
- `inputData` receives item.input exactly
  </action>
  <verify>
Run: `cd packages/core && pnpm test executor`
All workflow tests pass.
  </verify>
  <done>
6 workflow execution tests passing:
- success status
- failed status
- tripwire status
- suspended status
- paused status
- empty input
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

```bash
cd packages/core && pnpm test executor
```

Expected: 11 tests passing (5 agent + 6 workflow).

Verify test file structure:
```bash
wc -l packages/core/src/datasets/run/__tests__/executor.test.ts
```
Expected: 100+ lines.
</verification>

<success_criteria>
1. executor.test.ts exists with 11+ tests
2. All tests pass
3. Tests cover input variations from RESEARCH.md gaps
4. No changes to executor.ts needed (verification confirms implementation is correct)
</success_criteria>

<output>
After completion, create `.planning/phases/03-agent-workflow-targets/03-01-SUMMARY.md`
</output>
