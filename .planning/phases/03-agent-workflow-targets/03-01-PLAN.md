---
phase: 03-agent-workflow-targets
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/core/src/datasets/run/__tests__/executor.test.ts
  - packages/core/src/datasets/run/__tests__/runDataset.test.ts
autonomous: true

must_haves:
  truths:
    - 'User can run dataset against workflow by passing workflowId'
    - 'User can run dataset against agent by passing agentId'
    - 'Workflow success/failed/suspended/paused statuses all captured correctly'
    - 'Agent string and messages[] inputs both work'
    - 'v1 limitations documented in tests (context propagation deferred)'
  artifacts:
    - path: 'packages/core/src/datasets/run/__tests__/executor.test.ts'
      provides: 'Executor unit tests for input variations'
      min_lines: 100
      contains: 'executeTarget'
    - path: 'packages/core/src/datasets/run/__tests__/runDataset.test.ts'
      provides: 'Integration tests including workflow path'
      contains: "targetType: 'workflow'"
  key_links:
    - from: 'executor.test.ts'
      to: 'executor.ts'
      via: 'import executeTarget'
      pattern: 'import.*executeTarget.*from.*executor'
    - from: 'runDataset.test.ts'
      to: 'runDataset'
      via: 'workflow integration test'
      pattern: 'targetType.*workflow'
---

<objective>
Verify Agent and Workflow target execution handles all input variations and edge cases.

Purpose: Phase 2 implemented the executor. RESEARCH.md identified potential gaps: agent input variations, workflow status handling. This plan adds focused tests to verify correct behavior.
Output: executor.test.ts with unit tests + runDataset.test.ts with workflow integration test.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-agent-workflow-targets/03-CONTEXT.md
@.planning/phases/03-agent-workflow-targets/03-RESEARCH.md

@packages/core/src/datasets/run/executor.ts
@packages/core/src/datasets/run/**tests**/runDataset.test.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Executor Unit Tests</name>
  <files>packages/core/src/datasets/run/__tests__/executor.test.ts</files>
  <action>
Create new test file for executor.ts with unit tests for input variations.

**Agent Tests (5 cases):**

1. String input - `executeTarget(agent, 'agent', { input: "Hello" })` returns FullOutput
2. Messages array input - `executeTarget(agent, 'agent', { input: [{ role: 'user', content: 'Hi' }] })` returns FullOutput
3. Empty string input - verify target receives it (let agent decide behavior)
4. Agent throws error - error captured as string, output is null
5. Legacy model fallback - mock `isSupportedLanguageModel` returning false, verify generateLegacy called

**Workflow Tests (6 cases):**

1. Success status - `executeTarget(workflow, 'workflow', { input: { data: 'test' } })` returns result
2. Failed status - mock workflow returning `{ status: 'failed', error: { message: 'Failed' } }`, verify error captured
3. Tripwire status - mock `{ status: 'tripwire', tripwire: { reason: 'Limit exceeded' } }`, verify error message
4. Suspended status - mock `{ status: 'suspended' }`, verify error says "not yet supported"
5. Paused status - mock `{ status: 'paused' }`, verify error says "not yet supported"
6. Empty object input - `{ input: {} }` passed through to workflow

Use mock patterns from runDataset.test.ts. Import executeTarget directly for unit testing:

```typescript
import { executeTarget } from '../executor';
```

  </action>
  <verify>
Run: `cd packages/core && pnpm test executor`
All 11 tests pass.
  </verify>
  <done>
11 executor unit tests passing covering agent/workflow input variations.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add Workflow Integration Test</name>
  <files>packages/core/src/datasets/run/__tests__/runDataset.test.ts</files>
  <action>
Add workflow integration test to runDataset.test.ts to verify end-to-end workflow path.

Add new describe block after existing tests:

```typescript
describe('workflow target', () => {
  it('executes dataset items against workflow', async () => {
    const mockWorkflow = {
      id: 'test-workflow',
      name: 'Test Workflow',
      createRun: vi.fn().mockImplementation(async () => ({
        start: vi.fn().mockResolvedValue({
          status: 'success',
          result: { answer: 'Workflow result' },
        }),
      })),
    };

    (mastra.getWorkflow as ReturnType<typeof vi.fn>).mockReturnValue(mockWorkflow);
    (mastra.getWorkflowById as ReturnType<typeof vi.fn>).mockReturnValue(mockWorkflow);

    const result = await runDataset(mastra, {
      datasetId,
      targetType: 'workflow',
      targetId: 'test-workflow',
    });

    expect(result.status).toBe('completed');
    expect(result.succeededCount).toBe(2);
    expect(mockWorkflow.createRun).toHaveBeenCalledTimes(2);
  });
});
```

This verifies Success Criteria #1: "User can run dataset against workflow by passing workflowId"
</action>
<verify>
Run: `cd packages/core && pnpm test runDataset`
New workflow test passes alongside existing tests.
</verify>
<done>
runDataset.test.ts includes workflow integration test verifying end-to-end path.
</done>
</task>

<task type="auto">
  <name>Task 3: Document v1 Context Limitation</name>
  <files>packages/core/src/datasets/run/__tests__/executor.test.ts</files>
  <action>
Add a test documenting that request context is NOT propagated in v1.

Add to executor.test.ts:

```typescript
describe('v1 limitations', () => {
  it('does not pass request context to agent (v1 limitation)', async () => {
    // CONTEXT.md explicitly defers: "Runtime context propagation (auth, headers) â€” add when needed"
    // This test documents the v1 behavior for traceability
    const mockAgent = createMockAgent('Response');

    await executeTarget(mockAgent, 'agent', {
      input: 'Test',
      // Any context field here is NOT passed to generate()
    });

    // Verify generate was called without context parameter
    expect(mockAgent.generate).toHaveBeenCalledWith(
      'Test',
      expect.objectContaining({ scorers: {}, returnScorerData: true }),
    );
    // Verify the options object does NOT have a context field
    const callArgs = mockAgent.generate.mock.calls[0][1];
    expect(callArgs).not.toHaveProperty('context');
  });
});
```

This addresses Success Criteria #4 by documenting it's a v1 limitation per CONTEXT.md deferred items.
</action>
<verify>
Run: `cd packages/core && pnpm test executor`
v1 limitation test passes.
</verify>
<done>
v1 context limitation documented in tests per CONTEXT.md deferral.
</done>
</task>

</tasks>

<verification>
After all tasks complete:

```bash
cd packages/core && pnpm test executor && pnpm test runDataset
```

Expected:

- executor.test.ts: 12 tests passing (5 agent + 6 workflow + 1 v1 limitation)
- runDataset.test.ts: All existing tests + 1 new workflow test passing

Verify test file structure:

```bash
wc -l packages/core/src/datasets/run/__tests__/executor.test.ts
```

Expected: 100+ lines.
</verification>

<success_criteria>

1. executor.test.ts exists with 12+ tests covering input variations
2. runDataset.test.ts includes workflow integration test
3. v1 context limitation is documented in tests
4. All tests pass
5. No changes to executor.ts needed (verification confirms implementation is correct)
   </success_criteria>

<output>
After completion, create `.planning/phases/03-agent-workflow-targets/03-01-SUMMARY.md`
</output>
