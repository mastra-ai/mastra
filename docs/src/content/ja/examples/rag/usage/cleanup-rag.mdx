---
title: "例: 情報密度の最適化 | RAG | Mastra ドキュメント"
description: Mastra における RAG システムの実装例。LLM ベースの処理で情報密度を最適化し、データの重複を除去します。
---

import { GithubLink } from "@/components/github-link";


# 情報密度の最適化

この例では、Mastra、OpenAI の埋め込み、そしてベクトル保存に PGVector を用いて、Retrieval-Augmented Generation（RAG）システムを実装する方法を示します。
このシステムでは、エージェントが初期チャンクを整形して情報密度を高め、重複データを除去します。

## 概要

このシステムは Mastra と OpenAI を用いて RAG を実装し、LLM ベースの処理で情報密度を最適化します。主な処理は次のとおりです:

1. クエリ処理とドキュメントのクレンジングの両方に対応する gpt-4o-mini 搭載の Mastra エージェントをセットアップ
2. エージェントが利用するベクター検索およびドキュメント分割ツールを作成
3. 初期ドキュメントを処理:
   - テキストドキュメントを小さなチャンクに分割
   - 各チャンクの埋め込みを作成
   - PostgreSQL のベクターデータベースに保存
4. 初回クエリを実行し、ベースラインの応答品質を確認
5. データを最適化:
   - エージェントでチャンクをクレンジングし、重複を除去
   - クレンジング後のチャンクに対して新たに埋め込みを作成
   - 最適化済みデータでベクターストアを更新
6. 同じクエリを再実行し、応答品質の向上を検証

## セットアップ

### 環境のセットアップ

環境変数を設定してください：

```bash filename=".env"
OPENAI_API_KEY=ここにあなたの_openai_api_key_を入力
POSTGRES_CONNECTION_STRING=ここにあなたの接続文字列を入力
```


### 依存関係

次に、必要な依存モジュールをインポートします。

```typescript copy showLineNumbers filename="index.ts"
import { openai } from "@ai-sdk/openai";
import { Mastra } from "@mastra/core";
import { Agent } from "@mastra/core/agent";
import { PgVector } from "@mastra/pg";
import {
  MDocument,
  createVectorQueryTool,
  createDocumentChunkerTool,
} from "@mastra/rag";
import { embedMany } from "ai";
```


## ツールの作成

### ベクタークエリツール

@mastra/rag からインポートした createVectorQueryTool を使うと、ベクターデータベースに対してクエリを実行できるツールを作成できます。

```typescript copy showLineNumbers{8} filename="index.ts"
const vectorQueryTool = createVectorQueryTool({
  vectorStoreName: "pgVector",
  indexName: "embeddings",
  model: openai.embedding("text-embedding-3-small"),
});
```


### Document Chunker ツール

@mastra/rag からインポートした createDocumentChunkerTool を使うと、ドキュメントを分割し、その分割結果をエージェントに送信するツールを作成できます。

```typescript copy showLineNumbers{14} filename="index.ts"
const doc = MDocument.fromText(yourText);

const documentChunkerTool = createDocumentChunkerTool({
  doc,
  params: {
    strategy: "recursive",
    size: 512,
    overlap: 25,
    separator: "\n",
  },
});
```


## エージェントの設定

問い合わせとクレンジングの両方に対応できる単一の Mastra エージェントをセットアップします：

```typescript copy showLineNumbers{26} filename="index.ts"
const ragAgent = new Agent({
  name: "RAGエージェント",
  instructions: `あなたは、クエリ処理とドキュメントのクレンジングの両方を行う有能なアシスタントです。
    クレンジング時: データの処理・整備・ラベリングを行い、不要な情報を除去して重複を排除しつつ、重要な事実は保持してください。
    クエリ時: 利用可能なコンテキストに基づいて回答してください。回答は簡潔で関連性の高いものにしてください。
    
    重要: 質問への回答を求められた場合は、ツールで提供されたコンテキストのみに基づいて回答してください。コンテキストに質問へ完全に回答するのに十分な情報がない場合は、その旨を明確に述べてください。
    `,
  model: openai("gpt-4o-mini"),
  tools: {
    vectorQueryTool,
    documentChunkerTool,
  },
});
```


## PgVector と Mastra のインスタンス化

次のコンポーネントを使って、PgVector と Mastra のインスタンスを作成します：

```typescript copy showLineNumbers{41} filename="index.ts"
const pgVector = new PgVector({
  connectionString: process.env.POSTGRES_CONNECTION_STRING!,
});

export const mastra = new Mastra({
  agents: { ragAgent },
  vectors: { pgVector },
});
const agent = mastra.getAgent("ragAgent");
```


## ドキュメント処理

元のドキュメントを分割し、埋め込みを作成します。

```typescript copy showLineNumbers{49} filename="index.ts"
const chunks = await doc.chunk({
  strategy: "recursive",
  size: 256,
  overlap: 50,
  separator: "\n",
});

const { embeddings } = await embedMany({
  model: openai.embedding("text-embedding-3-small"),
  values: chunks.map((chunk) => chunk.text),
});

const vectorStore = mastra.getVector("pgVector");
await vectorStore.createIndex({
  indexName: "embeddings",
  dimension: 1536,
});

await vectorStore.upsert({
  indexName: "embeddings",
  vectors: embeddings,
  metadata: chunks?.map((chunk: any) => ({ text: chunk.text })),
});
```


## 最初のクエリ

ベースラインを取るために、生データに対してクエリしてみましょう。

```typescript copy showLineNumbers{73} filename="index.ts"
// 元の埋め込みを使ってレスポンスを生成する
const query = "宇宙探査に関して言及されている技術をすべて挙げてください。";
const originalResponse = await agent.generate(query);
console.log("\nクエリ:", query);
console.log("レスポンス:", originalResponse.text);
```


## データ最適化

初期結果を確認したら、品質を高めるためにデータをクレンジングできます：

```typescript copy showLineNumbers{79} filename="index.ts"
const chunkPrompt = `提供されたツールを使ってチャンクをクリーンアップしてください。宇宙に関係のない不要な情報を必ず除外し、重複を削除してください。`;

const newChunks = await agent.generate(chunkPrompt);
const updatedDoc = MDocument.fromText(newChunks.text);

const updatedChunks = await updatedDoc.chunk({
  strategy: "recursive",
  size: 256,
  overlap: 50,
  separator: "\n",
});

const { embeddings: cleanedEmbeddings } = await embedMany({
  model: openai.embedding("text-embedding-3-small"),
  values: updatedChunks.map((chunk) => chunk.text),
});

// クリーン済みの埋め込みでベクトルストアを更新する
await vectorStore.deleteIndex({ indexName: "embeddings" });
await vectorStore.createIndex({
  indexName: "embeddings",
  dimension: 1536,
});

await vectorStore.upsert({
  indexName: "embeddings",
  vectors: cleanedEmbeddings,
  metadata: updatedChunks?.map((chunk: any) => ({ text: chunk.text })),
});
```


## 最適化されたクエリ

クリーンアップ後にデータを再度照会し、応答に違いがあるか確認します。

```typescript copy showLineNumbers{109} filename="index.ts"
// クリーン化した埋め込みで再度クエリを実行
const cleanedResponse = await agent.generate(query);
console.log("\nクエリ:", query);
console.log("レスポンス:", cleanedResponse.text);
```

<br />

<br />

<hr className="dark:border-[#404040] border-gray-300" />

<br />

<br />

<GithubLink
  link={
"https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/cleanup-rag"
}
/>
