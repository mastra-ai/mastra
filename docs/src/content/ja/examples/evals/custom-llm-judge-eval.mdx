---
title: "例：Real World Countries | Evals | Mastra Docs"
description: カスタムのLLMベース評価指標を作成する例。
---

import { GithubLink } from "@/components/github-link";
import { ScorerCallout } from "@/components/scorer-callout";


# 判定者としての LLM 評価

<ScorerCallout />

この例では、世界の実在する国を特定するためのカスタム LLM ベースの評価指標を作成する方法を示します。この指標は `query` と `response` を受け取り、応答がクエリにどれだけ正確に合致しているかに基づいて、スコアと根拠を返します。

## インストール

```bash copy
npm install @mastra/evals
```


## カスタム eval を作成する

Mastra のカスタム eval は、構造化されたプロンプトと評価基準に基づいて、LLM を用いてレスポンスの品質を判定できます。これは次の4つの中核コンポーネントから成ります：

1. [**Instructions**](#eval-instructions)
2. [**Prompt**](#eval-prompt)
3. [**Judge**](#eval-judge)
4. [**Metric**](#eval-metric)

これらを組み合わせることで、Mastra の組み込みメトリクスではカバーされない可能性のあるカスタム評価ロジックを定義できます。

```typescript filename="src/mastra/evals/example-real-world-countries.ts" showLineNumbers copy
import { Metric, type MetricResult } from "@mastra/core";
import { MastraAgentJudge } from "@mastra/evals/judge";
import { type LanguageModel } from "@mastra/core/llm";
import { z } from "zod";

const INSTRUCTIONS = `あなたは地理の専門家です。元の質問に基づいて、回答に記載されている有効な国がいくつあるかを評価してください。`;

const generatePrompt = (query: string, response: string) => `

質問:「${query}」
回答:「${response}」

回答に記載されている有効な実在する国がいくつあるかを評価してください。

返却値:
{
  "score": number (0 to 1),
  "info": {
    "reason": 文字列、
    "matches": [文字列、文字列]、
    "misses": [文字列]
  }
}
`;

class WorldCountryJudge extends MastraAgentJudge {
  constructor(model: LanguageModel) {
    super("WorldCountryJudge", INSTRUCTIONS, model);
  }

  async evaluate(query: string, response: string): Promise<MetricResult> {
    const prompt = generatePrompt(query, response);
    const result = await this.agent.generate(prompt, {
      structuredOutput: {
        schema: z.object({
          score: z.number().min(0).max(1),
          info: z.object({
            reason: z.string(),
            matches: z.array(z.string()),
            misses: z.array(z.string())
          })
        })
      },
    });

    return result.object;
  }
}

export class WorldCountryMetric extends Metric {
  judge: WorldCountryJudge;

  constructor(model: LanguageModel) {
    super();
    this.judge = new WorldCountryJudge(model);
  }

  async measure(query: string, response: string): Promise<MetricResult> {
    return this.judge.evaluate(query, response);
  }
}
```


### 評価手順

審査者の役割を定め、LLM が回答をどのように評価すべきかの基準と期待値を示します。

### 評価プロンプト

`query` と `response` を使って一貫性のある評価用プロンプトを構築し、LLM が `score` と構造化された `info` オブジェクトを返すよう促します。

### Eval judge

`MastraAgentJudge` を拡張して、プロンプトの生成とスコアリングを管理します。

- `generatePrompt()` は指示をクエリとレスポンスに組み合わせます。
- `evaluate()` はプロンプトを LLM に送信し、Zod スキーマで出力を検証します。
- 数値の `score` とカスタマイズ可能な `info` オブジェクトを含む `MetricResult` を返します。

### 評価メトリック

Mastra の `Metric` クラスを拡張し、評価の主要なエントリポイントとして機能します。ジャッジを用いて `measure()` で結果を算出して返します。

## 高度なカスタム例

この例は、応答が評価基準と強く一致していることを示しています。評価指標は高いスコアを付与し、出力が期待に適合している理由を説明する裏付け情報を含んでいます。

```typescript filename="src/example-high-real-world-countries.ts" showLineNumbers copy
import { openai } from "@ai-sdk/openai";
import { WorldCountryMetric } from "./mastra/evals/example-real-world-countries";

const metric = new WorldCountryMetric(openai("gpt-4o-mini"));

const query = "世界の国をいくつか挙げてください。";
const response = "フランス、日本、アルゼンチン";

const result = await metric.measure(query, response);

console.log(result);
```


### カスタム出力（高評価）

この出力は、回答内容がジャッジの求める要件に完全に合致しているため、高いスコアを獲得します。`info` オブジェクトは、そのスコアが付与された理由を理解するうえで有用な文脈を提供します。

```typescript
{
  score: 1,
  info: {
    reason: '記載された国はすべて有効で、国際的に承認された国家です。',
    matches: [ 'France', 'Japan', 'Argentina' ],
    misses: []
  }
}
```


## 部分的なカスタム例

この例では、レスポンスに正しい要素と誤った要素が混在しています。指標はそれを反映して中程度のスコアを返し、何が正しく、何が見落とされたのかを説明する詳細も示します。

```typescript filename="src/example-partial-real-world-countries.ts" showLineNumbers copy
import { openai } from "@ai-sdk/openai";
import { WorldCountryMetric } from "./mastra/evals/example-real-world-countries";

const metric = new WorldCountryMetric(openai("gpt-4o-mini"));

const query = "世界の国名をいくつか挙げてください。";
const response = "Germany, Narnia, Australia";

const result = await metric.measure(query, response);

console.log(result);
```


### 部分的なカスタム出力

このスコアは、レスポンスに基準を満たす有効な項目と満たさない無効な項目が混在しているため、部分的な成功を示しています。`info` フィールドには、一致した点と一致しなかった点の内訳が示されています。

```typescript
{
  score: 0.67,
  info: {
    reason: '3件中2件が有効な国名です。',
    matches: [ 'Germany', 'Australia' ],
    misses: [ 'Narnia' ]
  }
}
```


## カスタムの低評価例

この例では、応答が評価基準をまったく満たしていません。期待される要素が一切含まれていないため、指標は低いスコアを返します。

```typescript filename="src/example-low-real-world-countries.ts" showLineNumbers copy
import { openai } from "@ai-sdk/openai";
import { WorldCountryMetric } from "./mastra/evals/example-real-world-countries";

const metric = new WorldCountryMetric(openai("gpt-4o-mini"));

const query = "世界の国をいくつか挙げてください。";
const response = "ゴッサム、ワカンダ、アトランティス";

const result = await metric.measure(query, response);

console.log(result);
```


### カスタム出力が低い

スコアは0です。これは、応答に必要な要素がまったく含まれていないためです。`info` フィールドは結果の説明と、そうなった原因となる不足点の一覧を示します。

```typescript
{
  score: 0,
  info: {
    reason: 'この回答には実在の国ではなく架空の地名が含まれています。',
    matches: [],
    misses: [ 'ゴッサム', 'ワカンダ', 'アトランティス' ]
  }
}
```


## 結果の理解

`WorldCountryMetric` は、次の形式の結果を返します。

```typescript
{
  score: number,
  info: {
    理由: string,
    一致項目: string[],
    欠落項目: string[]
  }
}
```


### カスタムスコア

0〜1のスコア:

- **1.0**: 応答には誤りのない有効な項目のみが含まれる。
- **0.7–0.9**: 応答はおおむね正しいが、1〜2件の誤った項目を含む場合がある。
- **0.4–0.6**: 応答は玉石混交で、有効なものと無効なものが混在している。
- **0.1–0.3**: 応答の大半が誤り、または無関係な項目で占められている。
- **0.0**: 評価基準に照らして有効な内容がまったく含まれていない。

### カスタム情報

スコアに関する説明。詳細には次が含まれます:

- 結果に対する平易な理由。
- 応答内で見つかった正しい要素を列挙する `matches` 配列。
- 誤りがある、または基準を満たさなかった項目を示す `misses` 配列。

<GithubLink
  outdated={true}
  marginTop='mt-16'
  link="https://github.com/mastra-ai/mastra/blob/main/examples/basics/evals/custom-eval"
/>