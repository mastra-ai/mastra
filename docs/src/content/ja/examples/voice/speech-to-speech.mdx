---
title: 音声対話
description: Mastra を使って音声対音声アプリケーションを作成する例。
---

import { GithubLink } from "@/components/github-link";


# Mastra による通話分析

このガイドでは、Mastra を使って分析機能付きの完全な音声会話システムを構築する方法を解説します。例として、リアルタイムの音声対音声の会話、録音管理、そして Roark Analytics と連携した通話分析を取り上げます。

## 概要

本システムは、Mastra エージェントとの音声通話を開始し、やり取り全体を録音して Cloudinary に保存用としてアップロードした後、会話データを Roark Analytics に送信して詳細な通話分析を実施します。

## 設定

### 前提条件

1. 音声認識および音声合成用の OpenAI APIキー
2. 音声ファイルの保存用 Cloudinary アカウント
3. 通話分析用 Roark Analytics APIキー

### 環境設定

提供されているサンプルをもとに、`.env` ファイルを作成してください：

```bash filename="speech-to-speech/call-analysis/sample.env" copy
OPENAI_API_KEY=
CLOUDINARY_CLOUD_NAME=
CLOUDINARY_API_KEY=
CLOUDINARY_API_SECRET=
ROARK_API_KEY=
```


### インストール

必要な依存関係をインストールします:

```bash copy
npm install
```


## 実装

### Mastra エージェントの作成

まず、音声機能を備えたエージェントを定義します。

```ts filename="speech-to-speech/call-analysis/src/mastra/agents/index.ts" copy
import { openai } from "@ai-sdk/openai";
import { Agent } from "@mastra/core/agent";
import { createTool } from "@mastra/core/tools";
import { OpenAIRealtimeVoice } from "@mastra/voice-openai-realtime";
import { z } from "zod";

// エージェントに処理をさせる
export const speechToSpeechServer = new Agent({
  name: "mastra",
  instructions: "あなたは頼りになるアシスタントです。",
  voice: new OpenAIRealtimeVoice(),
  model: openai("gpt-4o"),
  tools: {
    salutationTool: createTool({
      id: "salutationTool",
      description: "ツールの結果を読み取る",
      inputSchema: z.object({ name: z.string() }),
      outputSchema: z.object({ message: z.string() }),
      execute: async ({ context }) => {
        return { message: `こんにちは、${context.name}さん！` };
      },
    }),
  },
});
```


### Mastra の初期化

エージェントを Mastra に登録する:

```ts filename="speech-to-speech/call-analysis/src/mastra/index.ts" copy
import { Mastra } from "@mastra/core";
import { speechToSpeechServer } from "./agents";

export const mastra = new Mastra({
  agents: {
    speechToSpeechServer,
  },
});
```


### 音声ファイル保存のための Cloudinary 連携

録音した音声ファイルを保存できるように Cloudinary を設定します：

```ts filename="speech-to-speech/call-analysis/src/upload.ts" copy
import { v2 as cloudinary } from "cloudinary";

cloudinary.config({
  cloud_name: process.env.CLOUDINARY_CLOUD_NAME,
  api_key: process.env.CLOUDINARY_API_KEY,
  api_secret: process.env.CLOUDINARY_API_SECRET,
});

export async function uploadToCloudinary(path: string) {
  const response = await cloudinary.uploader.upload(path, {
    resource_type: "raw",
  });
  console.log(response);
  return response.url;
}
```


### メインアプリケーションのロジック

メインアプリケーションは、会話の流れ、録音、アナリティクス連携を統括します。

```ts filename="speech-to-speech/call-analysis/src/base.ts" copy
import { Roark } from "@roarkanalytics/sdk";
import chalk from "chalk";

import { mastra } from "./mastra";
import { createConversation, formatToolInvocations } from "./utils";
import { uploadToCloudinary } from "./upload";
import fs from "fs";

const client = new Roark({
  bearerToken: process.env.ROARK_API_KEY,
});

async function speechToSpeechServerExample() {
  const { start, stop } = createConversation({
    mastra,
    recordingPath: "./speech-to-speech-server.mp3",
    providerOptions: {},
    initialMessage: "やあ、相棒"
    onConversationEnd: async (props) => {
      // ファイルをアップロード
      fs.writeFileSync(props.recordingPath, props.audioBuffer);
      const url = await uploadToCloudinary(props.recordingPath);

      // Roark に送信
      console.log("Roark に送信:", url);
      const response = await client.callAnalysis.create({
        recordingUrl: url,
        startedAt: props.startedAt,
        callDirection: "INBOUND",
        interfaceType: "PHONE",
        participants: [
          {
            role: "AGENT",
            spokeFirst: props.agent.spokeFirst,
            name: props.agent.name,
            phoneNumber: props.agent.phoneNumber,
          },
          {
            role: "CUSTOMER",
            name: "Yujohn Nattrass",
            phoneNumber: "987654321",
          },
        ],
        properties: props.metadata,
        toolInvocations: formatToolInvocations(props.toolInvocations),
      });

      console.log("通話録音を送信しました:", response.data);
    },
    onWriting: (ev) => {
      if (ev.role === "assistant") {
        process.stdout.write(chalk.blue(ev.text));
      }
    },
  });

  await start();

  process.on("SIGINT", async (e) => {
    await stop();
  });
}

speechToSpeechServerExample().catch(console.error);
```


## 会話ユーティリティ

`utils.ts` ファイルには、会話を管理するためのヘルパー関数が含まれており、次の機能を提供します：

1. 会話セッションの作成と管理
2. 音声録音の取り扱い
3. ツール呼び出しの処理
4. 会話のライフサイクルイベントの管理

## 例を実行する

次のコマンドで会話を開始します:

```bash copy
npm run dev
```

The application will:

1. Mastra エージェントとリアルタイムの音声会話を開始する
2. 会話全体を録音する
3. 会話の終了時に録音を Cloudinary にアップロードする
4. 分析のために会話データを Roark Analytics に送信する
5. 分析結果を表示する


## 主な機能

- **リアルタイム音声対話**: OpenAIの音声モデルを用いて自然な会話を実現
- **会話の記録**: 後で分析できるよう、会話全体を保存
- **ツール呼び出しの追跡**: 会話中にAIツールがいつ、どのように使われたかを記録
- **アナリティクス連携**: 会話データをRoark Analyticsに送信して詳細な分析を実施
- **クラウドストレージ**: Cloudinaryに録音をアップロードして安全に保管・アクセス

## カスタマイズ

この例は次の方法でカスタマイズできます：

- エージェントの指示内容や機能を調整する
- エージェントが使うツールを追加する
- 会話フローや初期メッセージを変更する
- カスタムメタデータを用いてアナリティクス連携を拡張する

完全なサンプルコードは、[GitHub リポジトリ](https://github.com/mastra-ai/voice-examples/tree/main/speech-to-speech/call-analysis)をご覧ください。

<br />

<br />

<GithubLink link="https://github.com/mastra-ai/voice-examples/tree/main/speech-to-speech/call-analysis" />