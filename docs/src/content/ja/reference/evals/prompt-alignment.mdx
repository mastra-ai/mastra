---
title: "リファレンス: プロンプト整合性 | メトリクス | Evals | Mastra ドキュメント"
description: Mastra における「プロンプト整合性」メトリクスのドキュメント。LLM の出力が与えられたプロンプトの指示にどれだけ適合しているかを評価します。
---

import { ScorerCallout } from '@/components/scorer-callout'


# PromptAlignmentMetric

<ScorerCallout />

`PromptAlignmentMetric` クラスは、LLM の出力が与えられたプロンプトの指示にどれほど厳密に従っているかを評価します。各指示が正確に遵守されているかを判定するジャッジ方式を用い、逸脱がある場合にはその理由を詳細に提示します。

## 基本的な使い方

```typescript
import { openai } from "@ai-sdk/openai";
import { PromptAlignmentMetric } from "@mastra/evals/llm";

// 評価用にモデルを構成する
const model = openai("gpt-4o-mini");

const instructions = [
  "文は大文字で始める",
  "各文の末尾にピリオドを付ける",
  "現在形を使う",
];

const metric = new PromptAlignmentMetric(model, {
  instructions,
  scale: 1,
});

const result = await metric.measure(
  "天気を説明する",
  "太陽が照っている。雲が空に浮かんでいる。そよ風が吹いている。",
);

console.log(result.score); // アラインメント・スコア（0～1）
console.log(result.info.reason); // スコアの説明
```


## コンストラクターのパラメーター

<PropertiesTable
  content={[
    {
      name: "model",
      type: "LanguageModel",
      description:
        "命令整合性を評価するために使用するモデルの設定",
      isOptional: false,
    },
    {
      name: "options",
      type: "PromptAlignmentOptions",
      description: "このメトリクスの設定オプション",
      isOptional: false,
    },
  ]}
/>

### PromptAlignmentOptions

<PropertiesTable
  content={[
    {
      name: "instructions",
      type: "string[]",
      description: "出力が従うべき指示の配列",
      isOptional: false,
    },
    {
      name: "scale",
      type: "number",
      description: "最大スコア",
      isOptional: true,
      defaultValue: "1",
    },
  ]}
/>

## measure() のパラメータ

<PropertiesTable
  content={[
    {
      name: "input",
      type: "string",
      description: "元のプロンプトまたはクエリ",
      isOptional: false,
    },
    {
      name: "output",
      type: "string",
      description: "評価対象のLLMの応答",
      isOptional: false,
    },
  ]}
/>

## 返り値

<PropertiesTable
  content={[
    {
      name: "score",
      type: "number",
      description: "アライメントスコア（0〜scale、既定では 0〜1）",
    },
    {
      name: "info",
      type: "object",
      description:
        "指示遵守に関する詳細な指標を含むオブジェクト",
      properties: [
        {
          type: "string",
          parameters: [
            {
              name: "reason",
              type: "string",
              description:
                "スコアおよび指示遵守に関する詳細な説明",
            },
          ],
        },
      ],
    },
  ]}
/>

## スコアリングの詳細

この指標は、以下の観点から指示への整合性を評価します:

- 各指示の適用可否の評価
- 適用対象となる指示への厳密な遵守評価
- すべての判定に対する詳細な根拠の提示
- 適用対象の指示に基づく比例配点

### 命令の判定

各命令には次のいずれかの判定が付与されます：

- "yes": 命令は適用可能で、完全に満たされている
- "no": 命令は適用可能だが、満たされていない、または一部のみ満たされている
- "n/a": 命令は当該文脈には適用されない

### スコアリングプロセス

1. 指示の適用可否を評価:

   - 各指示が文脈に当てはまるか判断
   - 無関係な指示は「n/a」としてマーク
   - ドメイン特有の要件を考慮

2. 適用対象の指示の遵守状況を評価:

   - 各適用対象の指示を個別に評価
   - 「yes」の判定には完全な遵守が必要
   - すべての判定について具体的な理由を記録

3. アラインメントスコアを算出:
   - 準拠した指示（「yes」判定）の数をカウント
   - 適用対象の指示の総数で割る（「n/a」を除外）
   - 既定の範囲にスケーリング

最終スコア: `(followed_instructions / applicable_instructions) * scale`

### 重要な考慮事項

- 空の出力:
  - すべての書式指示は適用対象とみなす
  - 要件を満たせないため「no」としてマークする
- ドメイン固有の指示:
  - 問い合わせたドメインに関する場合は常に適用対象
  - 守られていない場合は「n/a」ではなく「no」としてマークする
- 「n/a」の判定:
  - まったく別のドメインに対してのみ使用する
  - 最終スコアの算出には影響しない

### スコアの解釈

（スケールは0から。デフォルトは0〜1）

- 1.0: 該当する指示をすべて完全に遵守
- 0.7-0.9: 該当する指示の大半を遵守
- 0.4-0.6: 該当する指示の遵守状況はばらつきがある
- 0.1-0.3: 該当する指示の遵守は限定的
- 0.0: 該当する指示をまったく遵守していない

## 解説付きの例

```typescript
import { openai } from "@ai-sdk/openai";
import { PromptAlignmentMetric } from "@mastra/evals/llm";

// 評価用にモデルを設定する
const model = openai("gpt-4o-mini");

const metric = new PromptAlignmentMetric(model, {
  instructions: [
    "各項目には箇条書きを使う",
    "例をちょうど3つ含める",
    "各項目をセミコロンで終える"
  ],
  scale: 1
});

const result = await metric.measure(
  "果物を3つ挙げてください",
  "• りんごは赤くて甘い；
• バナナは黄色で曲がっている；
• オレンジは柑橘で丸い。"
);

// 出力例:
// {
//   score: 1.0,
//   info: {
//     reason: "スコアが1.0である理由は、すべての指示に正確に従っているためです。
//           箇条書きが使用され、例がちょうど3つ提示され、
//           各項目がセミコロンで終わっています。"
//   }
// }

const result2 = await metric.measure(
  "果物を3つ挙げてください",
  "1. りんご
2. バナナ
3. オレンジとぶどう"
);

// 出力例:
// {
//   score: 0.33,
//   info: {
//     reason: "スコアが0.33である理由は次のとおりです。箇条書きではなく番号付きリストが使用され、
//           セミコロンが使われておらず、ちょうど3つではなく4つの果物が挙げられています。"
//   }
// }
```


## 関連項目

- [回答の関連性メトリクス](./answer-relevancy)
- [キーワード網羅性メトリクス](./keyword-coverage)