---
title: "概要"
description: Mastra のスコアラーの概要。AI の出力を評価し、パフォーマンスを測定するための機能について説明します。
---

import { Callout } from "nextra/components";


# Scorers の概要

従来のソフトウェアテストには明確な合否条件がありますが、AI の出力は非決定的で、同じ入力でも変わり得ます。**Scorers** は、エージェントの品質を定量的に測る指標を提供することで、このギャップを埋めます。

Scorers は、モデル採点・ルールベース・統計的手法を用いて Agent の出力を評価する自動テストです。Scorers は **scores**（スコア）を返します。これは数値（通常は 0〜1）で、出力が評価基準をどの程度満たしているかを定量化します。これらのスコアにより、パフォーマンスを客観的に追跡し、さまざまなアプローチを比較し、AI システムの改善点を特定できます。Scorers は、独自のプロンプトやスコアリング関数でカスタマイズ可能です。

Scorers はクラウド上で実行でき、リアルタイムに結果を取得できます。さらに、Scorers を CI/CD パイプラインに組み込むことで、エージェントを継続的にテストおよび監視できます。

## スコアラーの種類

スコアラーには目的ごとにさまざまな種類があります。一般的なタイプは次のとおりです。

1. **テキストスコアラー**: エージェントの応答について、正確性、信頼性、文脈理解を評価する
2. **分類スコアラー**: 事前に定義されたカテゴリに基づいてデータを分類する際の精度を測定する
3. **プロンプトエンジニアリング用スコアラー**: さまざまな指示や入力形式が与える影響を検証する

## インストール

Mastra のスコアラー機能にアクセスするには、`@mastra/evals` パッケージをインストールしてください。

```bash copy
npm install @mastra/evals@latest
```


## ライブ評価

**ライブ評価**では、エージェントやワークフローの稼働中に、AIの出力をリアルタイムで自動採点できます。評価を手動やバッチで実行するのではなく、スコアラーがAIシステムと並行して非同期に動作し、継続的に品質を監視します。

### エージェントにスコアラーを追加する

エージェントに組み込みのスコアラーを追加して、出力を自動評価できます。利用可能なオプションは、[組み込みスコアラーの一覧](/docs/scorers/off-the-shelf-scorers)をご覧ください。

```typescript filename="src/mastra/agents/evaluated-agent.ts" showLineNumbers copy
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
import { 
  createAnswerRelevancyScorer,
  createToxicityScorer 
} from "@mastra/evals/scorers/llm";

export const evaluatedAgent = new Agent({
  // ...
  scorers: {
    relevancy: {
      scorer: createAnswerRelevancyScorer({ model: openai("gpt-4o-mini") }),
      sampling: { type: "ratio", rate: 0.5 }
    },
    safety: {
      scorer: createToxicityScorer({ model: openai("gpt-4o-mini") }),
      sampling: { type: "ratio", rate: 1 }
    }
  }
});
```


### ワークフローの各ステップにスコアラーを追加する

プロセス内の特定のタイミングで出力を評価できるよう、ワークフローの各ステップにスコアラーを追加することも可能です。

```typescript filename="src/mastra/workflows/content-generation.ts" showLineNumbers copy
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
import { customStepScorer } from "../scorers/custom-step-scorer";

const contentStep = createStep({
  // ...
  scorers: {
    customStepScorer: {
      scorer: customStepScorer(),
      sampling: {
        type: "ratio",
        rate: 1, // すべてのステップ実行をスコア付け
      }
    }
  },
});

export const contentWorkflow = createWorkflow({ ... })
  .then(contentStep)
  .commit();
```


### ライブ評価の仕組み

**非同期実行**: ライブ評価はエージェントの応答やワークフローの実行を妨げることなく、バックグラウンドで行われます。これにより、監視しながらも AI システムのパフォーマンスを維持できます。

**サンプリング制御**: `sampling.rate` パラメータ（0〜1）は、スコアリング対象となる出力の割合を制御します:

- `1.0`: すべての応答をスコアリング（100%）
- `0.5`: 応答の半数をスコアリング（50%）
- `0.1`: 応答の 10% をスコアリング
- `0.0`: スコアリングを無効化

**自動保存**: すべてのスコアリング結果は、設定済みデータベースの `mastra_scorers` テーブルに自動保存され、時間経過に伴うパフォーマンスの傾向を分析できます。

## トレース評価

ライブ評価に加えて、スコアラーを使ってエージェントのやり取りやワークフローから得られた過去のトレースを評価できます。これは、過去のパフォーマンスの分析、問題のデバッグ、バッチ評価の実行に特に有用です。

<Callout type="info">
**オブザーバビリティが必要です**

トレースにスコアを付けるには、まず Mastra インスタンスでオブザーバビリティを設定し、トレースデータを収集できるようにする必要があります。セットアップ手順は [AI Tracing のドキュメント](../observability/ai-tracing) を参照してください。
</Callout>

### Playground でトレースをスコアリングする

トレースをスコアリングするには、まず Mastra インスタンスにスコアラーを登録する必要があります。

```typescript
const mastra = new Mastra({
  // ...
  scorers: {
    answerRelevancy: myAnswerRelevancyScorer,
    responseQuality: myResponseQualityScorer
  }
});
```

登録後は、Observability セクションにある Mastra のプレイグラウンドで、トレースに対して対話的にスコア付けできます。これにより、過去のトレースに対してスコアラーを実行するための使いやすいインターフェースが提供されます。


## スコアラーをローカルでテストする

Mastra には、スコアラーをテストできる CLI コマンド `mastra dev` が用意されています。プレイグラウンドにはスコアラー用のセクションがあり、テスト入力に対して個々のスコアラーを実行し、詳細な結果を確認できます。

詳しくは [Local Dev Playground](/docs/server-db/local-dev-playground) のドキュメントをご覧ください。

## 次のステップ

- [Creating Custom Scorers](/docs/scorers/custom-scorers) ガイドで独自のスコアラーの作り方を学ぶ
- [Off-the-shelf Scorers](/docs/scorers/off-the-shelf-scorers) セクションで組み込みスコアラーを確認する
- [Local Dev Playground](/docs/server-db/local-dev-playground) でスコアラーをテストする