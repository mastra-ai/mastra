---
title: "Mastra 中的语音转语音功能 | 语音"
description: "Mastra 中语音转语音功能的概述，包括实时交互和事件驱动架构。"
packages:
  - "@mastra/core"
  - "@mastra/node-audio"
  - "@mastra/voice-google-gemini-live"
  - "@mastra/voice-openai-realtime"
---

# Mastra 中的语音转语音功能

## 介绍

Mastra 中的语音转语音 (STS) 为跨多个提供商的实时交互提供了标准化接口。
STS 通过监听实时模型的事件实现连续的双向音频通信。与单独的 TTS 和 STT 操作不同，STS 保持开放连接，在两个方向上连续处理语音。

## 配置

- **`apiKey`**：你的 OpenAI API 密钥。回退到 `OPENAI_API_KEY` 环境变量。
- **`model`**：用于实时语音交互的模型 ID（例如 `gpt-5.1-realtime`）。
- **`speaker`**：语音合成的默认语音 ID。这允许你指定用于语音输出的声音。

```typescript
const voice = new OpenAIRealtimeVoice({
  apiKey: "your-openai-api-key",
  model: "gpt-5.1-realtime",
  speaker: "alloy", // 默认语音
});

// 如果使用默认设置，配置可以简化为：
const voice = new OpenAIRealtimeVoice();
```

## 使用 STS

```typescript
import { Agent } from "@mastra/core/agent";
import { OpenAIRealtimeVoice } from "@mastra/voice-openai-realtime";
import { playAudio, getMicrophoneStream } from "@mastra/node-audio";

const agent = new Agent({
  id: "agent",
  name: "OpenAI Realtime Agent",
  instructions: `You are a helpful assistant with real-time voice capabilities.`,
  model: "openai/gpt-5.1",
  voice: new OpenAIRealtimeVoice(),
});

// 连接到语音服务
await agent.voice.connect();

// 监听代理的音频响应
agent.voice.on("speaker", ({ audio }) => {
  playAudio(audio);
});

// 发起对话
await agent.voice.speak("How can I help you today?");

// 从麦克风发送连续音频
const micStream = getMicrophoneStream();
await agent.voice.send(micStream);
```

有关将语音转语音功能与代理集成，请参阅[向代理添加语音](/docs/cn/agents/adding-voice)文档。

## Google Gemini Live（实时）

```typescript
import { Agent } from "@mastra/core/agent";
import { GeminiLiveVoice } from "@mastra/voice-google-gemini-live";
import { playAudio, getMicrophoneStream } from "@mastra/node-audio";

const agent = new Agent({
  id: "agent",
  name: "Gemini Live Agent",
  instructions:
    "You are a helpful assistant with real-time voice capabilities.",
  // 用于文本生成的模型；语音提供商处理实时音频
  model: "openai/gpt-5.1",
  voice: new GeminiLiveVoice({
    apiKey: process.env.GOOGLE_API_KEY,
    model: "gemini-2.0-flash-exp",
    speaker: "Puck",
    debug: true,
    // Vertex AI 选项：
    // vertexAI: true,
    // project: 'your-gcp-project',
    // location: 'us-central1',
    // serviceAccountKeyFile: '/path/to/service-account.json',
  }),
});

await agent.voice.connect();

agent.voice.on("speaker", ({ audio }) => {
  playAudio(audio);
});

agent.voice.on("writing", ({ role, text }) => {
  console.log(`${role}: ${text}`);
});

await agent.voice.speak("How can I help you today?");

const micStream = getMicrophoneStream();
await agent.voice.send(micStream);
```

注意：

- Live API 需要 `GOOGLE_API_KEY`。Vertex AI 需要项目/位置和服务账户凭据。
- 事件：`speaker`（音频流）、`writing`（文本）、`turnComplete`、`usage` 和 `error`。
