---
title: "参考文档：voice.send() | 语音"
description: "实时语音提供者提供的 send() 方法文档，用于流式传输音频数据进行持续处理。"
packages:
  - "@mastra/node-audio"
  - "@mastra/node-speaker"
  - "@mastra/voice-openai-realtime"
---

# voice.send()

`send()` 方法实时向语音提供者流式传输音频数据进行持续处理。此方法对于实时语音到语音对话至关重要，允许您直接将麦克风输入发送到 AI 服务。

## 使用示例

```typescript
import { OpenAIRealtimeVoice } from "@mastra/voice-openai-realtime";
import Speaker from "@mastra/node-speaker";
import { getMicrophoneStream } from "@mastra/node-audio";

const speaker = new Speaker({
  sampleRate: 24100, // 音频采样率（Hz）- MacBook Pro 高品质音频的标准
  channels: 1, // 单声道音频输出（立体声为 2）
  bitDepth: 16, // 音频位深度 - CD 质量标准（16位分辨率）
});

// 初始化实时语音提供者
const voice = new OpenAIRealtimeVoice({
  realtimeConfig: {
    model: "gpt-5.1-realtime",
    apiKey: process.env.OPENAI_API_KEY,
  },
});

// 连接到实时服务
await voice.connect();

// 设置响应的事件监听器
voice.on("writing", ({ text, role }) => {
  console.log(`${role}: ${text}`);
});

voice.on("speaker", (stream) => {
  stream.pipe(speaker);
});

// 获取麦克风流（实现取决于您的环境）
const microphoneStream = getMicrophoneStream();

// 向语音提供者发送音频数据
await voice.send(microphoneStream);

// 您也可以发送 Int16Array 格式的音频数据
const audioBuffer = getAudioBuffer(); // 假设这返回 Int16Array
await voice.send(audioBuffer);
```

## 参数

<br />
<PropertiesTable
  content={[
    {
      name: "audioData",
      type: "NodeJS.ReadableStream | Int16Array",
      description:
        "要发送给语音提供者的音频数据。可以是可读流（如麦克风流）或音频样本的 Int16Array。",
      isOptional: false,
    },
  ]}
/>

## 返回值

返回 `Promise<void>`，在音频数据被语音提供者接受时解析。

## 注意事项

- 此方法仅由支持语音到语音功能的实时语音提供者实现
- 如果在不支持此功能的语音提供者上调用，它会记录警告并立即解析
- 在使用 `send()` 之前必须调用 `connect()` 以建立 WebSocket 连接
- 音频格式要求取决于具体的语音提供者
- 对于连续对话，您通常调用 `send()` 传输用户音频，然后调用 `answer()` 触发 AI 响应
- 提供者在处理音频时通常会发出带有转录文本的 'writing' 事件
- 当 AI 响应时，提供者会发出带有音频响应的 'speaking' 事件
