---
title: "参考文档：voice.speak() | 语音"
description: "所有 Mastra 语音提供者提供的 speak() 方法文档，用于将文本转换为语音。"
packages:
  - "@mastra/core"
  - "@mastra/node-speaker"
  - "@mastra/voice-openai"
  - "@mastra/voice-openai-realtime"
  - "@mastra/voice-playai"
---

# voice.speak()

`speak()` 是所有 Mastra 语音提供者都提供的核心功能，用于将文本转换为语音。它接受文本输入并返回可以播放或保存的音频流。

## 参数

<PropertiesTable
  content={[
    {
      name: "input",
      type: "string | NodeJS.ReadableStream",
      description:
        "要转换为语音的文本。可以是字符串或文本的可读流。",
      isOptional: false,
    },
    {
      name: "options",
      type: "object",
      description: "语音合成选项",
      isOptional: true,
    },
    {
      name: "options.speaker",
      type: "string",
      description:
        "用于此特定请求的语音 ID。覆盖构造函数中设置的默认语音。",
      isOptional: true,
    },
  ]}
/>

## 返回值

返回 `Promise<NodeJS.ReadableStream | void>`，其中：

- `NodeJS.ReadableStream`：可以播放或保存的音频数据流
- `void`：当使用通过事件发出音频而非直接返回的实时语音提供者时

## 提供者特定选项

每个语音提供者可能支持其特定实现的额外选项。以下是一些示例：

### OpenAI

<PropertiesTable
  content={[
    {
      name: "options.speed",
      type: "number",
      description:
        "语音速度倍数。支持 0.25 到 4.0 之间的值。",
      isOptional: true,
      defaultValue: "1.0",
    },
  ]}
/>

### ElevenLabs

<PropertiesTable
  content={[
    {
      name: "options.stability",
      type: "number",
      description:
        "语音稳定性。较高的值会产生更稳定、表现力较低的语音。",
      isOptional: true,
      defaultValue: "0.5",
    },
    {
      name: "options.similarity_boost",
      type: "number",
      description: "语音清晰度以及与原始语音的相似度。",
      isOptional: true,
      defaultValue: "0.75",
    },
  ]}
/>

### Google

<PropertiesTable
  content={[
    {
      name: "options.languageCode",
      type: "string",
      description: "语音的语言代码（例如 'en-US'）。",
      isOptional: true,
    },
    {
      name: "options.audioConfig",
      type: "object",
      description:
        "来自 Google Cloud 文本转语音 API 的音频配置选项。",
      isOptional: true,
      defaultValue: "{ audioEncoding: 'LINEAR16' }",
    },
  ]}
/>

### Murf

<PropertiesTable
  content={[
    {
      name: "options.properties.rate",
      type: "number",
      description: "语音速度倍数。",
      isOptional: true,
    },
    {
      name: "options.properties.pitch",
      type: "number",
      description: "语音音调调整。",
      isOptional: true,
    },
    {
      name: "options.properties.format",
      type: "'MP3' | 'WAV' | 'FLAC' | 'ALAW' | 'ULAW'",
      description: "输出音频格式。",
      isOptional: true,
    },
  ]}
/>

## 使用示例

```typescript
import { OpenAIVoice } from "@mastra/voice-openai";
// 初始化语音提供者
const voice = new OpenAIVoice({
  speaker: "alloy", // 默认语音
});
// 使用默认设置的基本用法
const audioStream = await voice.speak("您好，世界！");
// 对此特定请求使用不同的语音
const audioStreamWithDifferentVoice = await voice.speak("再次您好！", {
  speaker: "nova",
});
// 使用提供者特定选项
const audioStreamWithOptions = await voice.speak("带选项的您好！", {
  speaker: "echo",
  speed: 1.2, // OpenAI 特定选项
});
// 使用文本流作为输入
import { Readable } from "stream";
const textStream = Readable.from(["您好", " 来自", " 流！"]);
const audioStreamFromTextStream = await voice.speak(textStream);
```

## 与 CompositeVoice 一起使用

使用 `CompositeVoice` 时，`speak()` 方法委托给配置的语音提供者：

```typescript
import { CompositeVoice } from "@mastra/core/voice";
import { OpenAIVoice } from "@mastra/voice-openai";
import { PlayAIVoice } from "@mastra/voice-playai";

const voice = new CompositeVoice({
  output: new PlayAIVoice(),
  input: new OpenAIVoice(),
});

// 这将使用 PlayAIVoice 提供者
const audioStream = await voice.speak("您好，世界！");
```

### 使用 AI SDK 模型提供者

您也可以直接在 `CompositeVoice` 中使用 AI SDK 语音模型：

```typescript
import { CompositeVoice } from "@mastra/core/voice";
import { openai } from "@ai-sdk/openai";
import { elevenlabs } from "@ai-sdk/elevenlabs";

// 使用 AI SDK 语音模型
const voice = new CompositeVoice({
  output: elevenlabs.speech('eleven_turbo_v2'),  // AI SDK 模型
  input: openai.transcription('whisper-1'),      // AI SDK 模型
});

// 同样可以使用
const audioStream = await voice.speak("您好，来自 AI SDK！");

// 可以通过提供者特定选项
const audioWithOptions = await voice.speak("带选项的您好！", {
  speaker: 'Rachel',  // ElevenLabs 语音
  providerOptions: {
    elevenlabs: {
      stability: 0.5,
      similarity_boost: 0.75,
    }
  }
});
```

有关 AI SDK 集成的更多详细信息，请参阅 [CompositeVoice 参考文档](/reference/voice/composite-voice)。

## 实时语音提供者

使用 `OpenAIRealtimeVoice` 等实时语音提供者时，`speak()` 方法的行为不同：

- 它不会返回音频流，而是发出带有音频数据的 'speaking' 事件
- 您需要注册事件监听器来接收音频块

```typescript
import { OpenAIRealtimeVoice } from "@mastra/voice-openai-realtime";
import Speaker from "@mastra/node-speaker";

const speaker = new Speaker({
  sampleRate: 24100, // 音频采样率（Hz）- MacBook Pro 高品质音频的标准
  channels: 1, // 单声道音频输出（立体声为 2）
  bitDepth: 16, // 音频位深度 - CD 质量标准（16位分辨率）
});

const voice = new OpenAIRealtimeVoice();
await voice.connect();
// 注册音频块的事件监听器
voice.on("speaker", (stream) => {
  // 处理音频块（例如，播放或保存它）
  stream.pipe(speaker);
});
// 这将发出 'speaking' 事件而不是返回流
await voice.speak("您好，这是实时语音！");
```

## 注意事项

- `speak()` 的行为可能因提供者而略有不同，但所有实现都遵循相同的基本接口。
- 使用实时语音提供者时，该方法可能不会直接返回音频流，而是发出 'speaking' 事件。
- 如果提供文本流作为输入，提供者通常会在处理前将其转换为字符串。
- 返回流的音频格式取决于提供者。常见格式包括 MP3、WAV 和 OGG。
- 为获得最佳性能，请在完成后考虑关闭或结束音频流。
