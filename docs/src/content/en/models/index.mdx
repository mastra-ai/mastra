---
title: "Models"
description: "Access 44+ AI providers and 618+ models through Mastra's model router."
---

{/* This file is auto-generated by generate-model-docs.ts - DO NOT EDIT MANUALLY */}

import { Tabs, Tab } from "@/components/tabs";
import { CardGrid, CardGridItem } from "@/components/cards/card-grid";
import { Callout } from "nextra/components";

# Model Providers


Integrating AI models into applications typically means dealing with different SDKs, authentication methods, and API formats for each provider. Every new model requires learning a new integration.

Mastra's Model Router standardizes how you work with AI models across providers. This means you can focus on building your application instead of managing provider-specific implementations. Access 600+ models from 40+ providers through a single interface — no package installs, no provider-specific imports. Just specify the model as a string and Mastra handles the rest.

Whether you're using providers directly like OpenAI, Anthropic, or Google, or working through gateways like OpenRouter, the Model Router manages all the routing, authentication, and API compatibility for you. When you need a specific model, you write it as `provider/model-name` and Mastra reads the appropriate environment variable (like `ANTHROPIC_API_KEY`) automatically.

<Tabs items={["OpenAI", "Anthropic", "Google Gemini", "xAI", "OpenRouter"]}>
  <Tab>
    ```typescript copy
    import { Agent } from "@mastra/core";

    const agent = new Agent({
      name: "my-agent",
      instructions: "You are a helpful assistant",
      model: "openai/gpt-4o"  // Works with any provider/model
    });
    ```
  </Tab>
  <Tab>
    ```typescript copy
    import { Agent } from "@mastra/core";

    const agent = new Agent({
      name: "assistant", 
      instructions: "You are a helpful assistant",
      model: "anthropic/claude-3-5-sonnet"
    });
    ```
  </Tab>
  <Tab>
    ```typescript copy
    import { Agent } from "@mastra/core";

    const agent = new Agent({
      name: "my-agent",
      instructions: "You are a helpful assistant",
      model: "openai/gpt-4o"  // Works with any provider/model
    });
    ```
  </Tab>
  <Tab>
    ```typescript copy
    import { Agent } from "@mastra/core";

    const agent = new Agent({
      name: "my-agent",
      instructions: "You are a helpful assistant",
      model: "openai/gpt-4o"  // Works with any provider/model
    });
    ```
  </Tab>
  <Tab>
    ```typescript copy
    import { Agent } from "@mastra/core";

    const agent = new Agent({
      name: "assistant",
      instructions: "You are a helpful assistant", 
      model: "openrouter/meta-llama/llama-3.1-8b-instruct"
    });
    ```
  </Tab>
</Tabs>

## Features

- One API for any model - Access hundreds of models without learning different SDKs or managing multiple integrations. The same Agent interface works with every provider.
- Access the newest AI - New models appear in your IDE autocomplete as soon as they're released. No package updates needed — the model registry updates automatically.
- Single integration - Instead of integrating with each provider separately, you get a model-agnostic interface that works with OpenAI, Anthropic, Google, and dozens more through one consistent API.
- Mix and match models - Use different models for different tasks within your application. Run GPT-4o-mini for processing large documents with its 128K context window, then switch to Claude Opus 4.1 for complex reasoning tasks. Optimize cost and performance by selecting the right model for each job.
- Model discovery - Full TypeScript autocomplete for all 600+ models in your IDE. Start typing and see what's available — no need to check documentation. The playground UI also provides browsable model selection.
- Guaranteed uptime - If a provider experiences an outage or high latency period, automatically reroute to other providers so your customers never experience any issues. Unlike other solutions that force you through their gateway, Mastra handles this at the application level and routes directly to providers for the lowest latency. Learn more about guaranteed uptime →

## Model directory
Mastra supports models from all major AI providers and gateways. Browse the complete list using the navigation on the left, or explore all available models below.

<CardGrid>
    <CardGridItem
      title="Gateways"
      href="./models/gateways"
    >
      <div className="space-y-3">
        <div className="flex flex-col gap-2">
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/openrouter.svg" alt="OpenRouter" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>OpenRouter</span>
          </div>
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/fireworks-ai.svg" alt="Fireworks AI" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>Fireworks AI</span>
          </div>
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/togetherai.svg" alt="Together AI" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>Together AI</span>
          </div>
        </div>
        <div className="text-sm text-gray-600 dark:text-gray-400 mt-3">+ 4 more</div>
      </div>
    </CardGridItem>
    <CardGridItem
      title="Providers"
      href="./models/providers"
    >
      <div className="space-y-3">
        <div className="flex flex-col gap-2">
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/openai.svg" alt="OpenAI" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>OpenAI</span>
          </div>
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/anthropic.svg" alt="Anthropic" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>Anthropic</span>
          </div>
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/google.svg" alt="Google" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>Google</span>
          </div>
        </div>
        <div className="text-sm text-gray-600 dark:text-gray-400 mt-3">+ 34 more</div>
      </div>
    </CardGridItem>
</CardGrid>

Direct providers include OpenAI, Anthropic, Google, Cohere, and more. Gateways like OpenRouter give you access to even more models through a unified billing interface. Each provider's models are available with the same simple string format: provider/model-name.

## Model discovery

Mastra automatically pulls in the latest models from providers like OpenAI, Anthropic, Google, and gateways like OpenRouter. When providers release new models, they appear in your IDE autocomplete without any package updates.

The model registry fetches from sources including models.dev, OpenRouter's model list, and provider APIs. This means you're always working with the most current selection of available models — no manual updates, no waiting for package releases.

<Callout type="info">
**Auto-Refresh**: The model registry automatically refreshes every hour during development. To disable, set `MASTRA_AUTO_REFRESH_PROVIDERS=false`. Auto-refresh is disabled by default in production.
</Callout>



## Mix and match models

Use different models for different parts of your application to optimize both cost and performance. Each agent can use the model best suited for its specific task:

```typescript
import { Agent } from "@mastra/core";

// Use a cost-effective model for document processing
const documentProcessor = new Agent({
  name: "document-processor",
  instructions: "Extract and summarize key information from documents",
  model: "openai/gpt-4o-mini"  // 128K context window, low cost
});

// Use a powerful reasoning model for complex analysis
const reasoningAgent = new Agent({
  name: "reasoning-agent", 
  instructions: "Analyze data and provide strategic recommendations",
  model: "anthropic/claude-opus-4-1"  // Advanced reasoning capabilities
});

// Use a fast model for real-time interactions
const chatAgent = new Agent({
  name: "chat-agent",
  instructions: "Provide quick responses in conversations",
  model: "google/gemini-1.5-flash"  // Optimized for speed
});
```

This approach lets you build sophisticated AI systems where each component uses the optimal model — avoiding overpaying for simple tasks while ensuring complex operations have the power they need.

## Model fallbacks

Configure backup models that activate automatically when your primary provider fails:

```typescript
import { Agent } from '@mastra/core';

const agent = new Agent({
  name: 'resilient-assistant',
  instructions: 'You are a helpful assistant.',
  model: [
    {
      model: "openai/gpt-4o-mini",
      maxRetries: 3,
    },
    {
      model: "anthropic/claude-3-5-sonnet",
      maxRetries: 2,
    },
    {
      model: "google/gemini-pro",
      maxRetries: 2,
    },
  ],
});
```
The system tries your primary model first. If it encounters a 500 error, rate limit, or timeout, it automatically switches to your first fallback. If that fails too, it moves to the next. Each model gets its own retry count before moving on.

Your users never experience the disruption — the response comes back with the same format, just from a different model. The error context is preserved as the system moves through your fallback chain, ensuring clean error propagation while maintaining streaming compatibility.


## Dynamic nodel selection at runtime

Since models are just strings, you can select them dynamically based on runtime context:


```typescript
const agent = new Agent({
  name: "dynamic-assistant",
  model: ({ runtimeContext }) => {
    const provider = runtimeContext.get("provider-id");
    const model = runtimeContext.get("model-id");
    return `${provider}/${model}`;
  },
});
```

This enables powerful patterns:

- Split testing - Compare model performance in production
- User-selectable models - Let users choose their preferred model in your app
- Multi-tenant applications - Each customer can bring their own API keys and model preferences

## Provider-specific options


You can pass provider-specific options either at the agent level or per message. This gives you fine-grained control over model behavior:

### Agent-level provider options

Configure provider options that apply to all messages from an agent:
```typescript
import { openai } from "@ai-sdk/openai";
import { Agent } from "@mastra/core";

const agent = new Agent({
  name: "technical-writer",
  instructions: {
    role: 'system',
    content: 'You are a helpful assistant specialized in technical documentation.',
    providerOptions: {
      openai: { 
        reasoningEffort: 'low'  // OpenAI-specific option for o1 models
      }
    }
  },
  model: openai("o1-preview")
});
```

### Message-level provider options

Override provider options for specific generation calls:
```typescript
const response = await agent.generate([
  { 
    role: "user", 
    content: "Help me organize my day",
    providerOptions: {
      openai: {
        reasoningEffort: 'high'  // Use more reasoning for this complex task
      }
    }
  },
  { role: "user", content: "My day starts at 9am and finishes at 5:30pm" },
  { role: "user", content: "I take lunch between 12:30 and 13:30" },
  { role: "user", content: "I have meetings Monday to Friday between 10:30 and 11:30" }
]);
```

Provider options let you access model-specific features like OpenAI's reasoning effort, Anthropic's cache controls, or Google's safety settings. Message-level options override agent-level defaults, giving you flexibility to adjust behavior based on the specific task.


## Custom headers

When using Mastra's model router, you can specify custom headers for your API requests. This is useful when you need to use a different API endpoint, pull your API key from a secrets manager, or add organization-specific headers:


```typescript
const agent = new Agent({
  name: "custom-agent",
  model: {
    modelId: "gpt-4-turbo",
    apiKey: process.env.OPENAI_API_KEY,
    headers: {
      "OpenAI-Organization": "org-abc123"  // Specify which OpenAI organization to use
    }
  }
});
```

<Callout>
Note: Each model provider handles configuration slightly differently. While OpenAI accepts just the model name, providers like ElevenLabs require additional parameters like URLs. Check the individual model pages in the documentation for provider-specific configuration details — each has a section on custom headers with the exact format required.
</Callout>

## Direct Provider Installation

Mastra uses official SDK clients for major providers like OpenAI, Anthropic, Google, XAI, and OpenRouter, giving you full access to their native features. For other providers, Mastra uses the OpenAI-compatible API standard, which works perfectly for most use cases.
However, some providers implement unique features that require specific SDK client logic not available through the OpenAI-compatible interface. In these rare cases, you can use the Vercel AI SDK directly:

```typescript
import { openai } from "@ai-sdk/openai";
import { Agent } from "@mastra/core";

const agent = new Agent({
  name: "AISDKAgent",
  model: openai("gpt-4-turbo")  // AI SDK model provider
});
```

When to use direct providers:

You need a provider-specific feature not available through OpenAI-compatible APIs
You're already using AI SDK in your project and want consistency
You require custom client configuration beyond what the model router supports

Recommendation: Start with Mastra's built-in model router ("provider/model" strings) for simplicity. Only switch to direct providers when you need specific features not available through the standard interface.


## Custom models
To implement support for a model API that's not already supported, do this:

```typescript
example
```