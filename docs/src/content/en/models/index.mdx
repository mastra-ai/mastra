---
title: "Models"
description: "Access 44+ AI providers and 618+ models through Mastra's model router."
---

{/* This file is auto-generated by generate-model-docs.ts - DO NOT EDIT MANUALLY */}

import { CardGrid, CardGridItem } from "@/components/cards/card-grid";
import { Tab, Tabs } from "@/components/tabs";
import { Callout } from "nextra/components";

# Model Providers

Mastra provides a unified interface for working with LLMs across multiple providers. You can access over 600 models from 40+ providers without managing separate integrations for each one

Whether you're using OpenAI, Anthropic, Google, or a gateway like OpenRouter, specify the model as `"provider/model-name"` and Mastra handles the rest:


<Tabs items={["OpenAI", "Anthropic", "Google Gemini", "xAI", "OpenRouter"]}>
  <Tab>
    ```typescript copy
    import { Agent } from "@mastra/core";

    const agent = new Agent({
      name: "my-agent",
      instructions: "You are a helpful assistant",
      model: "openai/gpt-4o"
    })
    ```
  </Tab>
  <Tab>
    ```typescript copy
    import { Agent } from "@mastra/core";

    const agent = new Agent({
      name: "my-agent", 
      instructions: "You are a helpful assistant",
      model: "anthropic/claude-3-5-sonnet"
    })
    ```
  </Tab>
  <Tab>
    ```typescript copy
    import { Agent } from "@mastra/core";

    const agent = new Agent({
      name: "my-agent",
      instructions: "You are a helpful assistant",
      model: "google/gemini-2.5-flash"
    })
    ```
  </Tab>
  <Tab>
    ```typescript copy
    import { Agent } from "@mastra/core";

    const agent = new Agent({
      name: "my-agent",
      instructions: "You are a helpful assistant",
      model: "xai/grok-4"
    })
    ```
  </Tab>
  <Tab>
    ```typescript copy
    import { Agent } from "@mastra/core";

    const agent = new Agent({
      name: "assistant",
      instructions: "You are a helpful assistant", 
      model: "openrouter/anthropic/claude-3.5-haiku"
    })
    ```
  </Tab>
</Tabs>

Mastra automatically reads the appropriate environment variable (e.g. `ANTHROPIC_API_KEY`) and routes to the correct provider. If an API key is missing, you'll get a clear runtime error showing exactly which variable to set.

## Features

- **One API for any model** - Access any model without having to install and manage additional provider modules/dependencies.

- **Access the newest AI** - Use new models the moment they're released, no matter which provider they come from. Avoid vendor lock-in with Mastra's provider-agnostic interface.  

- **Mix and match models** - Use different models for different tasks. For example, run GPT-4o-mini for large-context processing, then switch to Claude Opus 4.1 for reasoning tasks.  

- **Guaranteed uptime** - If a provider experiences an outage, Mastra can automatically [fall back](#model-fallbacks) to another provider at the application level, minimizing latency and maintaining reliability.  

## Model directory

Browse the directory of available models using the navigation on the left, or explore the models below.

<CardGrid>
    <CardGridItem
      title="Gateways"
      href="./models/gateways"
    >
      <div className="space-y-3">
        <div className="flex flex-col gap-2">
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/openrouter.svg" alt="OpenRouter" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>OpenRouter</span>
          </div>
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/fireworks-ai.svg" alt="Fireworks AI" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>Fireworks AI</span>
          </div>
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/togetherai.svg" alt="Together AI" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>Together AI</span>
          </div>
        </div>
        <div className="text-sm text-gray-600 dark:text-gray-400 mt-3">+ 4 more</div>
      </div>
    </CardGridItem>
    <CardGridItem
      title="Providers"
      href="./models/providers"
    >
      <div className="space-y-3">
        <div className="flex flex-col gap-2">
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/openai.svg" alt="OpenAI" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>OpenAI</span>
          </div>
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/anthropic.svg" alt="Anthropic" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>Anthropic</span>
          </div>
          <div className="flex items-center gap-2 text-sm">
            <img src="https://models.dev/logos/google.svg" alt="Google" className="w-4 h-4 object-contain dark:invert dark:brightness-0 dark:contrast-200" />
            <span>Google</span>
          </div>
        </div>
        <div className="text-sm text-gray-600 dark:text-gray-400 mt-3">+ 34 more</div>
      </div>
    </CardGridItem>
</CardGrid>

You can also explore models directly in your editor. Mastra provides full autocomplete for the `model` field - just start typing, and your IDE will show available options.

Alternatively, browse and test models in the playground UI.

<Callout>
The model list refreshes every hour, so you'll always have the latest available models without needing to update. To disable, set `MASTRA_AUTO_REFRESH_PROVIDERS=false`. Auto-refresh is disabled by default in production.
</Callout>


## Mix and match models

Some models are faster but less capable, while others offer larger context windows or stronger reasoning skills. Use different models from the same provider, or mix and match across providers to fit each task.

```typescript
import { Agent } from "@mastra/core";

// Use a cost-effective model for document processing
const documentProcessor = new Agent({
  name: "document-processor",
  instructions: "Extract and summarize key information from documents",
  model: "openai/gpt-4o-mini" 
})

// Use a powerful reasoning model for complex analysis
const reasoningAgent = new Agent({
  name: "reasoning-agent", 
  instructions: "Analyze data and provide strategic recommendations",
  model: "anthropic/claude-opus-4-1"
})
```

## Provider-specific options

Different model providers expose their own configuration options. With OpenAI, you might adjust the `reasoning_effort` for tasks that need more or less reasoning. With Anthropic, you might tune `cacheControl` behavior for efficiency. Mastra lets you set these specific `providerOptions` either at the agent level or per message.

```typescript
// Agent level (apply to all future messages)
const planner = new Agent({
  instructions: {
    role: "system",
    content: "You are a helpful assistant.",
    providerOptions: {
      openai: { reasoning_effort: "low" }
    }
  },
  model: openai("o1-preview"),
});

const lowEffort = 
  await planner.generate("Plan a simple 3 item dinner menu");

// Message level (apply only to this message)
const highEffort = await planner.generate([
  {
    role: "user",
    content: "Plan a simple 3 item dinner menu",
    providerOptions: {
      openai: { reasoning_effort: "high" }
    }
  }
]);
```
These options map directly to request body parameters under the hood. You can find the full list of supported options in the respective provider's documentation (e.g. [OpenAI API Reference](https://platform.openai.com/docs/api-reference)).

## Custom headers

If you need to specify custom headers, such as an organization ID or other provider-specific fields, use this syntax:


```typescript
const agent = new Agent({
  name: "custom-agent",
  model: {
    modelId: "gpt-4-turbo",
    apiKey: process.env.OPENAI_API_KEY,
    headers: {
      "OpenAI-Organization": "org-abc123"
    }
  }
});
```

<Callout type="warning">
Configuration differs by provider. See our provider pages in the left navigation for custom headers and configuration details.
</Callout>

## Dynamic model selection

Since models are just strings, you can select them dynamically based on [runtime context](/docs/server-db/runtime-context), variables, or any other logic:

```typescript
const agent = new Agent({
  name: "dynamic-assistant",
  model: ({ runtimeContext }) => {
    const provider = runtimeContext.get("provider-id");
    const model = runtimeContext.get("model-id");
    return `${provider}/${model}`;
  },
});
```

This enables powerful patterns:

- A/B testing - Compare model performance in production.
- User-selectable models - Let users choose their preferred model in your app.
- Multi-tenant applications - Each customer can bring their own API keys and model preferences.

## Model fallbacks

Relying on a single model creates a single point of failure for your application. Model fallbacks provide automatic failover between models and providers. If the primary model becomes unavailable, requests are retried against the next configured fallback until one succeeds.


```typescript
import { Agent } from '@mastra/core';

const agent = new Agent({
  name: 'resilient-assistant',
  instructions: 'You are a helpful assistant.',
  model: [
    {
      model: "openai/gpt-4o-mini",
      maxRetries: 3,
    },
    {
      model: "anthropic/claude-3-5-sonnet",
      maxRetries: 2,
    },
    {
      model: "google/gemini-pro",
      maxRetries: 2,
    },
  ],
});
```
Mastra tries your primary model first. If it encounters a 500 error, rate limit, or timeout, it automatically switches to your first fallback. If that fails too, it moves to the next. Each model gets its own retry count before moving on.

Your users never experience the disruption - the response comes back with the same format, just from a different model. The error context is preserved as the system moves through your fallback chain, ensuring clean error propagation while maintaining streaming compatibility.

## Extended support via Vercel AI SDK

Under the hood, Mastra uses official SDK clients for major providers like OpenAI, Anthropic, Google, XAI, and OpenRouter, so you get complete access to each provider's native features. For other providers, Mastra uses the OpenAI-compatible API standard, which covers most use cases. 

However, some providers offer features that require specific SDK client logic not compatible with the OpenAI-compatible interface. In these situations, we recommend using the Vercel AI SDK provider, which is supported in Mastra:


```typescript
import { groq } from '@ai-sdk/groq';
import { Agent } from "@mastra/core";

const agent = new Agent({
  name: "my-agent",
  model: groq('gemma2-9b-it')
})
```
You can use an AI SDK model (e.g. `groq('gemma2-9b-it')`) anywhere that accepts a `"provider/model"` string, including within model router fallbacks.

<Callout type="important">
Start with Mastra's built-in model router ("provider/model" strings) for simplicity. Only switch to direct providers when you need a provider-specific feature not available through OpenAI-compatible API.
</Callout>