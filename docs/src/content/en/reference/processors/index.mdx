---
title: "Reference: Processor | Mastra Docs"
description: Documentation for the Processor interface and InputProcessor/OutputProcessor types in Mastra
---

import { Callout } from 'nextra/components';

# Processor

Processors transform agent input/output at different stages of execution. They enable content moderation, PII detection, structured output generation, and other transformations.

## Processor Interface

```typescript
interface Processor {
  readonly name: string;

  processInput?(args: {
    messages: MastraMessageV2[];
    abort: (reason?: string) => never;
    tracingContext?: TracingContext;
  }): Promise<MastraMessageV2[]> | MastraMessageV2[];

  processOutputStream?(args: {
    part: ChunkType;
    streamParts: ChunkType[];
    state: Record<string, any>;
    abort: (reason?: string) => never;
    tracingContext?: TracingContext;
  }): Promise<ChunkType | null | undefined>;

  processOutputResult?(args: {
    messages: MastraMessageV2[];
    abort: (reason?: string) => never;
    tracingContext?: TracingContext;
  }): Promise<MastraMessageV2[]> | MastraMessageV2[];
}
```

## InputProcessor

An InputProcessor processes messages before they are sent to the LLM.

```typescript
type InputProcessor = WithRequired<Processor, 'name' | 'processInput'> & Processor;
```

<PropertiesTable
  content={[
    {
      name: "name",
      type: "string",
      description: "Unique identifier for this processor",
    },
    {
      name: "processInput",
      type: "(args) => Promise<MastraMessageV2[]> | MastraMessageV2[]",
      description: "Transform input messages before sending to LLM",
    },
  ]}
/>

### processInput Arguments

<PropertiesTable
  content={[
    {
      name: "messages",
      type: "MastraMessageV2[]",
      description: "Array of input messages to process",
    },
    {
      name: "abort",
      type: "(reason?: string) => never",
      description: "Function to abort execution with error message",
    },
    {
      name: "tracingContext",
      type: "TracingContext",
      isOptional: true,
      description: "Tracing context for observability",
    },
  ]}
/>

### Example

```typescript
import type { InputProcessor } from '@mastra/core';

const moderator: InputProcessor = {
  name: 'content-moderator',

  async processInput({ messages, abort }) {
    for (const message of messages) {
      if (containsInappropriateContent(message)) {
        abort('Content violates policy');
      }
    }
    return messages;
  }
};

const result = await agent.stream(messages, {
  inputProcessors: [moderator]
});
```

## OutputProcessor

An OutputProcessor processes agent output during streaming or after completion.

```typescript
type OutputProcessor =
  | (WithRequired<Processor, 'name' | 'processOutputStream'> & Processor)
  | (WithRequired<Processor, 'name' | 'processOutputResult'> & Processor);
```

Must implement at least one of:
- `processOutputStream` - Process chunks during streaming
- `processOutputResult` - Process final output after completion

<PropertiesTable
  content={[
    {
      name: "name",
      type: "string",
      description: "Unique identifier for this processor",
    },
    {
      name: "processOutputStream",
      type: "(args) => Promise<ChunkType | null | undefined>",
      isOptional: true,
      description: "Process streaming chunks in real-time. Return null to skip chunk.",
    },
    {
      name: "processOutputResult",
      type: "(args) => Promise<MastraMessageV2[]> | MastraMessageV2[]",
      isOptional: true,
      description: "Process complete output after generation",
    },
  ]}
/>

### processOutputStream Arguments

<PropertiesTable
  content={[
    {
      name: "part",
      type: "ChunkType",
      description: "Current streaming chunk",
    },
    {
      name: "streamParts",
      type: "ChunkType[]",
      description: "All chunks received so far (including current)",
    },
    {
      name: "state",
      type: "Record<string, any>",
      description: "Persistent state object shared across chunks",
    },
    {
      name: "abort",
      type: "(reason?: string) => never",
      description: "Function to abort execution",
    },
    {
      name: "tracingContext",
      type: "TracingContext",
      isOptional: true,
      description: "Tracing context for observability",
    },
  ]}
/>

### processOutputResult Arguments

<PropertiesTable
  content={[
    {
      name: "messages",
      type: "MastraMessageV2[]",
      description: "Final output messages from agent",
    },
    {
      name: "abort",
      type: "(reason?: string) => never",
      description: "Function to abort execution",
    },
    {
      name: "tracingContext",
      type: "TracingContext",
      isOptional: true,
      description: "Tracing context for observability",
    },
  ]}
/>

### Example (Stream Processing)

```typescript
import type { OutputProcessor } from '@mastra/core';

const piiRedactor: OutputProcessor = {
  name: 'pii-redactor',

  async processOutputStream({ part, state, abort }) {
    if (part.type === 'text-delta') {
      // Track total text in state
      state.totalText = (state.totalText || '') + part.payload.text;

      // Redact PII
      const redacted = redactPII(part.payload.text);
      return {
        ...part,
        payload: { ...part.payload, text: redacted }
      };
    }
    return part;
  }
};

const stream = await agent.stream(messages, {
  outputProcessors: [piiRedactor]
});
```

<Callout type="info">
The `state` object persists across all chunks in a stream, enabling stateful processing.
</Callout>

### Example (Result Processing)

```typescript
const validator: OutputProcessor = {
  name: 'output-validator',

  async processOutputResult({ messages, abort }) {
    const lastMessage = messages[messages.length - 1];

    if (!isValid(lastMessage)) {
      abort('Invalid output format');
    }

    return messages;
  }
};

const result = await agent.generate(messages, {
  outputProcessors: [validator]
});
```

## Using Processors with Agents

Configure processors when calling agent methods:

```typescript
const result = await agent.stream(messages, {
  inputProcessors: [moderationProcessor, piiDetector],
  outputProcessors: [structuredOutputProcessor]
});
```

### Execution Order

1. **Input processors** run sequentially before LLM call
2. **Output stream processors** run for each chunk during streaming
3. **Output result processors** run after streaming/generation completes

### Error Handling

Use `abort()` to stop execution:

```typescript
if (violatesPolicy(content)) {
  abort('Content policy violation detected');
}
```

The abort function throws a `TripWire` error that propagates to the caller.

## Built-in Processors

Mastra provides built-in processor implementations:

- **ModerationProcessor** - Content moderation with configurable categories
- **PIIDetector** - PII detection and redaction
- **PromptInjectionDetector** - Detect prompt injection attempts
- **LanguageDetector** - Language detection and translation
- **StructuredOutputProcessor** - Convert output to JSON
- **TokenLimiterProcessor** - Limit token count in messages
- **UnicodeNormalizer** - Normalize Unicode characters
- **SystemPromptScrubber** - Detect system prompt leakage
- **BatchPartsProcessor** - Batch stream chunks for efficiency

## Creating Custom Processors

Implement the `Processor` interface:

```typescript
import type { InputProcessor } from '@mastra/core';

class CustomProcessor implements InputProcessor {
  readonly name = 'custom-processor';

  async processInput({ messages, abort }) {
    // Your logic here
    return messages;
  }
}

const processor = new CustomProcessor();
const result = await agent.stream(messages, {
  inputProcessors: [processor]
});
```

<Callout type="warning">
Processors run synchronously in the execution pipeline. Avoid expensive operations that impact latency.
</Callout>
