---
title: "Reference: createLLMScorer | Scorers | Mastra Docs"
description: Documentation for creating LLM-based scorers in Mastra, allowing users to define evaluation logic using language models.
---

# LLM Scorer

The `createLLMScorer()` function lets you define custom scorers that use a language model (LLM) as a judge for evaluation. LLM scorers are ideal for tasks where you want to use prompt-based evaluation, such as answer relevancy, faithfulness, or custom prompt-based metrics. LLM scorers integrate seamlessly with the Mastra scoring framework and can be used anywhere built-in scorers are used.

For a usage example, see the [Custom LLM Judge Examples](/examples/scorers/custom-llm-judge-eval).

## createLLMScorer Options

<PropertiesTable
  content={[
    {
      name: "name",
      type: "string",
      required: true,
      description: "Name of the scorer.",
    },
    {
      name: "description",
      type: "string",
      required: true,
      description: "Description of what the scorer does.",
    },
    {
      name: "judge",
      type: "object",
      required: true,
      description: "Judge configuration object. Must include a model and instructions (system prompt). See Judge Object section below.",
    },
    {
      name: "extract",
      type: "object",
      required: false,
      description: "(Optional) Extraction step configuration object. See Extract Object section below.",
    },
    {
      name: "analyze",
      type: "object",
      required: true,
      description: "Analysis step configuration object. See Analyze Object section below.",
    },
    {
      name: "reason",
      type: "object",
      required: false,
      description: "(Optional) Reason step configuration object. See Reason Object section below.",
    },
    {
      name: "calculateScore",
      type: "function",
      required: true,
      description: "Function: ({ run }) => number. Computes the final score from the analyze step result.",
    },
  ]}
/>

This function returns an instance of the MastraScorer class. See the [MastraScorer reference](./mastra-scorer) for details on the `.run()` method and its input/output.

## Judge Object
<PropertiesTable
  content={[
    {
      name: "model",
      type: "LanguageModel",
      required: true,
      description: "The LLM model instance to use for evaluation.",
    },
    {
      name: "instructions",
      type: "string",
      required: true,
      description: "System prompt/instructions for the LLM.",
    },
  ]}
/>

## Extract Object
<PropertiesTable
  content={[
    {
      name: "description",
      type: "string",
      required: true,
      description: "Description of the extract step.",
    },
    {
      name: "judge",
      type: "object",
      required: false,
      description: "(Optional) LLM judge for this step (can override main judge/model). See Judge Object section.",
    },
    {
      name: "outputSchema",
      type: "ZodSchema",
      required: true,
      description: "Zod schema for the expected output of the extract step.",
    },
    {
      name: "createPrompt",
      type: "function",
      required: true,
      description: "Function: ({ run: ScoringInput }) => string. Returns the prompt for the LLM.",
    },
  ]}
/>

## Analyze Object
<PropertiesTable
  content={[
    {
      name: "description",
      type: "string",
      required: true,
      description: "Description of the analyze step.",
    },
    {
      name: "judge",
      type: "object",
      required: false,
      description: "(Optional) LLM judge for this step (can override main judge/model). See Judge Object section.",
    },
    {
      name: "outputSchema",
      type: "ZodSchema",
      required: true,
      description: "Zod schema for the expected output of the analyze step.",
    },
    {
      name: "createPrompt",
      type: "function",
      required: true,
      description: "Function: ({ run: ScoringInput & { extractStepResult } }) => string. Returns the LLM prompt.",
    },
  ]}
/>

## Reason Object
<PropertiesTable
  content={[
    {
      name: "description",
      type: "string",
      required: true,
      description: "Description of the reason step.",
    },
    {
      name: "judge",
      type: "object",
      required: false,
      description: "(Optional) LLM judge for this step (can override main judge/model). See Judge Object section.",
    },
    {
      name: "createPrompt",
      type: "function",
      required: true,
      description: "Function: ({ run }) => string. `run` includes input, output, extractStepResult, analyzeStepResult, and score. Returns the prompt for the LLM.",
    },
  ]}
/>

LLM scorers may also include step-specific prompt fields in the return value, such as `extractPrompt`, `analyzePrompt`, and `reasonPrompt`.

