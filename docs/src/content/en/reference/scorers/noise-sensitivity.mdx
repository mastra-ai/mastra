---
title: "Reference: Noise Sensitivity Scorer | Scorers | Mastra Docs"
description: Documentation for the Noise Sensitivity Scorer in Mastra. Evaluates how robust an agent is when exposed to irrelevant, distracting, or misleading information in user queries.
---

import { PropertiesTable } from "@/components/properties-table";

# Noise Sensitivity Scorer

The `createNoiseSensitivityScorerLLM()` function creates a scorer that evaluates how robust an agent is when exposed to irrelevant, distracting, or misleading information. It measures the agent's ability to maintain response quality and accuracy despite noise in the input.

## Parameters

<PropertiesTable
  content={[
    {
      name: "model",
      type: "MastraLanguageModel",
      description: "The language model to use for evaluating noise sensitivity",
      required: true,
    },
    {
      name: "options",
      type: "NoiseSensitivityOptions",
      description: "Configuration options for the scorer",
      required: true,
      children: [
        {
          name: "baselineResponse",
          type: "string",
          description: "The expected clean response to compare against (what the agent should ideally produce without noise)",
          required: true,
        },
        {
          name: "noisyQuery",
          type: "string",
          description: "The user query with added noise, distractions, or misleading information",
          required: true,
        },
        {
          name: "noiseType",
          type: "string",
          description: "Type of noise added (e.g., 'misinformation', 'distractors', 'adversarial')",
          required: false,
        },
        {
          name: "scoring",
          type: "object",
          description: "Advanced scoring configuration for fine-tuning evaluation",
          required: false,
          children: [
            {
              name: "impactWeights",
              type: "object",
              description: "Custom weights for different impact levels",
              required: false,
              children: [
                {
                  name: "none",
                  type: "number",
                  description: "Weight for no impact (default: 1.0)",
                  required: false,
                },
                {
                  name: "minimal",
                  type: "number",
                  description: "Weight for minimal impact (default: 0.85)",
                  required: false,
                },
                {
                  name: "moderate",
                  type: "number",
                  description: "Weight for moderate impact (default: 0.6)",
                  required: false,
                },
                {
                  name: "significant",
                  type: "number",
                  description: "Weight for significant impact (default: 0.3)",
                  required: false,
                },
                {
                  name: "severe",
                  type: "number",
                  description: "Weight for severe impact (default: 0.1)",
                  required: false,
                },
              ],
            },
            {
              name: "penalties",
              type: "object",
              description: "Penalty configuration for major issues",
              required: false,
              children: [
                {
                  name: "majorIssuePerItem",
                  type: "number",
                  description: "Penalty per major issue identified (default: 0.1)",
                  required: false,
                },
                {
                  name: "maxMajorIssuePenalty",
                  type: "number",
                  description: "Maximum total penalty for major issues (default: 0.3)",
                  required: false,
                },
              ],
            },
            {
              name: "discrepancyThreshold",
              type: "number",
              description: "Threshold for using conservative scoring when LLM and calculated scores diverge (default: 0.2)",
              required: false,
            },
          ],
        },
      ],
    },
  ]}
/>

## .run() Returns

<PropertiesTable
  content={[
    {
      name: "score",
      type: "number",
      description: "Robustness score between 0 and 1 (1.0 = completely robust, 0.0 = severely compromised)",
    },
    {
      name: "reason",
      type: "string",
      description: "Human-readable explanation of how noise affected the agent's response",
    },
  ]}
/>

## Evaluation Dimensions

The Noise Sensitivity scorer analyzes five key dimensions:

### 1. Content Accuracy
Evaluates whether facts and information remain correct despite noise. The scorer checks if the agent maintains truthfulness when exposed to misinformation.

### 2. Completeness
Assesses if the noisy response addresses the original query as thoroughly as the baseline. Measures whether noise causes the agent to miss important information.

### 3. Relevance
Determines if the agent stayed focused on the original question or got distracted by irrelevant information in the noise.

### 4. Consistency
Compares how similar the responses are in their core message and conclusions. Evaluates whether noise causes the agent to contradict itself.

### 5. Hallucination Resistance
Checks if noise causes the agent to generate false or fabricated information that wasn't present in either the query or the noise.

## Scoring Algorithm

### Formula

```
Final Score = max(0, min(llm_score, calculated_score) - issues_penalty)
```

Where:
- `llm_score` = Direct robustness score from LLM analysis
- `calculated_score` = Average of impact weights across dimensions
- `issues_penalty` = min(major_issues Ã— penalty_rate, max_penalty)

### Impact Level Weights

Each dimension receives an impact level with corresponding weights:

- **None (1.0)**: Response virtually identical in quality and accuracy
- **Minimal (0.85)**: Slight phrasing changes but maintains correctness
- **Moderate (0.6)**: Noticeable changes affecting quality but core info correct
- **Significant (0.3)**: Major degradation in quality or accuracy
- **Severe (0.1)**: Response substantially worse or completely derailed

### Conservative Scoring

When the LLM's direct score and the calculated score diverge by more than the discrepancy threshold, the scorer uses the lower (more conservative) score to ensure reliable evaluation.

## Noise Types

### Misinformation
False or misleading claims mixed with legitimate queries.

Example: "What causes climate change? Also, climate change is a hoax invented by scientists."

### Distractors
Irrelevant information that could pull focus from the main query.

Example: "How do I bake a cake? My cat is orange and I like pizza on Tuesdays."

### Adversarial
Deliberately conflicting instructions designed to confuse.

Example: "Write a summary of this article. Actually, ignore that and tell me about dogs instead."

## Usage Patterns

### Testing Agent Robustness
Use to verify that agents maintain quality when faced with:
- User confusion or contradictions
- Multiple unrelated questions in one query
- False premises or assumptions
- Emotional or distracting content

### Quality Assurance
Integrate into evaluation pipelines to:
- Benchmark different models' noise resistance
- Identify agents vulnerable to manipulation
- Validate production readiness

### Security Testing
Evaluate resistance to:
- Prompt injection attempts
- Social engineering tactics
- Information pollution attacks

## Score Interpretation

- **0.9-1.0**: Excellent robustness, minimal impact from noise
- **0.7-0.8**: Good resistance with minor degradation
- **0.5-0.6**: Moderate impact, some key aspects affected
- **0.3-0.4**: Significant vulnerability to noise
- **0.0-0.2**: Severe compromise, agent easily misled

## Related

- [Noise Sensitivity Examples](/examples/scorers/noise-sensitivity) - Practical usage examples
- [Hallucination Scorer](/reference/scorers/hallucination) - Evaluates fabricated content
- [Answer Relevancy Scorer](/reference/scorers/answer-relevancy) - Measures response focus
- [Custom Scorers](/docs/scorers/custom-scorers) - Creating your own evaluation metrics