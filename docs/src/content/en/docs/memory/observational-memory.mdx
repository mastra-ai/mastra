---
title: "Observational Memory | Memory"
description: "Learn how Observational Memory keeps your agent's context window small while preserving long-term memory across conversations."
packages:
  - "@mastra/core"
  - "@mastra/memory"
---

# Observational Memory

Observational Memory (OM) is Mastra's memory system for long-context agentic memory. Two background agents â€” an **Observer** and a **Reflector** â€” watch your agent's conversations and maintain a dense observation log that replaces raw message history as it grows.

## Quick Start

```typescript title="src/mastra/agents/agent.ts"
import { Memory } from "@mastra/memory";
import { Agent } from "@mastra/core/agent";

export const agent = new Agent({
  name: "my-agent",
  instructions: "You are a helpful assistant.",
  model: "openai/gpt-5-mini",
  memory: new Memory({
    options: {
      observationalMemory: true,
    },
  }),
});
```

That's it. The agent now has humanlike long-term memory that persists across conversations.

> **Note:** Observational Memory currently only supports `@mastra/pg`, `@mastra/libsql`, and `@mastra/mongodb` storage adapters.

## How It Works

You don't remember every word of every conversation you've ever had. You observe what happened subconsciously, then your brain reflects â€” reorganizing, combining, and condensing into long-term memory. OM works the same way.

Every time an agent responds, it sees a context window containing its system prompt, recent message history, and any injected context (tool results, semantic recall, working memory, etc). The context window is finite â€” even models with large token limits perform worse when the window is full. This causes two problems in long conversations:

- **Context rot**: the more raw message history an agent carries, the worse it performs. Responses get less focused, instructions get ignored, and important details get lost in the noise.
- **Context waste**: most of that history contains tokens which are no longer needed to keep the agent on task. Ten 50k-token tool call results can usually be represented as 500 tokens of observations instead of 500,000 tokens of raw output.

### Message history

Raw message history is the space dedicated to the current task. It contains the exact back-and-forth of the conversation: what the user said, what the agent responded, tool calls and results.

This is high-fidelity but expensive. A 50-message conversation could use 100,000+ tokens of context, most of which contain tokens no longer needed to keep the agent on task.

### Observations as long-term memory

When message history tokens exceed a threshold (default: 30,000 tokens), the Observer is called. It watches the conversation and makes observations â€” a list of concise, dense notes about what actually happened:

```
Date: 2026-01-15
- ðŸ”´ 12:10 User is building a Next.js app with Supabase auth, due in 1 week (meaning January 22nd 2026 - 1 week from now)
  - ðŸ”´ 12:10 App uses server components with client-side hydration
  - ðŸŸ¡ 12:12 User asked about middleware configuration for protected routes
  - ðŸŸ¡ 12:13 Agent provided middleware setup using createServerClient pattern
  - ðŸ”´ 12:15 User stated the app name is "Acme Dashboard"
```

New observations are appended to any previous observations, replacing the messages they observed. The compression is typically between 5x and 40x â€” instead of 30,000 tokens of back-and-forth, the agent sees a compact list of observations about what happened. The more tool calls there are, the higher the compression ratio typically is.

#### Observation formatting

Observations are output as raw appendable text. They're structured as a bullet point list, with indentation to represent grouped observations, emojis to represent information priority, and timestamps to indicate when the observations occurred.

The system injects temporal markers automatically. When the Observer references relative time (e.g., "the user plans to do X in a week"), the system converts it to an absolute date and adds context like "(meaning June 4th 2027 â€” 4 months from today)" or "(meaning Jan 2025 - 1 year ago)".

#### Task and response extraction

The Observer tracks a **current task** (what the agent is working on right now) and a **suggested response** (how the agent should respond next to maintain conversation continuity, since message history is being replaced). These are injected into the agent's context alongside the observations, so the agent picks up where it left off even when large portions of the message history are replaced.

### Reflections

As observations accumulate over time, they eventually hit their own threshold (default: 40,000 tokens). The Reflector then restructures them â€” combining related items, reflecting on overarching patterns, and condensing where possible. Reflections always completely replace all previous observations and are intended to condense the size of observations to save context window space.

If the Reflector's output isn't smaller than the input, it's automatically run a second time with a tweaked prompt that instructs it to condense more aggressively.

The result is a three-tier system:

1. **Recent messages** â€” exact conversation history for the current task
2. **Observations** â€” a log of what the Observer has seen
3. **Reflections** â€” condensed, reorganized observations when memory becomes too long

Each tier is progressively refined to drop context rot/waste and gracefully degrade old irrelevant memories.

### Example: browser automation

Consider an agent using Playwright MCP to automate a browser. Each page snapshot might be 50,000+ tokens. After a few pages, the context window is full of stale snapshots that actively hurt the agent's ability to follow instructions.

With OM, when the snapshot comes in, the Observer watches the interaction and makes observations about what the page contained and what the agent did with it. The agent then continues with a concise understanding of what happened â€” not the raw 50k snapshot, but a few hundred tokens of observations about what was on the page and what actions were taken. The agent stays on task with full context of what it's seen, without carrying the weight of every raw snapshot.

This applies to any token-heavy workflow: document analysis, coding sessions, research tasks, multi-step tool use, even long text conversations.

## Scopes

Observational Memory supports two scopes:

### Thread scope (default)

Observations are kept per-conversation. Each thread has its own observations. This is the simplest mode â€” each conversation is independent.

```typescript
const memory = new Memory({
  options: {
    observationalMemory: {
      scope: "thread",
    },
  },
});
```

### Resource scope (use carefully)

Observations are shared across all threads for a resource (typically a user). This enables cross-conversation memory â€” the agent remembers what happened in previous conversations with the same user.

```typescript
const memory = new Memory({
  options: {
    observationalMemory: {
      scope: "resource",
    },
  },
});
```

**Understand what this does before enabling it.** In resource scope, the context window for each thread is a perspective on *all* threads simultaneously. Any unobserved messages across all threads are in context. When the total unobserved tokens across threads meets the threshold, observations are created (separated by thread in context).

This means if a user has 100 threads of unobserved messages, the first message sent will cause the Observer to block the response and iteratively chunk through every unobserved message across all threads until the context is small enough to fit. For users with many existing threads, this initial observation pass could take significant time.

For existing apps, it will be easier to use thread scope.

## Token Budgets

Observational Memory uses token thresholds to decide when to observe and when to reflect.

### Message tokens

`observation.messageTokens` (default: 30,000) controls when the Observer runs. When unobserved message tokens exceed this value, the Observer is called.

Lower values mean the Observer runs more frequently, but it also tends to capture more fine details since it's observing smaller batches of messages. This often leads to quicker reflection too, since more observations accumulate faster. Higher values let more raw history accumulate before observing.

```typescript
const memory = new Memory({
  options: {
    observationalMemory: {
      observation: {
        messageTokens: 15_000,
      },
    },
  },
});
```

### Batch size

`observation.maxTokensPerBatch` (default: 10,000) controls how many tokens the Observer processes at once when there are multiple threads in resource scope.

This is independent of `messageTokens`. For example, you might want to observe at 20,000 message tokens but still use 5,000 token batches â€” the Observer will notice more details in each smaller batch rather than processing everything at once.

```typescript
const memory = new Memory({
  options: {
    observationalMemory: {
      observation: {
        messageTokens: 20_000,
        maxTokensPerBatch: 5_000,
      },
    },
  },
});
```

### Observation tokens

`reflection.observationTokens` (default: 40,000) controls when the Reflector runs. When observation tokens exceed this value, the Reflector condenses them.

```typescript
const memory = new Memory({
  options: {
    observationalMemory: {
      reflection: {
        observationTokens: 20_000,
      },
    },
  },
});
```

### Shared token budget

By default, message tokens and observation tokens have independent thresholds. With `shareTokenBudget: true`, message history can borrow from the observation token budget:

```typescript
const memory = new Memory({
  options: {
    observationalMemory: {
      shareTokenBudget: true,
      observation: {
        messageTokens: 20_000,
      },
      reflection: {
        observationTokens: 44_000,
      },
    },
  },
});
```

The total budget here is 64,000 tokens (20k + 44k). While observations are still small, message history can expand beyond its 20k threshold into the unused observation space. This lets you choose a rough max context window size and stay within it â€” if you want to stay under 64k tokens total, this configuration does that while still allowing longer message history when observations haven't accumulated yet.

## Models

The Observer and Reflector are agents that run in the background. Any model supported by the [Model Router](/reference/models/model-router) works.

### Default model

The default is `google/gemini-2.5-flash`. This is what we've tested with the most internally and it works well for both observation and reflection. Gemini Flash's 1M token context window gives the Reflector headroom when reflecting on large amounts of observations.

### Alternative models

We've also been testing the system with `deepseek`, `qwen3`, and `glm-4.7` for the Observer and they work well. For the Reflector, make sure the model's context window has enough headroom to fit all observations â€” Gemini Flash's 1M context window is a safe choice. If your `reflection.observationTokens` threshold is lower (e.g., 20k-40k), context window size matters less and smaller models work fine.
Note that Claude 4.5 models currently do not work well as observer or reflector - something about their training seems to prevent them from fully observing all message history correctly.

Set a single model for both agents:

```typescript
const memory = new Memory({
  options: {
    observationalMemory: {
      model: "deepseek/deepseek-reasoner",
    },
  },
});
```

Or use different models for each:

```typescript
const memory = new Memory({
  options: {
    observationalMemory: {
      observation: {
        model: "deepseek/deepseek-chat",
      },
      reflection: {
        model: "deepseek/deepseek-reasoner",
      },
    },
  },
});
```

## Viewing in Mastra Studio

Mastra Studio shows Observational Memory status in real time. When an observation or reflection is in progress, you'll see:

- Token usage progress bars for messages and observations
- Which model is running (Observer or Reflector)
- The current observations and history of past reflections

This is useful for understanding how OM is behaving and tuning thresholds for your use case.

## Related

- [Observational Memory Reference](/reference/memory/observational-memory) â€” configuration options and API
- [Memory Overview](/docs/memory/overview)
- [Message History](/docs/memory/message-history)
- [Memory Processors](/docs/memory/memory-processors)
- [Processors](/docs/agents/processors)
