---
title: "Textual Evals | Scorers"
description: "Understand how Mastra uses LLM-as-judge methodology to evaluate text quality."
---

# Textual Evals

:::info Scorers
This documentation refers to the legacy evals API. For the latest scorer features, see [Scorers](/docs/v0/scorers/overview).
:::

Textual evals use an LLM-as-judge methodology to evaluate agent outputs. This approach leverages language models to assess various aspects of text quality, similar to how a teaching assistant might grade assignments using a rubric.

Each eval focuses on specific quality aspects and returns a score between 0 and 1, providing quantifiable metrics for non-deterministic AI outputs.

Mastra provides several eval metrics for assessing Agent outputs. Mastra is not limited to these metrics, and you can also [define your own evals](/docs/v0/scorers/evals-legacy/custom-eval).

## Why use textual evals?

Textual evals help ensure your agent:

- Produces accurate and reliable responses
- Uses context effectively
- Follows output requirements
- Maintains consistent quality over time

## Available metrics

### Accuracy and reliability

These metrics evaluate how correct, truthful, and complete your agent's answers are:

- [`hallucination`](/reference/v0/evals/hallucination): Detects facts or claims not present in provided context
- [`faithfulness`](/reference/v0/evals/faithfulness): Measures how accurately responses represent provided context
- [`content-similarity`](/reference/v0/evals/content-similarity): Evaluates consistency of information across different phrasings
- [`completeness`](/reference/v0/evals/completeness): Checks if responses include all necessary information
- [`answer-relevancy`](/reference/v0/evals/answer-relevancy): Assesses how well responses address the original query
- [`textual-difference`](/reference/v0/evals/textual-difference): Measures textual differences between strings

### Understanding context

These metrics evaluate how well your agent uses provided context:

- [`context-position`](/reference/v0/evals/context-position): Analyzes where context appears in responses
- [`context-precision`](/reference/v0/evals/context-precision): Evaluates whether context chunks are grouped logically
- [`context-relevancy`](/reference/v0/evals/context-relevancy): Measures use of appropriate context pieces
- [`contextual-recall`](/reference/v0/evals/contextual-recall): Assesses completeness of context usage

### Output quality

These metrics evaluate adherence to format and style requirements:

- [`tone`](/reference/v0/evals/tone-consistency): Measures consistency in formality, complexity, and style
- [`toxicity`](/reference/v0/evals/toxicity): Detects harmful or inappropriate content
- [`bias`](/reference/v0/evals/bias): Detects potential biases in the output
- [`prompt-alignment`](/reference/v0/evals/prompt-alignment): Checks adherence to explicit instructions like length restrictions, formatting requirements, or other constraints
- [`summarization`](/reference/v0/evals/summarization): Evaluates information retention and conciseness
- [`keyword-coverage`](/reference/v0/evals/keyword-coverage): Assesses technical terminology usage
