---
title: "Agent Overview | Agents | Mastra Docs"
description: Overview of agents in Mastra, detailing their capabilities and how they interact with tools, workflows, and external systems.
---

import { Steps, Callout, Tabs } from "nextra/components";

# Using Agents

Agents use LLMs and tools to solve open-ended tasks. They reason about goals, decide which tools to use, retain conversation memory, and iterate internally until the model emits a final answer or an optional stop condition is met. Agents produce structured responses you can render in your UI or process programmatically. Use agents directly or compose them into workflows or agent networks.

![Agents overview](/image/agents/agents-overview.jpg)

> **ðŸ“¹ Watch**:  â†’ An introduction to agents, and how they compare to workflows [YouTube (7 minutes)](https://youtu.be/0jg2g3sNvgw)

## Setting up agents

<Tabs items={["Mastra model router", "Vercel AI SDK"]}>
  <Tabs.Tab>
    <Steps>
### Install dependencies [#install-dependencies-mastra-router]

Add the Mastra core package to your project:

```bash
npm install @mastra/core
```

### Set your API key [#set-api-key-mastra-router]

Mastra's model router auto-detects environment variables for your chosen provider. For OpenAI, set `OPENAI_API_KEY`:

```bash filename=".env" copy
OPENAI_API_KEY=<your-api-key>
```

> Mastra supports more than 600 models. Choose from the full list [here](/models).

### Creating an agent [#creating-an-agent-mastra-router]

Create an agent by instantiating the `Agent` class with system `instructions` and a `model`:

```typescript filename="src/mastra/agents/test-agent.ts" showLineNumbers copy
import { Agent } from "@mastra/core/agent";

export const testAgent = new Agent({
  name: "test-agent",
  instructions: "You are a helpful assistant.",
  model: "openai/gpt-4o-mini"
});
```
    </Steps>
  </Tabs.Tab>
  <Tabs.Tab>
    <Steps>

### Install dependencies [#install-dependencies-ai-sdk]

Include the Mastra core package alongside the Vercel AI SDK provider you want to use:

```bash
npm install @mastra/core @ai-sdk/openai
```

### Set your API key [#set-api-key-ai-sdk]

Set the corresponding environment variable for your provider. For OpenAI via the AI SDK:

```bash filename=".env" copy
OPENAI_API_KEY=<your-api-key>
```

> See the [AI SDK Providers](https://ai-sdk.dev/providers/ai-sdk-providers) in the Vercel AI SDK docs for additional configuration options.

### Creating an agent [#creating-an-agent-ai-sdk]

To create an agent in Mastra, use the `Agent` class. Every agent must include `instructions` to define its behavior, and a `model` parameter to specify the LLM provider and model. When using the Vercel AI SDK, provide the client to your agent's `model` field:

```typescript filename="src/mastra/agents/test-agent.ts" showLineNumbers copy
import { openai } from "@ai-sdk/openai";
import { Agent } from "@mastra/core/agent";

export const testAgent = new Agent({
  name: "test-agent",
  instructions: "You are a helpful assistant.",
  model: openai("gpt-4o-mini")
});
```
    </Steps>
  </Tabs.Tab>
</Tabs>

#### Instruction formats

Instructions define the agent's behavior, personality, and capabilities.
They are system-level prompts that establish the agent's core identity and expertise.

Instructions can be provided in multiple formats for greater flexibility. The examples below illustrate the supported shapes:

```typescript copy
// String (most common)
instructions: "You are a helpful assistant."

// Array of strings
instructions: [
  "You are a helpful assistant.",
  "Always be polite.",
  "Provide detailed answers."
]

// Array of system messages
instructions: [
  { role: "system", content: "You are a helpful assistant." },
  { role: "system", content: "You have expertise in TypeScript." }
]
```

#### Provider-specific options

Each model provider also enables a few different options, including prompt caching and configuring reasoning. We provide a `providerOptions` flag to manage these. You can set `providerOptions` on the instruction level to set different caching strategy per system instruction/prompt.

```typescript copy
// With provider-specific options (e.g., caching, reasoning)
instructions: {
  role: "system",
  content:
    "You are an expert code reviewer. Analyze code for bugs, performance issues, and best practices.",
  providerOptions: {
    openai: { reasoningEffort: "high" },        // OpenAI's reasoning models
    anthropic: { cacheControl: { type: "ephemeral" } }  // Anthropic's prompt caching
  }
}
```

> See the [Agent reference doc](../../reference/agents/agent.mdx) for more information.

#### Model settings

Control the behavior of your agent's model by configuring parameters like `temperature`, `topP`, `maxTokens`, and more. These settings can be defined as defaults when creating the agent, ensuring consistent behavior across all invocations.

##### Static configuration

Set default model settings that apply to all agent calls:

```typescript copy
import { Agent } from "@mastra/core/agent";

export const creativeAgent = new Agent({
  name: "creative-agent",
  instructions: "You are a creative writing assistant.",
  model: "openai/gpt-4o",
  
  // For generate() and stream() methods (AI SDK v5)
  defaultVNextStreamOptions: {
    modelSettings: {
      temperature: 0.9,        // Higher temperature for more creative responses
      topP: 0.95,              // Nucleus sampling parameter
      maxTokens: 2000,         // Maximum tokens in response
      frequencyPenalty: 0.5,   // Reduce repetition
      presencePenalty: 0.3,    // Encourage topic diversity
    }
  }
});
```

For legacy methods (`generateLegacy()` and `streamLegacy()` using AI SDK v4):

```typescript copy
export const preciseAgent = new Agent({
  name: "precise-agent",
  instructions: "You are a precise and factual assistant.",
  model: "openai/gpt-4o",
  
  // For generateLegacy() method
  defaultGenerateOptions: {
    temperature: 0.3,
    topP: 0.8,
    maxTokens: 1000,
  },
  
  // For streamLegacy() method
  defaultStreamOptions: {
    temperature: 0.3,
    topP: 0.8,
    maxTokens: 1000,
  }
});
```

##### Dynamic configuration

Adjust model settings based on runtime context for more flexible behavior:

```typescript copy
export const adaptiveAgent = new Agent({
  name: "adaptive-agent",
  instructions: "You are an adaptive assistant.",
  model: "openai/gpt-4o",
  
  defaultVNextStreamOptions: ({ runtimeContext }) => {
    const mode = runtimeContext.get("mode");
    
    // Different settings based on user preference
    const settings = {
      creative: { temperature: 0.9, topP: 0.95, maxTokens: 2000 },
      balanced: { temperature: 0.7, topP: 0.9, maxTokens: 1500 },
      precise: { temperature: 0.3, topP: 0.8, maxTokens: 1000 },
    };
    
    return {
      modelSettings: settings[mode] || settings.balanced
    };
  }
});

// Usage with runtime context
const response = await adaptiveAgent.generate("Write a story", {
  runtimeContext: new RuntimeContext({ mode: "creative" })
});
```

##### Available model settings

| Setting | Type | Description |
|---------|------|-------------|
| `temperature` | `number` | Controls randomness (0-2). Lower values make output more focused and deterministic, higher values make it more creative and varied. |
| `topP` | `number` | Nucleus sampling parameter (0-1). Controls diversity by only sampling from tokens whose cumulative probability is above this threshold. |
| `maxTokens` | `number` | Maximum number of tokens to generate in the response. |
| `frequencyPenalty` | `number` | Penalizes tokens based on their frequency in the text so far (âˆ’2 to 2). Reduces repetition. |
| `presencePenalty` | `number` | Penalizes tokens based on whether they appear in the text so far (âˆ’2 to 2). Encourages new topics. |
| `seed` | `number` | Random seed for deterministic sampling. Using the same seed should produce similar outputs. |

<Callout type="info">
  Model settings defined in `defaultVNextStreamOptions`, `defaultGenerateOptions`, or `defaultStreamOptions` are applied automatically to every agent call. You can still override these on individual calls if needed.
</Callout>

### Registering an agent

Register your agent in the Mastra instance to make it available throughout your application. Once registered, it can be called from workflows, tools, or other agents, and has access to shared resources such as memory, logging, and observability features:

```typescript {6} showLineNumbers filename="src/mastra/index.ts" copy
import { Mastra } from "@mastra/core/mastra";
import { testAgent } from './agents/test-agent';

export const mastra = new Mastra({
  // ...
  agents: { testAgent },
});
```

## Referencing an agent

You can call agents from workflow steps, tools, the Mastra Client, or the command line. Get a reference by calling `.getAgent()` on your `mastra` or `mastraClient` instance, depending on your setup:

```typescript showLineNumbers copy
const testAgent = mastra.getAgent("testAgent");
```
<Callout type="info">
  <p>
    `mastra.getAgent()` is preferred over a direct import, since it provides access to the Mastra instance configuration (logger, telemetry, storage, registered agents, and vector stores).
  </p>
</Callout>

> See [Calling agents](../../examples/agents/calling-agents.mdx) for more information.

## Generating responses

Agents can return results in two ways: generating the full output before returning it or streaming tokens in real time. Choose the approach that fits your use case: generate for short, internal responses or debugging, and stream to deliver pixels to end users as quickly as possible.

<Tabs items={["Generate", "Stream"]}>
  <Tabs.Tab>
Pass a single string for simple prompts, an array of strings when providing multiple pieces of context, or an array of message objects with `role` and `content`.

(The `role` defines the speaker for each message. Typical roles are `user` for human input, `assistant` for agent responses, and `system` for instructions.)

```typescript showLineNumbers copy
const response = await testAgent.generate([
  { role: "user", content: "Help me organize my day" },
  { role: "user", content: "My day starts at 9am and finishes at 5.30pm" },
  { role: "user", content: "I take lunch between 12:30 and 13:30" },
  { role: "user", content: "I have meetings Monday to Friday between 10:30 and 11:30" }
]);

console.log(response.text);
```
  </Tabs.Tab>
  <Tabs.Tab>
Pass a single string for simple prompts, an array of strings when providing multiple pieces of context, or an array of message objects with `role` and `content`.

(The `role` defines the speaker for each message. Typical roles are `user` for human input, `assistant` for agent responses, and `system` for instructions.)

```typescript showLineNumbers copy
const stream = await testAgent.stream([
  { role: "user", content: "Help me organize my day" },
  { role: "user", content: "My day starts at 9am and finishes at 5.30pm" },
  { role: "user", content: "I take lunch between 12:30 and 13:30" },
  { role: "user", content: "I have meetings Monday to Friday between 10:30 and 11:30" }
]);

for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}
```

### Completion using `onFinish()`

When streaming responses, the `onFinish()` callback runs after the LLM finishes generating its response and all tool executions are complete.
It provides the final `text`, execution `steps`, `finishReason`, token `usage` statistics, and other metadata useful for monitoring or logging.

```typescript showLineNumbers copy
const stream = await testAgent.stream("Help me organize my day", {
  onFinish: ({ steps, text, finishReason, usage }) => {
    console.log({ steps, text, finishReason, usage });
  }
});

for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}
```
  </Tabs.Tab>
</Tabs>

> See [.generate()](../../reference/agents/generate.mdx) or [.stream()](../../reference/agents/stream.mdx) for more information.

## Structured output

Agents can return structured, type-safe data by defining the expected output using either [Zod](https://zod.dev/) or [JSON Schema](https://json-schema.org/). We recommend Zod for better TypeScript support and developer experience. The parsed result is available on `response.object`, allowing you to work directly with validated and typed data.

### Using Zod

Define the `output` shape using [Zod](https://zod.dev/):

```typescript showLineNumbers copy
import { z } from "zod";

const response = await testAgent.generate(
  [
    {
      role: "system",
      content: "Provide a summary and keywords for the following text:"
    },
    {
      role: "user",
      content: "Monkey, Ice Cream, Boat"
    }
  ],
  {
    structuredOutput: {
      schema: z.object({
        summary: z.string(),
        keywords: z.array(z.string())
      })
    },
  }
);

console.log(response.object);
```

### With Tool Calling

Use the `model` property to ensure that your agent can execute multi-step LLM calls with tool calling.

```typescript showLineNumbers copy
import { z } from "zod";

const response = await testAgentWithTools.generate(
  [
    {
      role: "system",
      content: "Provide a summary and keywords for the following text:"
    },
    {
      role: "user",
      content: "Please use your test tool and let me know the results"
    }
  ],
  {
    structuredOutput: {
      schema: z.object({
        summary: z.string(),
        keywords: z.array(z.string())
      }),
      model: "openai/gpt-4o"
    },
  }
);

console.log(response.object);
console.log(response.toolResults)
```

### Response format

By default `structuredOutput` will use `response_format` to pass the schema to the model provider. If the model provider does not natively support `response_format` it's possible that this will error or not give the desired results. To keep using the same model use `jsonPromptInjection` to bypass response format and inject a system prompt message to coerce the model to return structured output.

```typescript showLineNumbers copy
import { z } from "zod";

const response = await testAgentThatDoesntSupportStructuredOutput.generate(
  [
    {
      role: "system",
      content: "Provide a summary and keywords for the following text:"
    },
    {
      role: "user",
      content: "Monkey, Ice Cream, Boat"
    }
  ],
  {
    structuredOutput: {
      schema: z.object({
        summary: z.string(),
        keywords: z.array(z.string())
      }),
      jsonPromptInjection: true
    },
  }
);

console.log(response.object);
```


## Working with images

Agents can analyze and describe images by processing both the visual content and any text within them. To enable image analysis, pass an object with `type: 'image'` and the image URL in the `content` array. You can combine image content with text prompts to guide the agent's analysis.

```typescript showLineNumbers copy
const response = await testAgent.generate([
  {
    role: "user",
    content: [
      {
        type: "image",
        image: "https://placebear.com/cache/395-205.jpg",
        mimeType: "image/jpeg"
      },
      {
        type: "text",
        text: "Describe the image in detail, and extract all the text in the image."
      }
    ]
  }
]);

console.log(response.text);
```

For a detailed guide to creating and configuring tools, see the [Tools Overview](../tools-mcp/overview.mdx) page.

### Using `maxSteps`

The `maxSteps` parameter controls the maximum number of sequential LLM calls an agent can make. Each step includes generating a response, executing any tool calls, and processing the result. Limiting steps helps prevent infinite loops, reduce latency, and control token usage for agents that use tools. The default is 1, but can be increased:

```typescript showLineNumbers copy
const response = await testAgent.generate("Help me organize my day", {
  maxSteps: 5
});

console.log(response.text);
```

### Using `onStepFinish`

You can monitor the progress of multi-step operations using the `onStepFinish` callback. This is useful for debugging or providing progress updates to users.

`onStepFinish` is only available when streaming or generating text without structured output.

```typescript showLineNumbers copy
const response = await testAgent.generate("Help me organize my day", {
  onStepFinish: ({ text, toolCalls, toolResults, finishReason, usage }) => {
    console.log({ text, toolCalls, toolResults, finishReason, usage });
  }
});
```

## Using tools

Agents can use tools to go beyond language generation, enabling structured interactions with external APIs and services. Tools allow agents to access data and perform clearly defined operations in a reliable, repeatable way.

```typescript filename="src/mastra/agents/test-agent.ts" showLineNumbers
export const testAgent = new Agent({
  // ...
  tools: { testTool }
});
```

> See [Using Tools](./using-tools.mdx) for more information.

## Using `RuntimeContext`

Use `RuntimeContext` to access request-specific values. This lets you conditionally adjust behavior based on the context of the request.

```typescript filename="src/mastra/agents/test-agent.ts" showLineNumbers
export type UserTier = {
  "user-tier": "enterprise" | "pro";
};

export const testAgent = new Agent({
  // ...
  model: ({ runtimeContext }) => {
    const userTier = runtimeContext.get("user-tier") as UserTier["user-tier"];

    return userTier === "enterprise"
      ? openai("gpt-4o-mini")
      : openai("gpt-4.1-nano");
  }
});
```

> See [Runtime Context](../server-db/runtime-context.mdx) for more information.

## Testing with Mastra Playground

Use the Mastra [Playground](../server-db/local-dev-playground.mdx) to test agents with different messages, inspect tool calls and responses, and debug agent behavior.

## Related

- [Using Tools](./using-tools.mdx)
- [Agent Memory](./agent-memory.mdx)
- [Runtime Context](../../examples/agents/runtime-context.mdx)
- [Calling Agents](../../examples/agents/calling-agents.mdx)
