# Building a Knowledge Base

Retrieval-Augmented Generation (RAG) enriches LLM outputs by retrieving relevant context from your own data. It typically involves:

1. **Document Ingestion & Processing**: Break large documents into smaller chunks, extract metadata, and optionally enhance content with summaries, keywords, and Q&A hints.
2. **Embedding**: Represent each chunk as a high-dimensional vector using an embedding model.
3. **Vector Storage**: Store embeddings in a vector database (e.g., PostgreSQL with pgvector, Pinecone, or Qdrant).
4. **Querying**: At runtime, embed the user's query, retrieve similar chunks from the vector store, and feed them as context to the model.

With Mastra's RAG utilities, you can orchestrate these steps easily.

## Step 0: Document Initialization

Before processing, create a `MastraDocument` instance from your content. You can initialize it from text, HTML, Markdown, or JSON:

```ts
const docFromText = MastraDocument.fromText('Your plain text content...');

const docFromHTML = MastraDocument.fromHTML('<html>Your HTML content...</html>');

const docFromMarkdown = MastraDocument.fromMarkdown('# Your Markdown content...');

const docFromJSON = MastraDocument.fromJSON(`{ "key": "value" }`);
```

Each method returns a `MastraDocument` ready for processing.

## Step 1: Document Processing

Then, use the `chunk` method to split the document into manageable pieces. You can also extract metadata like titles, summaries, and keywords.

**Example:**

```ts showLineNumbers copy
const chunks = doc.chunk({
  strategy: 'recursive',
  options: {
    chunkSize: 512,
    chunkOverlap: 50,
    separator: '\n',
  },
  extract: {
    title: true,
    summary: true,
    keywords: true,
    questions: true,
  },
});
```

This splits the document into chunks and extracts metadata. There are a variety of chunking strategies and options, which we go into in the [MastraDocument reference documentation](/docs/guide/reference/rag/document.mdx). Metadata is preserved in the chunks, which you can use for retrieval.

**Note:** Metadata extraction may use LLM calls behind the scenes, so ensure `OPENAI_API_KEY` is set.

## Step 2: Embedding

Generate embeddings for your chunks using the `embed` functions. This converts text into high-dimensional vectors suitable for similarity searches.

**Example:**

```ts showLineNumbers copy
const { embedding } = await doc.embed(chunks, {
  model: {
    provider: 'OPEN_AI',
    name: 'text-embedding-3-small',
  },
});
```

## Step 3: Vector Storage

Store embeddings in a vector database for efficient querying. Mastra supports various vector stores like **PgVector**, **Pinecone**, and **Qdrant**, each offering similar APIs.

**Example with PgVector:**

```ts showLineNumbers copy
import { PgVector } from '@mastra/rag';

await PgVector.upsert({
  connectionString: 'postgresql://localhost:5432/mydb',
  tableName: 'embeddings',
  documents: chunks,
  embeddings,
});
```

For more information on vector stores and their APIs, refer to the documentation for [PgVector](/docs/guide/reference/rag/pgstore.mdx), [Pinecone](/docs/guide/reference/rag/pinecone.mdx), and [Qdrant](/docs/guide/reference/rag/qdrant.mdx).

## Step 4: Querying the Knowledge Base

When a user asks a question, embed their query and retrieve relevant chunks from your vector store.

**Example:**

```ts showLineNumbers copy
const queryEmbedding = await embed('What are the main points in the article?', {
  model: 'text-embedding-ada-002',
});

const results = await PgVector.query({
  connectionString: 'postgresql://localhost:5432/mydb',
  tableName: 'embeddings',
  queryEmbedding,
  topK: 5,
});

// Use `results` as context for your LLM prompt
```

## Summary

With Mastra's RAG utilities, you can:

- **Initialize Documents**: Create `MastraDocument` instances from various formats.
- **Process Documents**: Use `chunk` to split documents and extract metadata.
- **Generate Embeddings**: Use `embed` functions to generate embeddings for documents or chunks.
- **Store and Query**: Utilize vector stores like PgVector, Pinecone, and Qdrant to persist and retrieve embeddings.
- **Enhance LLM Outputs**: Incorporate retrieved chunks as context to improve LLM responses.
