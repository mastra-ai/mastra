# Calling LLMs

Mastra has direct LLM support through the `LLM` class. Mastra supports a variety of LLM providers, including OpenAI, Anthropic, Google Gemini. You can choose the specific model and provider, choose system and user prompts, and decide whether to stream the response.

The LLM class is meant to be a simple way to make one-off LLM calls. You can specify a model and a user prompt.

## Basic Setup

```ts showLineNumbers filename="src/mastra/index.ts" copy
import { Mastra, type ModelConfig } from "@mastra/core";

async function main() {
  // Configure your model

  const mastra = new Mastra({});

  const modelConfig: ModelConfig = {
    provider: "OPEN_AI",
    name: "gpt-4o",
    toolChoice: "auto",
  };

  const llm = mastra.llm;

  const response = await llm.text({
    messages: [
      {
        role: "user",
        content: "What is machine learning?",
      },
    ],
    model: modelConfig,
  });
  console.log(response.text);
}

main();
```

Run the code with:

```bash copy
npx bun src/mastra/index.ts
```

The code for this example can be found [here](https://github.com/mastra-ai/mastra/tree/main/examples/llm).

## Available Models

Mastra supports major LLM providers (OpenAI, Anthropic, Google Gemini) out of the box, plus additional providers through AI SDK integrations. Custom providers can be added via Portkey service.

Here are the most popular models we support:

| Provider         | Supported Models                                                                                                                                                         |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| OpenAI           | `gpt-4`, `gpt-4-turbo`, `gpt-3.5-turbo`, `gpt-4o`, `gpt-4o-mini`                                                                                                         |
| Anthropic        | `claude-3-5-sonnet-20241022`, `claude-3-5-sonnet-20240620`, `claude-3-5-haiku-20241022`, `claude-3-opus-20240229`, `claude-3-sonnet-20240229`, `claude-3-haiku-20240307` |
| Google Gemini    | `gemini-1.5-pro-latest`, `gemini-1.5-pro`, `gemini-1.5-flash-latest`, `gemini-1.5-flash`                                                                                 |

A full list of supported models can be found [here](../reference/llm/providers-and-models.mdx).

## Usage Examples

### Text Generation

```typescript copy
const response = await llm.text({
  messages: [
    {
      role: "system",
      content: "You are a machine learning expert",
    },
    {
      role: "user",
      content: "What is machine learning?",
    },
  ],
  model: modelConfig,
});

console.log(response.text);
```

### Streaming Responses

```typescript copy
const stream = await llm.stream({
  messages: [
    {
      role: "system",
      content: "You are a helpful assistant",
    },
    {
      role: "user",
      content: "Explain quantum computing",
    },
  ],
  model: modelConfig,
  onStepFinish: (step) => {
    console.log("Step completed:", step);
  },
  maxSteps: 3,
});
```

## Environment Setup

Make sure to set your API keys. If you don't have an API key for an LLM provider, you can get one from the following services:

- [OpenAI](https://platform.openai.com/). Env variable: `OPENAI_API_KEY`
- [Anthropic](https://console.anthropic.com/settings/keys). Env variable: `ANTHROPIC_API_KEY`
- [Google Gemini](https://ai.google.dev/gemini-api/docs). Env variable: `GOOGLE_GENERATIVE_AI_API_KEY`
- [Groq](https://console.groq.com/keys). Env variable: `GROQ_API_KEY`

If you don't have an account with these providers, you can sign up and get an API key. OpenAI and Anthropic require a credit card to get an API key. Gemini does not and has a generous free tier for its API.
