---
title: "Using Agent Memory | Agents | Mastra Docs"
description: Documentation on how agents in Mastra use memory to store conversation history and contextual information.
---

# Agent Memory

Agents in Mastra have a sophisticated memory system that stores conversation history and contextual information. This memory system supports both traditional message storage and vector-based semantic search, enabling agents to maintain state across interactions and retrieve relevant historical context.

## Memory Configuration

The Mastra memory system is highly configurable and supports multiple storage backends. Here's a basic configuration example:

```typescript
import { Memory } from '@mastra/memory';
import { PostgresStore } from '@mastra/store-pg';
import { PgVector } from '@mastra/vector-pg';

const memory = new Memory({
  // Required: Storage backend for messages
  storage: new PostgresStore({
    host: 'localhost',
    port: 5432,
    user: 'postgres',
    database: 'postgres',
    password: 'postgres',
  }),
  
  // Optional: Vector store for semantic search
  vector: new PgVector(connectionString),
  
  // Optional: Memory configuration options
  options: {
    // Number of recent messages to include (false to disable)
    lastMessages: 10,
    // Configure vector-based semantic search
    historySearch: {
      topK: 3,         // Number of semantic search results
      messageRange: 2  // Messages before and after each result
    },
  },
  
  // Required if using vector search
  embeddingOptions: {
    provider: 'OPEN_AI',
    model: 'text-embedding-ada-002',
    maxRetries: 3,
  },
});
```

When you initialize a Mastra instance with this memory configuration, all agents will automatically use these memory settings when you call their `stream()` or `generate()` methods. The memory configuration options specified in the `options` object will be applied to all conversations.

You can override these default settings for individual calls by passing an `options` parameter:

```typescript
// Use default memory settings from Memory configuration
const response1 = await agent.generate("What were we discussing earlier?", {
  resourceId: "user_123",
  threadId: "thread_456"
});

// Override memory settings for this specific call
const response2 = await agent.generate("What were we discussing earlier?", {
  resourceId: "user_123",
  threadId: "thread_456",
  memoryOptions: {
    lastMessages: 5,  // Only inject 5 recent messages
    historySearch: {
      topK: 2,         // Only get 2 semantic search results
      messageRange: {   // Context around each result
        before: 1,
        after: 1
      }
    },
  }
});
```

The same applies to the `stream()` method:

```typescript
const stream = await agent.stream("Tell me about our previous conversation", {
  resourceId: "user_123",
  threadId: "thread_456",
  memoryOptions: {
    lastMessages: 20  // Override to get more context
  }
});
```

## Storage Options

Mastra currently supports several storage backends:

### LibSQL Storage
```typescript
import { MastraStorageLibSql } from '@mastra/core';

const storage = new MastraStorageLibSql({
  config: {
    url: 'file:example.db',
  },
});
```

### PostgreSQL Storage
```typescript
import { PostgresStore } from '@mastra/store-pg';

const storage = new PostgresStore({
  host: 'localhost',
  port: 5432,
  user: 'postgres',
  database: 'postgres',
  password: 'postgres',
});
```

### Upstash KV Storage
```typescript
import { UpstashStore } from '@mastra/store-upstash';

const storage = new UpstashStore({
  url: 'http://localhost:8089',
  token: 'your_token',
});
```

## Vector Search

Mastra supports semantic search through vector embeddings. When configured with a vector store, agents can find relevant historical messages based on semantic similarity. To enable vector search:

1. Configure a vector store (currently supports PostgreSQL):
```typescript
import { PgVector } from '@mastra/vector-pg';

const vector = new PgVector(connectionString);
```

2. Configure embedding options:
```typescript
const embeddingOptions = {
  provider: 'OPEN_AI',
  model: 'text-embedding-ada-002',
  maxRetries: 3,
};
```

3. Enable vector search in memory configuration options:
```typescript
const optionsConfig = {
  historySearch: {
    topK: 3,           // Number of similar messages to find
    messageRange: {     // Context around each result
      before: 2,       // Context before each result
      after: 2         // Context after each result
    }
  },
};
```

## Using Memory in Agents

Once configured, the memory system is automatically used by agents. Here's how to use it:

```typescript
// Initialize Mastra with memory
const mastra = new Mastra({
  agents: { myAgent },
  memory,
});

// Memory is automatically used in agent interactions
const response = await myAgent.generate(
  "What were we discussing earlier about performance?",
  {
    resourceId: "user_123",
    threadId: "thread_456",
  },
);
```

The memory system will automatically:
1. Store all messages in the configured storage backend
2. Create vector embeddings for semantic search (if configured)
3. Inject relevant historical context into new conversations
4. Maintain conversation threads and context

## Manually Managing Threads

While threads are automatically managed when using agent methods, you can also manually manage threads using the memory API directly. This is useful for advanced use cases like:
- Creating threads before starting conversations
- Managing thread metadata
- Explicitly saving or retrieving messages
- Cleaning up old threads

Here's how to manually work with threads:

```typescript
import { Memory } from '@mastra/memory';
import { PostgresStore } from '@mastra/store-pg';

// Initialize memory
const memory = new Memory({
  storage: new PostgresStore({
    host: 'localhost',
    port: 5432,
    user: 'postgres',
    database: 'postgres',
    password: 'postgres',
  })
});

// Create a new thread
const thread = await memory.createThread({
  resourceId: "user_123",
  title: "Project Discussion",
  metadata: {
    project: "mastra",
    topic: "architecture"
  }
});

// Manually save messages to a thread
await memory.saveMessages({
  messages: [{
    id: "msg_1",
    threadId: thread.id,
    role: "user",
    content: "What's the project status?",
    createdAt: new Date(),
    type: "text"
  }]
});

// Get messages from a thread with various filters
const messages = await memory.getMessages({
  threadId: thread.id,
  selectBy: {
    last: 10,                         // Get last 10 messages
    vectorSearchString: "performance", // Find messages about performance
  }
});

// Get thread by ID
const existingThread = await memory.getThreadById({
  threadId: "thread_123"
});

// Get all threads for a resource
const threads = await memory.getThreadsByResourceId({
  resourceId: "user_123"
});

// Update thread metadata
await memory.updateThread({
  id: thread.id,
  title: "Updated Project Discussion",
  metadata: {
    status: "completed"
  }
});

// Delete a thread and all its messages
await memory.deleteThread(thread.id);
```

Note that in most cases, you won't need to manage threads manually since the agent's `generate()` and `stream()` methods handle thread management automatically. Manual thread management is primarily useful for advanced use cases or when you need more fine-grained control over the conversation history.

```typescript
// Use memoryOptions to get more context for this specific request
await agent.generate("What did we discuss earlier?", {
  resourceId: "user_123",
  threadId: "thread_456",
  memoryOptions: {
    lastMessages: 20  // Override to get more context
  }
});
