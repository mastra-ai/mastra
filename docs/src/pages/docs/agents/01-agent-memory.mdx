---
title: "Using Agent Memory | Agents | Mastra Docs"
description: Documentation on how agents in Mastra use memory to store conversation history and contextual information.
---

# Agent Memory

Agents in Mastra have a sophisticated memory system that stores conversation history and contextual information. This memory system supports both traditional message storage and vector-based semantic search, enabling agents to maintain state across interactions and retrieve relevant historical context.

## How Memory Works

The Memory API uses two main mechanisms to maintain context in conversations:

### Recent Message History

The `lastMessages` setting maintains a sliding window of the most recent messages. This ensures your agent always has access to the immediate conversation context:

```tsx
const memory = new Memory({
  options: {
    lastMessages: 1, // Keep 1 most recent messages
  },
});

// Example conversation flow:
// 1. User asks about project timeline
await agent.stream('When will the project be completed?', {
  threadId: 'project_123',
  resourceId: 'user_123',
});

// 2. User asks about specific feature
await agent.stream('How will the new search feature work?', {
  threadId: 'project_123',
  resourceId: 'user_123',
});

// 3. When user asks this question, the agent will see the last 10 messages,
// allowing them to remember the conversation further back than the default we set above (lastMessages: 1)
await agent.stream('Can you summarize the search feature requirements?', {
  threadId: 'project_123',
  resourceId: 'user_123',
  memoryOptions: {
    lastMessages: 10,
  },
});
```

### Semantic Search

When vector search is enabled, Memory can find relevant past messages using semantic similarity. This allows your agent to recall information from earlier in the conversation:

```tsx
const memory = new Memory({
  options: {
    semanticRecall: {
      topK: 10, // Include 10 most relevant past messages
      messageRange: 2, // Messages before and after each result
    },
  },
});

// Example: User asks about a past feature discussion
await agent.stream('What did we decide about the search feature last week?', {
  threadId: 'support_123',
  resourceId: 'user_123',
  memoryOptions: {
    lastMessages: 10,
    semanticRecall: {
      topK: 3,
      messageRange: 2,
    },
  },
});
// The vector search will:
// 1. Find message_3 as relevant (topK: 10)
// 2. Include messages [1,2] before and [4,5] after
// 3. Include all of this context in the agent's response
```

## Memory Configuration

The Mastra memory system is highly configurable and supports multiple storage backends. Here's a basic configuration example:

```typescript
import { Memory } from '@mastra/memory';
import { PostgresStore } from '@mastra/store-pg';
import { PgVector } from '@mastra/vector-pg';

const memory = new Memory({
  // Required: Storage backend for messages
  storage: new PostgresStore({
    host: 'localhost',
    port: 5432,
    user: 'postgres',
    database: 'postgres',
    password: 'postgres',
  }),
  
  // Optional: Vector store for semantic search
  vector: new PgVector(connectionString),
  
  // Optional: Memory configuration options
  options: {
    // Number of recent messages to include (false to disable)
    lastMessages: 10,
    // Configure vector-based semantic search
    semanticRecall: {
      topK: 3,         // Number of semantic search results
      messageRange: 2  // Messages before and after each result
    },
  },
  
  // Required if using vector search
  embedding: {
    provider: 'OPEN_AI',
    model: 'text-embedding-3-small',
    maxRetries: 3,
  },
});
```

### Overriding Memory Settings

When you initialize a Mastra instance with memory configuration, all agents will automatically use these memory settings when you call their `stream()` or `generate()` methods. You can override these default settings for individual calls:

```typescript
// Use default memory settings from Memory configuration
const response1 = await agent.generate("What were we discussing earlier?", {
  resourceId: "user_123",
  threadId: "thread_456"
});

// Override memory settings for this specific call
const response2 = await agent.generate("What were we discussing earlier?", {
  resourceId: "user_123",
  threadId: "thread_456",
  memoryOptions: {
    lastMessages: 5,  // Only inject 5 recent messages
    semanticRecall: {
      topK: 2,         // Only get 2 semantic search results
      messageRange: 1   // Context around each result
    },
  }
});
```

### Configuring Memory for Different Use Cases

You can adjust memory settings based on your agent's needs:

```tsx
// Customer support agent with minimal context
await agent.stream('What are your store hours?', {
  threadId,
  resourceId,
  memoryOptions: {
    lastMessages: 5, // Quick responses need minimal conversation history
    historySearch: false, // no need to search through earlier messages
  },
});

// Project management agent with extensive context
await agent.stream('Update me on the project status', {
  threadId,
  resourceId,
  memoryOptions: {
    lastMessages: 50, // Maintain longer conversation history across project discussions
    semanticRecall: {
      topK: 5, // Find more relevant project details
      messageRange: 3, // Number of messages before and after each result
    },
  },
});
```

## Storage Options

Mastra currently supports several storage backends:

### LibSQL Storage
```typescript
import { MastraStorageLibSql } from '@mastra/core';

const storage = new MastraStorageLibSql({
  config: {
    url: 'file:example.db',
  },
});
```

### PostgreSQL Storage
```typescript
import { PostgresStore } from '@mastra/store-pg';

const storage = new PostgresStore({
  host: 'localhost',
  port: 5432,
  user: 'postgres',
  database: 'postgres',
  password: 'postgres',
});
```

### Upstash KV Storage
```typescript
import { UpstashStore } from '@mastra/store-upstash';

const storage = new UpstashStore({
  url: 'http://localhost:8089',
  token: 'your_token',
});
```

## Vector Search

Mastra supports semantic search through vector embeddings. When configured with a vector store, agents can find relevant historical messages based on semantic similarity. To enable vector search:

1. Configure a vector store (currently supports PostgreSQL):
```typescript
import { PgVector } from '@mastra/vector-pg';

const vector = new PgVector(connectionString);
```

2. Configure embedding options:
```typescript
const embedding = {
  provider: 'OPEN_AI',
  model: 'text-embedding-3-small',
  maxRetries: 3,
};
```

3. Enable vector search in memory configuration options:
```typescript
const optionsConfig = {
  historySearch: {
    topK: 3,         // Number of similar messages to find
    messageRange: 2, // Context around each result
  },
};
```

## Using Memory in Agents

Once configured, the memory system is automatically used by agents. Here's how to use it:

```typescript
// Initialize Mastra with memory
const mastra = new Mastra({
  agents: { myAgent },
  memory,
});

// Memory is automatically used in agent interactions
const response = await myAgent.generate(
  "What were we discussing earlier about performance?",
  {
    resourceId: "user_123",
    threadId: "thread_456",
  },
);
```

The memory system will automatically:
1. Store all messages in the configured storage backend
2. Create vector embeddings for semantic search (if configured)
3. Inject relevant historical context into new conversations
4. Maintain conversation threads and context

## Manually Managing Threads

While threads are automatically managed when using agent methods, you can also manually manage threads using the memory API directly. This is useful for advanced use cases like:
- Creating threads before starting conversations
- Managing thread metadata
- Explicitly saving or retrieving messages
- Cleaning up old threads

Here's how to manually work with threads:

```typescript
import { Memory } from '@mastra/memory';
import { PostgresStore } from '@mastra/store-pg';

// Initialize memory
const memory = new Memory({
  storage: new PostgresStore({
    host: 'localhost',
    port: 5432,
    user: 'postgres',
    database: 'postgres',
    password: 'postgres',
  })
});

// Create a new thread
const thread = await memory.createThread({
  resourceId: "user_123",
  title: "Project Discussion",
  metadata: {
    project: "mastra",
    topic: "architecture"
  }
});

// Manually save messages to a thread
await memory.saveMessages({
  messages: [{
    id: "msg_1",
    threadId: thread.id,
    role: "user",
    content: "What's the project status?",
    createdAt: new Date(),
    type: "text"
  }]
});

// Get messages from a thread with various filters
const messages = await memory.query({
  threadId: thread.id,
  selectBy: {
    last: 10,                         // Get last 10 messages
    vectorSearchString: "performance", // Find messages about performance
  }
});

// Get thread by ID
const existingThread = await memory.getThreadById({
  threadId: "thread_123"
});

// Get all threads for a resource
const threads = await memory.getThreadsByResourceId({
  resourceId: "user_123"
});

// Update thread metadata
await memory.updateThread({
  id: thread.id,
  title: "Updated Project Discussion",
  metadata: {
    status: "completed"
  }
});

// Delete a thread and all its messages
await memory.deleteThread(thread.id);
```

Note that in most cases, you won't need to manage threads manually since the agent's `generate()` and `stream()` methods handle thread management automatically. Manual thread management is primarily useful for advanced use cases or when you need more fine-grained control over the conversation history.

```typescript
// Use memoryOptions to get more context for this specific request
await agent.generate("What did we discuss earlier?", {
  resourceId: "user_123",
  threadId: "thread_456",
  memoryOptions: {
    lastMessages: 20  // Override to get more context
  }
});
