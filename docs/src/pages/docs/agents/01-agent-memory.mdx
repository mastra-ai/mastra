---
title: "Using Agent Memory | Agents | Mastra Docs"
description: Documentation on how agents in Mastra use memory to store conversation history and contextual information.
---

# Agent Memory

Agents in Mastra have a sophisticated memory system that stores conversation history and contextual information. This memory system supports both traditional message storage and vector-based semantic search, enabling agents to maintain state across interactions and retrieve relevant historical context.

## How Memory Works

The Memory API uses two main mechanisms to maintain context in conversations:

### Recent Message History

The `lastMessages` setting maintains a sliding window of the most recent messages. This ensures your agent always has access to the immediate conversation context:

```tsx
const memory = new Memory({
  options: {
    lastMessages: 1, // Keep 1 most recent messages
  },
});

// Example conversation flow:
// 1. User asks about project timeline
await agent.stream("When will the project be completed?", {
  threadId: "project_123",
  resourceId: "user_123",
});

// 2. User asks about specific feature
await agent.stream("How will the new search feature work?", {
  threadId: "project_123",
  resourceId: "user_123",
});

// 3. When user asks this question, the agent will see the last 10 messages,
// allowing them to remember the conversation further back than the default we set above (lastMessages: 1)
await agent.stream("Can you summarize the search feature requirements?", {
  threadId: "project_123",
  resourceId: "user_123",
  memoryOptions: {
    lastMessages: 10,
  },
});
```

### Semantic Search

When vector search is enabled, Memory can find relevant past messages using semantic similarity. This allows your agent to recall information from earlier in the conversation:

```tsx
const memory = new Memory({
  options: {
    semanticRecall: {
      topK: 10, // Include 10 most relevant past messages
      messageRange: 2, // Messages before and after each result
    },
  },
});

// Example: User asks about a past feature discussion
await agent.stream("What did we decide about the search feature last week?", {
  threadId: "support_123",
  resourceId: "user_123",
  memoryOptions: {
    lastMessages: 10,
    semanticRecall: {
      topK: 3,
      messageRange: 2,
    },
  },
});
// The vector search will:
// 1. Find message_3 as relevant (topK: 10)
// 2. Include messages [1,2] before and [4,5] after
// 3. Include all of this context in the agent's response
```

## Memory Configuration

The Mastra memory system is highly configurable and supports multiple storage backends. Here's a basic configuration example:

```typescript
import { openai } from "@ai-sdk/openai";
import { Memory } from "@mastra/memory";
import { PostgresStore, PgVector } from "@mastra/pg";

const memory = new Memory({
  // Required: Storage backend for messages
  storage: new PostgresStore({
    host: "localhost",
    port: 5432,
    user: "postgres",
    database: "postgres",
    password: "postgres",
  }),

  // Optional: Vector store for semantic search
  vector: new PgVector(connectionString),

  // Optional: Memory configuration options
  options: {
    // Number of recent messages to include (false to disable)
    lastMessages: 10,
    // Configure vector-based semantic search
    semanticRecall: {
      topK: 3, // Number of semantic search results
      messageRange: 2, // Messages before and after each result
    },
  },

  // Required if using vector search
  embedder: openai.embedding("text-embedding-3-small"),
});
```

### Overriding Memory Settings

When you initialize a Mastra instance with memory configuration, all agents will automatically use these memory settings when you call their `stream()` or `generate()` methods. You can override these default settings for individual calls:

```typescript
// Use default memory settings from Memory configuration
const response1 = await agent.generate("What were we discussing earlier?", {
  resourceId: "user_123",
  threadId: "thread_456",
});

// Override memory settings for this specific call
const response2 = await agent.generate("What were we discussing earlier?", {
  resourceId: "user_123",
  threadId: "thread_456",
  memoryOptions: {
    lastMessages: 5, // Only inject 5 recent messages
    semanticRecall: {
      topK: 2, // Only get 2 semantic search results
      messageRange: 1, // Context around each result
    },
  },
});
```

### Configuring Memory for Different Use Cases

You can adjust memory settings based on your agent's needs:

```tsx
// Customer support agent with minimal context
await agent.stream("What are your store hours?", {
  threadId,
  resourceId,
  memoryOptions: {
    lastMessages: 5, // Quick responses need minimal conversation history
    historySearch: false, // no need to search through earlier messages
  },
});

// Project management agent with extensive context
await agent.stream("Update me on the project status", {
  threadId,
  resourceId,
  memoryOptions: {
    lastMessages: 50, // Maintain longer conversation history across project discussions
    semanticRecall: {
      topK: 5, // Find more relevant project details
      messageRange: 3, // Number of messages before and after each result
    },
  },
});
```

## Storage Options

Mastra currently supports several storage backends:

### LibSQL Storage

```typescript
import { DefaultStorage } from "@mastra/core/storage";

const storage = new DefaultStorage({
  config: {
    url: "file:example.db",
  },
});
```

### PostgreSQL Storage

```typescript
import { PostgresStore } from "@mastra/pg";

const storage = new PostgresStore({
  host: "localhost",
  port: 5432,
  user: "postgres",
  database: "postgres",
  password: "postgres",
});
```

### Upstash KV Storage

```typescript
import { UpstashStore } from "@mastra/upstash";

const storage = new UpstashStore({
  url: "http://localhost:8089",
  token: "your_token",
});
```

## Vector Search

Mastra supports semantic search through vector embeddings. When configured with a vector store, agents can find relevant historical messages based on semantic similarity. To enable vector search:

1. Configure a vector store (currently supports PostgreSQL):

```typescript
import { PgVector } from "@mastra/pg";

const vector = new PgVector(connectionString);

const memory = new Memory({ vector });
```

2. Configure embedding options:

```typescript
const memory = new Memory({
  vector,
  embedder: openai.embedding("text-embedding-3-small"),
});
```

3. Enable vector search in memory configuration options:

```typescript
const memory = new Memory({
  vector,
  embedder,

  options: {
    historySearch: {
      topK: 3, // Number of similar messages to find
      messageRange: 2, // Context around each result
    },
  },
});
```

## Using Memory in Agents

Once configured, the memory system is automatically used by agents. Here's how to use it:

```typescript
// Initialize Mastra with memory
const mastra = new Mastra({
  agents: { myAgent },
  memory,
});

// Memory is automatically used in agent interactions
const response = await myAgent.generate(
  "What were we discussing earlier about performance?",
  {
    resourceId: "user_123",
    threadId: "thread_456",
  },
);
```

The memory system will automatically:

1. Store all messages in the configured storage backend
2. Create vector embeddings for semantic search (if configured)
3. Inject relevant historical context into new conversations
4. Maintain conversation threads and context

## Manually Managing Threads

While threads are automatically managed when using agent methods, you can also manually manage threads using the memory API directly. This is useful for advanced use cases like:

- Creating threads before starting conversations
- Managing thread metadata
- Explicitly saving or retrieving messages
- Cleaning up old threads

Here's how to manually work with threads:

```typescript
import { Memory } from "@mastra/memory";
import { PostgresStore } from "@mastra/pg";

// Initialize memory
const memory = new Memory({
  storage: new PostgresStore({
    host: "localhost",
    port: 5432,
    user: "postgres",
    database: "postgres",
    password: "postgres",
  }),
});

// Create a new thread
const thread = await memory.createThread({
  resourceId: "user_123",
  title: "Project Discussion",
  metadata: {
    project: "mastra",
    topic: "architecture",
  },
});

// Manually save messages to a thread
await memory.saveMessages({
  messages: [
    {
      id: "msg_1",
      threadId: thread.id,
      role: "user",
      content: "What's the project status?",
      createdAt: new Date(),
      type: "text",
    },
  ],
});

// Get messages from a thread with various filters
const messages = await memory.query({
  threadId: thread.id,
  selectBy: {
    last: 10, // Get last 10 messages
    vectorSearchString: "performance", // Find messages about performance
  },
});

// Get thread by ID
const existingThread = await memory.getThreadById({
  threadId: "thread_123",
});

// Get all threads for a resource
const threads = await memory.getThreadsByResourceId({
  resourceId: "user_123",
});

// Update thread metadata
await memory.updateThread({
  id: thread.id,
  title: "Updated Project Discussion",
  metadata: {
    status: "completed",
  },
});

// Delete a thread and all its messages
await memory.deleteThread(thread.id);
```

Note that in most cases, you won't need to manage threads manually since the agent's `generate()` and `stream()` methods handle thread management automatically. Manual thread management is primarily useful for advanced use cases or when you need more fine-grained control over the conversation history.

### Working Memory

Working memory is a powerful feature that allows agents to maintain persistent information across conversations, even with minimal context. This is particularly useful for remembering user preferences, personal details, or any other contextual information that should persist throughout interactions.

Inspired by the working memory concept from the MemGPT whitepaper, our implementation improves upon it in several key ways:

- No extra roundtrips or tool calls required
- Full support for streaming messages
- Seamless integration with the agent's natural response flow

#### How It Works

Working memory operates through a system of XML tags and automatic updates:

1. **Template Structure**: Define what information should be remembered using XML tags. The Memory class comes with a comprehensive default template for user information, or you can create your own template to match your specific needs.

2. **Automatic Updates**: The Memory class injects special instructions into the agent's system prompt that tell it to:

   - Store relevant information by including `<working_memory>...</working_memory>` tags in its responses
   - Update information proactively when anything changes
   - Maintain the XML structure while updating values
   - Keep this process invisible to users

3. **Memory Management**: The system:
   - Extracts working memory blocks from agent responses
   - Stores them for future use
   - Injects working memory into the system prompt on the next agent call

The agent is instructed to be proactive about storing information - if there's any doubt about whether something might be useful later, it should be stored. This helps maintain conversation context even when using very small context windows.

#### Basic Usage

```typescript
import { openai } from "@ai-sdk/openai";

const agent = new Agent({
  name: "Customer Service",
  instructions:
    "You are a helpful customer service agent. Remember customer preferences and past interactions.",
  model: openai("gpt-4o-mini"),

  memory: new Memory({
    storage: new DefaultStorage({ url: ":memory:" }),
    options: {
      workingMemory: {
        enabled: true, // enables working memory
      },
      lastMessages: 5, // Only keep recent context
    },
  }),
});
```

Working memory becomes particularly powerful when combined with specialized system prompts. For example, you could create a TODO list manager that maintains state even though it only has access to the previous message:

```typescript
const todoAgent = new Agent({
  name: "TODO Manager",
  instructions:
    "You are a TODO list manager. Update the todo list in working memory whenever tasks are added, completed, or modified.",
  model: openai("gpt-4o-mini"),
  memory: new Memory({
    storage: new DefaultStorage({ url: ":memory:" }),
    options: {
      workingMemory: {
        enabled: true,

        // optional XML-like template to encourage agent to store specific kinds of info.
        // if you leave this out a default template will be used
        template: `<todos>
  <in-progress></in-progress>
  <pending></pending>
  <completed></completed>
</todos>`,
      },
      lastMessages: 1, // Only keep the last message in context
    },
  }),
});
```

### Handling Memory Updates in Streaming

When an agent responds, it includes working memory updates directly in its response stream. These updates appear as XML blocks in the text:

```
// Raw agent response stream:
Let me help you with that! <working_memory><user><first_name>John</first_name>...</user></working_memory> Based on your question...
```

To prevent these memory blocks from being visible to users while still allowing the system to process them, use the `maskStreamTags` utility:

```typescript
import { maskStreamTags } from "@mastra/core/utils";

// Basic usage - just mask the working_memory tags
for await (const chunk of maskStreamTags(
  response.textStream,
  "working_memory",
)) {
  process.stdout.write(chunk);
}

// Without masking: "Let me help you! <working_memory>...</working_memory> Based on..."
// With masking: "Let me help you! Based on..."
```

You can also hook into memory update events:

```typescript
const maskedStream = maskStreamTags(response.textStream, "working_memory", {
  onStart: () => showLoadingSpinner(),
  onEnd: () => hideLoadingSpinner(),
  onMask: (chunk) => console.debug(chunk),
});
```

The `maskStreamTags` utility:

- Removes content between specified XML tags in a streaming response
- Optionally provides lifecycle callbacks for memory updates
- Handles tags that might be split across stream chunks
