---
title: "Evals Overview | Mastra Docs"
description: "Mastra evals help you measure LLM output quality with metrics for relevance, bias, hallucination, and more."
---

# Running Evals

Evals are automated tests that evaluate LLM outputs using model-graded, rule-based, and statistical methods. Each eval returns a normalized score between 0-1 that can be logged and compared. Evals can be customized with your own prompts and scoring functions.

Evals suites run in the cloud, but as tests, it's logical to store them in your codebase. Because LLMs are non-deterministic, you might not get a 100% pass rate every time.

## Mastra Evals

Mastra provides several eval metrics for assessing LLM outputs:

### LLM Metrics
- [`answer-relevancy`](/docs/reference/evals/answer-relevancy): Measures how well an answer addresses the input question
- [`bias`](/docs/reference/evals/bias): Detects potential biases in the output
- [`context-position`](/docs/reference/evals/context-position): Evaluates the placement of context in responses
- [`context-precision`](/docs/reference/evals/context-precision): Assesses the accuracy of context usage
- [`context-relevancy`](/docs/reference/evals/context-relevancy): Measures the relevance of used context
- [`contextual-recall`](/docs/reference/evals/contextual-recall): Evaluates information recall from context
- [`faithfulness`](/docs/reference/evals/faithfulness): Checks output alignment with source material
- [`hallucination`](/docs/reference/evals/hallucination): Detects fabricated or unsupported information
- [`prompt-alignment`](/docs/reference/evals/prompt-alignment): Measures adherence to prompt instructions
- [`summarization`](/docs/reference/evals/summarization): Evaluates summary quality
- [`toxicity`](/docs/reference/evals/toxicity): Detects harmful or inappropriate content

### NLP Metrics
- [`completeness`](/docs/reference/evals/completeness): Measures if all required information is present
- [`content-similarity`](/docs/reference/evals/content-similarity): Compares text similarity
- [`keyword-coverage`](/docs/reference/evals/keyword-coverage): Checks for presence of key terms
- [`textual-difference`](/docs/reference/evals/textual-difference): Measures text changes
- [`tone`](/docs/reference/evals/tone-consistency): Analyzes writing style and tone
