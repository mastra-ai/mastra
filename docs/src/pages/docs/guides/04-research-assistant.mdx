---
title: "Building a Research Paper Assistant | Mastra RAG Guides"
description: Guide on creating an AI research assistant that can analyze and answer questions about academic papers using RAG.
---

import { Steps } from "nextra/components";

# Building a Research Paper Assistant with RAG

In this guide, we'll create an AI research assistant that can analyze academic papers and answer specific questions about their content using Retrieval Augmented Generation (RAG).

We'll use the GPT-3 paper [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf) as our example.

## Understanding RAG Components

RAG consists of three main parts:
1. Document Processing: Loading and chunking papers
2. Vector Storage: Creating and storing embeddings
3. Retrieval Process: Finding and using relevant content

## Project Structure

```
research-assistant/
├── src/
│   ├── agents/
│   │   └── researchAgent.ts
│   └── index.ts
├── package.json
└── .env
```

<Steps>
### Initialize Project and Install Dependencies

First, create a new directory for your project and navigate into it:

```bash
mkdir research-assistant
cd research-assistant
```

Initialize a new Node.js project and install the required dependencies:

```bash
npm init -y
npm install @mastra/core @mastra/rag @mastra/pg @ai-sdk/openai zod
```

Set up environment variables for API access and database connection:

```bash filename=".env" copy
OPENAI_API_KEY=your_openai_api_key
POSTGRES_CONNECTION_STRING=your_connection_string
```

Create the necessary files for our project:

```bash
mkdir -p src/agents
touch src/agents/researchAgent.ts src/index.ts
```

### Create the Research Assistant Agent

Now we'll create our RAG-enabled research assistant. The agent uses:
- A vector query tool to search our paper database
- GPT-4o-mini for understanding queries and generating responses
- Custom instructions for academic paper analysis

```typescript copy showLineNumbers filename="src/agents/researchAgent.ts"
import { Agent } from '@mastra/core/agent';
import { openai } from '@ai-sdk/openai';
import { createVectorQueryTool } from '@mastra/rag';

const vectorQueryTool = createVectorQueryTool({
  vectorStoreName: 'pgVector',
  indexName: 'papers',
  model: openai.embedding('text-embedding-3-small'),
});

export const researchAgent = new Agent({
  name: 'Research Assistant',
  instructions: 'You are a helpful research assistant that analyzes academic papers...',
  model: openai('gpt-4o-mini'),
  tools: {
    vectorQueryTool,
  },
});
```

### Set Up the Mastra Instance and Vector Store

```typescript copy showLineNumbers filename="src/index.ts"
import { MDocument } from '@mastra/rag';
import { Mastra } from '@mastra/core';
import { PgVector } from '@mastra/pg';
import { embedMany } from 'ai';

import { researchAgent } from './agents/researchAgent';

const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING!);
export const mastra = new Mastra({
  agents: { researchAgent },
  vectors: { pgVector },
});
```

### Load and Process the Paper

This step handles the initial document processing. We:
1. Fetch the research paper from its URL
2. Convert it into a document object
3. Split it into smaller, manageable chunks for better processing

```typescript copy showLineNumbers{14} filename="src/index.ts"
// Load the paper
const paperUrl = "https://arxiv.org/pdf/2005.14165.pdf";
const response = await fetch(paperUrl);
const paperText = await response.text();

// Create document and chunk it
const doc = MDocument.fromText(paperText);
const chunks = await doc.chunk({
  strategy: 'recursive',
  size: 512,
  overlap: 50,
  separator: '\n',
});
```

### Create and Store Embeddings

Finally, we'll prepare our content for RAG by:
1. Generating embeddings for each chunk of text
2. Creating a vector store index to hold our embeddings
3. Storing both the embeddings and their associated text
This allows our agent to efficiently search and retrieve relevant information.

```typescript copy showLineNumbers{28} filename="src/index.ts"
// Generate embeddings
const { embeddings } = await embedMany({
  model: openai.embedding('text-embedding-3-small'),
  values: chunks.map(chunk => chunk.text),
});

// Get the vector store instance from Mastra
const vectorStore = mastra.getVector('pgVector');

// Create an index for our paper chunks
await vectorStore.createIndex({
  indexName: 'papers',
  dimension: 1536,
});

// Store embeddings
await vectorStore.upsert({
  indexName: 'papers',
  vectors: embeddings,
  metadata: chunks.map(chunk => ({
    text: chunk.text,
    source: 'gpt3-paper'
  })),
});
```

This will:
1. Load the paper from the URL
2. Split it into manageable chunks
3. Generate embeddings for each chunk
4. Store both the embeddings and text in our vector database

### Test the Assistant

Let's test our research assistant with different types of queries:

```typescript filename="src/index.ts" showLineNumbers{52} copy
const agent = mastra.getAgent('researchAgent');

// Query about experimental findings
const query1 = "What were the main findings about few-shot performance?";
const response1 = await agent.generate(query1);
console.log("\nQuery:", query1);
console.log("Response:", response1.text);
```

You should see output like:
```
Query: What were the main findings about few-shot performance?
Response: The GPT-3 paper demonstrated that larger language models exhibit strong few-shot learning capabilities. 
Specifically, they found that models could perform tasks with just a few examples, or even zero examples, 
showing performance that scaled predictably with model size. The 175B parameter model achieved particularly 
strong results, approaching or matching SOTA performance on many NLP tasks without any gradient updates or 
fine-tuning.
```

Let's try a question about technical details:
```typescript filename="src/index.ts" showLineNumbers{60} copy
// Query about technical details
const query2 = "Explain the scaling laws they discovered";
const response2 = await agent.generate(query2);
console.log("\nQuery:", query2);
console.log("Response:", response2.text);
```

Output:
```
Query: Explain the scaling laws they discovered
Response: The researchers found that model performance improved smoothly and predictably with scale. 
They observed that performance improved roughly log-linearly with model size, compute, and dataset size. 
Specifically, they found that doubling the model size consistently resulted in a linear improvement in 
performance across their measured tasks, suggesting that even larger models could continue this trend.
```

Finally, let's ask about broader implications:
```typescript filename="src/index.ts" showLineNumbers{66} copy
// Query about broader implications
const query3 = "What limitations and risks were identified?";
const response3 = await agent.generate(query3);
console.log("\nQuery:", query3);
console.log("Response:", response3.text);
```

Output:
```
Query: What limitations and risks were identified?
Response: The paper identified several key limitations and risks. Computationally, the models require 
significant resources to train and deploy. They also noted potential misuse risks, including generating 
misleading information or spam. The authors specifically highlighted concerns about bias in training data 
and the model's potential to amplify these biases. They also acknowledged limitations in their evaluation 
methodology and the need for more robust testing frameworks.
```

### Serve the Application

Start the Mastra server to expose your research assistant via API:

```bash
mastra dev --dir src
```

Your research assistant will be available at:
```
http://localhost:4111/api/agents/researchAgent/generate
```

Test with curl:

```bash
curl -X POST http://localhost:4111/api/agents/researchAgent/generate \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      { "role": "user", "content": "What were the main findings about few-shot performance?" }
    ]
  }'
```

### Next Steps

After completing this guide, explore:
- Filter RAG for handling paper metadata
- Cleanup RAG for better academic text chunks
- Chain of Thought RAG for complex research queries
</Steps>

## Additional Resources

- [Advanced RAG Techniques](link-to-advanced-rag)
- [Paper Analysis Best Practices](link-to-best-practices)
- [Research Assistant Examples](link-to-examples)
