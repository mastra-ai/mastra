# Conversation History

Conversation history enables agents to understand and respond to multi-turn conversations.

When a new message is sent, a set number of the most recent messages are retrieved from the memory thread and are added to the LLM's context window, and the new message is saved in the DB.

Even if you don't use more advanced memory features, this feature alone will allow your agents to be able to maintain coherent conversations.

## Quick Start

```typescript
import { Memory } from "@mastra/memory";
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";

// Create an agent with memory
const agent = new Agent({
  name: "ConversationAgent",
  instructions: "You are a helpful assistant.",
  model: openai("gpt-4o"),
  memory: new Memory(), // Uses default setting of 40 most recent messages
});
```

## How Conversation History Works

This feature captures and retrieves conversation history. Each time you call an agent with a `resourceId` and `threadId`, the memory system:

- Stores the current message automatically
- Retrieves recent messages from that thread (by default, the 40 most recent)
- Adds these messages to the agent's context window
- Enables the agent to respond with awareness of the conversation history
- Stores the agent's response and any tool calls for future context

As conversations grow longer, older messages drop out of the context window, maintaining a sliding window of recent context.

```text
                   ┌─────────────────────┐
                   │  New User Message   │
                   └─────────┬───────────┘
                             │
                             ▼
                   ┌─────────────────────┐
                   │  Agent with Memory  │
                   └─────────┬───────────┘
                             │ invokes
                             ▼
                ┌────────────────────────────┐
                │    Memory Last Messages    │
                │    Retrieval Function      │
                └────────────┬───────────────┘
                             │ queries
                             ▼
         ┌─────────────────────────────────────┐
         │       Thread Message Storage        │
         │  ───────────────────────────────    │
         │  Message 40 (oldest) - timestamp    │
         │  Message 39        - timestamp      │
         │  ...                                │
         │  Message 2         - timestamp      │
         │  Message 1 (newest) - timestamp     │
         └────────────────────┬────────────────┘
                              │ returns last 40 messages
                              ▼
           ┌───────────────────────────────┐
           │       Context Window          │
           │  ─────────────────────────    │
           │  System Instructions          │
           │  Last 40 Messages             │
           │  Current User Message         │
           └───────────────┬───────────────┘
                           │ sent to
                           ▼
                  ┌──────────────────┐
                  │    LLM Provider  │
                  └──────────────────┘
```

**Important:** When using Mastra's memory system, you should **only send the latest message** from the user with each agent call. Do not manage or send the entire message history yourself. Mastra retrieves and injects the necessary conversation history based on your memory configuration. Sending the full history will lead to message duplication and incorrect context.

## Configuration

You can set the number of last messages when creating your agent:
See the [Memory Class Reference](/docs/reference/memory/Memory.mdx#options) for all available configuration options.

You can also override the setting for individual requests:

```typescript
// Change message count for a specific request
await agent.stream("Hello again", {
  resourceId: "user_123",
  threadId: "thread_456",
  memoryOptions: {
    lastMessages: 10, // Only use 10 messages for this specific request
  },
});
```

## Token Considerations

The message count directly impacts token usage and context window space. Different conversation types benefit from different settings:

**Recommended settings by conversation type:**

| Conversation Type   | Suggested Setting | Reasoning                                  |
| ------------------- | ----------------- | ------------------------------------------ |
| Simple Q&A          | 5-10 messages     | Short exchanges need minimal context       |
| Customer Service    | 15-20 messages    | Balance between history and token usage    |
| Complex Discussions | 30-40 messages    | Detailed context helps with complex topics |

**Note:** For one-off processing tasks like simple summarization, you can disable messages with `lastMessages: false`, though it's generally better to create an agent without memory for these cases.

For automatic token management, consider using the [Token Limiter memory processor](./token-management.mdx).

## Related Features

Last messages provide the foundation for conversational memory, but additional capabilities can complement this functionality:

- **[Semantic Recall](./semantic-recall.mdx)**: Find relevant messages from anywhere in conversation history, not just recent messages
- **[Working Memory](./working-memory.mdx)**: Store persistent user information in a structured format
- **[Token Management](./token-management.mdx)**: Optimize token usage across different memory features
- **[Configuring Memory](./configuring-memory.mdx)**: Configure memory backends and settings

