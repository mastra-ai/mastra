# Optimizing Token Usage

The context window of language models is a finite resource that affects both cost and performance. You should always use the `TokenLimiter` processor to prevent API errors when there are more tokens than what your model supports. This processor automatically manages your token usage while maintaining response quality. See the [Memory Processors reference](/docs/reference/memory/memory-processors.mdx) for more details on available processors.

## Quick Start

Here's a minimal example of using the `TokenLimiter` to manage token usage:

```typescript
import { Memory } from "@mastra/memory";
import { TokenLimiter } from "@mastra/memory/processors";

// Create memory with token limiting processor
const memory = new Memory({
  // Configure memory retrieval options
  options: {
    lastMessages: 50,
  },
  // Add TokenLimiter processor
  processors: [
    new TokenLimiter(127000), // Limit total tokens (for GPT-4o)
  ],
});

// Create agent with token-managed memory
const agent = new Agent({
  name: "EfficientAgent",
  instructions: "You are a helpful assistant.",
  model: openai("gpt-4o"),
  memory,
});
```

## How `TokenLimiter` Works

The `TokenLimiter` processor acts as a safeguard that automatically manages token usage before messages are sent to the LLM:

```text
    ┌────────────────────┐
    │   User Message     │
    └─────────┬──────────┘
              ▼
┌─────────────────────────┐
│   Memory Retrieval      │
│   ───────────────       │
│   Last Messages         │
│   Semantic Recall       │
│   Working Memory        │
└───────────┬─────────────┘
            ▼
┌───────────────────────────┐
│     TokenLimiter          │
│     ───────────────       │
│     Counts tokens         │
│     Removes messages      │
│     Ensures limit         │
└───────────┬───────────────┘
            ▼
┌────────────────────────────┐
│    Optimized Context       │
│    ───────────────         │
│    System Instructions     │
│    Token-Limited Memory    │
│    Current User Message    │
└──────────────┬─────────────┘
               ▼
      ┌─────────────────┐
      │   LLM Provider  │
      └─────────────────┘
```

When the `TokenLimiter` runs:

1. It counts tokens in all retrieved memory messages
2. If the total exceeds your specified limit, it removes messages
3. It prioritizes keeping the most recent messages (older messages are removed first)
4. Your context remains within the limit, preventing "context too long" errors

## Configuring `TokenLimiter`

```typescript
import { Memory } from "@mastra/memory";
import { TokenLimiter } from "@mastra/memory/processors";

const memory = new Memory({
  processors: [new TokenLimiter(127000)],
});
```

For a more specific configuration, you can customize the encoding:

```typescript
import cl100k_base from "js-tiktoken/ranks/cl100k_base";

const memory = new Memory({
  processors: [
    new TokenLimiter({
      limit: 16000, // For smaller context models
      encoding: cl100k_base, // Model-specific encoding
    }),
  ],
});
```

## Setting Appropriate Limits

When setting your token limit:

1. **Check your model's context window size** - Each model has different limits
2. **Set a conservative limit** - The `TokenLimiter` uses estimates that may be 95-98% accurate depending on the model
3. **Account for non-memory tokens** - Remember that system instructions and the current message also consume tokens

For example, if your model supports 128,000 tokens (like GPT-4o), you might set the limit to 127,000 to provide a small safety margin.

## Best Practices

1. **Always use `TokenLimiter`**: This is the most reliable way to prevent context overflow errors
2. **Set conservative limits**: Choose a limit slightly below your model's context window size
3. **Start small**: Begin with aggressive token limits and increase as needed
4. **Test with realistic data**: Verify token usage with typical conversations
5. **Remember estimation accuracy**: TokenLimiter uses estimates that may be 95-98% accurate

## Related Features

- **[Configuring Memory](./configuring-memory.mdx)**: For more information on memory processors and configuration
- **[Conversation History](./last-messages.mdx)**: For controlling recent conversation context
- **[Recalling Relevant History](./semantic-recall.mdx)**: For optimizing relevant message retrieval
