---
title: "Supported evals"
description: "Mastra provides several default evals for assessing Agent outputs."
---

# Supported evals in Mastra

Mastra provides several eval metrics for assessing Agent outputs. Mastra is not limited to these metrics, and you can also [define your own evals](/docs/evals/02-custom-eval).

### Accuracy and Reliability
- [`hallucination`](/docs/reference/evals/hallucination): Detects fabricated or unsupported information
- [`faithfulness`](/docs/reference/evals/faithfulness): Checks output alignment with source material  
- [`content-similarity`](/docs/reference/evals/content-similarity): Compares text similarity
- [`textual-difference`](/docs/reference/evals/textual-difference): Measures text changes
- [`completeness`](/docs/reference/evals/completeness): Measures if all required information is present
- [`answer-relevancy`](/docs/reference/evals/answer-relevancy): Measures how well an answer addresses the input question

### Understanding Context
- [`context-position`](/docs/reference/evals/context-position): Evaluates the placement of context in responses
- [`context-precision`](/docs/reference/evals/context-precision): Assesses the accuracy of context usage
- [`context-relevancy`](/docs/reference/evals/context-relevancy): Measures the relevance of used context
- [`contextual-recall`](/docs/reference/evals/contextual-recall): Evaluates information recall from context

### Output Quality
- [`tone`](/docs/reference/evals/tone-consistency): Analyzes writing style and tone
- [`toxicity`](/docs/reference/evals/toxicity): Detects harmful or inappropriate content
- [`bias`](/docs/reference/evals/bias): Detects potential biases in the output
- [`prompt-alignment`](/docs/reference/evals/prompt-alignment): Measures adherence to prompt instructions
- [`summarization`](/docs/reference/evals/summarization): Evaluates summary quality
- [`keyword-coverage`](/docs/reference/evals/keyword-coverage): Checks for presence of key terms

