# AnswerRelevancyMetric

The `AnswerRelevancyMetric` class evaluates how well an LLM's output answers or addresses the input query. It uses a judge-based system to determine relevancy and provides detailed scoring and reasoning.

## Basic Usage

```typescript
import { AnswerRelevancyMetric } from "@mastra/evals";

// Configure the model for evaluation
const model = {
  provider: "OPEN_AI",
  name: "gpt-4",
  apiKey: process.env.OPENAI_API_KEY
};

const metric = new AnswerRelevancyMetric(model, {
  uncertaintyWeight: 0.3,
  scale: 10
});

const result = await metric.measure({
  input: "What is the capital of France?",
  output: "Paris is the capital of France."
});

console.log(result.score); // Score from 0-10
console.log(result.reason); // Explanation of the score
```

## Constructor Parameters

<PropertiesTable
  content={[
    {
      name: "model",
      type: "ModelConfig",
      description: "Configuration for the model used to evaluate relevancy",
      isOptional: false,
    },
    {
      name: "options",
      type: "AnswerRelevancyMetricOptions",
      description: "Configuration options for the metric",
      isOptional: true,
      defaultValue: "{ uncertaintyWeight: 0.3, scale: 10 }",
    }
  ]}
/>

### AnswerRelevancyMetricOptions

<PropertiesTable
  content={[
    {
      name: "uncertaintyWeight",
      type: "number",
      description: "Weight given to 'unsure' verdicts in scoring (0-1)",
      isOptional: true,
      defaultValue: "0.3",
    },
    {
      name: "scale",
      type: "number",
      description: "Maximum score value",
      isOptional: true,
      defaultValue: "10",
    }
  ]}
/>

## measure() Parameters

<PropertiesTable
  content={[
    {
      name: "input",
      type: "string",
      description: "The original query or prompt",
      isOptional: false,
    },
    {
      name: "output",
      type: "string",
      description: "The LLM's response to evaluate",
      isOptional: false,
    }
  ]}
/>

## Returns

<PropertiesTable
  content={[
    {
      name: "score",
      type: "number",
      description: "Relevancy score (0 to scale, default 0-10)",
    },
    {
      name: "reason",
      type: "string",
      description: "Detailed explanation of the score",
    }
  ]}
/>

## Scoring Details

The metric evaluates relevancy through multiple verdicts and calculates a score based on:
- Direct relevance to the query
- Completeness of the answer
- Accuracy of information
- Appropriate level of detail

The final score is normalized to the configured scale (default 0-10) where:
- 10: Perfect relevance
- 7-9: High relevance with minor issues
- 4-6: Moderate relevance with significant gaps
- 1-3: Low relevance with major issues
- 0: Completely irrelevant or incorrect

## Example with Custom Configuration

```typescript
const metric = new AnswerRelevancyMetric(
  {
    provider: "OPEN_AI",
    name: "gpt-4",
    apiKey: process.env.OPENAI_API_KEY
  },
  {
    uncertaintyWeight: 0.5, // Higher weight for uncertain verdicts
    scale: 5 // Use 0-5 scale instead of 0-10
  }
);

const result = await metric.measure({
  input: "What are the benefits of exercise?",
  output: "Regular exercise improves cardiovascular health, builds strength, and boosts mental wellbeing."
});
```

## Related

- [Prompt Alignment Metric](./prompt-alignment)
- [Context Precision Metric](./context-precision)
- [Tone Consistency Metric](./tone-consistency)
