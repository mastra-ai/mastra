---
title: "Reference: Langfuse Integration | Mastra Observability Docs"
description: Documentation for integrating Langfuse with Mastra, an open-source observability platform for LLM applications.
---

# Langfuse

[Langfuse](https://langfuse.com) ([GitHub](https://github.com/langfuse/langfuse)) is an open-source platform for LLM observability. It provides tracing and monitoring capabilities for AI applications, helping developers debug, analyze, and optimize their AI systems. Langfuse integrates with various tools and frameworks via native integrations, OpenTelemetry, and SDKs.

> **Note**: Currently, only AI-related calls will contain detailed telemetry data. Other operations will create traces but with limited information.

<Steps>

  ## Create a Mastra project
  If you don't have a Mastra project yet, you can create one using the Mastra CLI:

  ```bash
  npx create-mastra
  ```

  Move into the project directory:

  ```bash
  cd your-mastra-project
  ```

  You can get the full Mastra installation instructions [here](https://mastra.ai/docs/getting-started/installation)

  ## Set up Langfuse project
  Create a project in [Langfuse](https://cloud.langfuse.com) and get your API keys from the project settings page.


  ## Add environment variables
  Create or update your `.env.development` file with the following variables:

  ```bash
  # Your LLM API key
  OPENAI_API_KEY=your-api-key

  # Langfuse credentials
  LANGFUSE_SECRET_KEY=sk-lf-...
  LANGFUSE_PUBLIC_KEY=pk-lf-...
  LANGFUSE_HOST=https://cloud.langfuse.com # Optional. Defaults to https://cloud.langfuse.com
  ```

  ## Install the `langfuse-vercel` package
  Add the `langfuse-vercel` package to your project:

  ```bash
  npm install langfuse-vercel
  ```

  ## Set up an agent
  Create an agent in your project. For example, create a file `agents/chefAgent.ts`:

  ```typescript
  import { Agent } from "@mastra/core/agent";
  import { openai } from "@ai-sdk/openai";

  export const chefAgent = new Agent({
    name: "chef-agent",
    instructions:
      "You are Michel, a practical and experienced home chef " +
      "You help people cook with whatever ingredients they have available.",
    model: openai("gpt-4o-mini"),
  });
  ```

> **Note**: You can use any model provider from `ai-sdk`.


  ## Register agent and configure Langfuse
  Create or update your Mastra instance configuration to register the agent and configure Langfuse integration. For example, create a file `mastra.ts`:

  ```typescript
  import { Mastra } from "@mastra/core";
  import { LangfuseExporter } from "langfuse-vercel";
  import { chefAgent } from "./agents/chefAgent";

  export const mastra = new Mastra({
    agents: { chefAgent },
    telemetry: {
      serviceName: "ai", // this must be set to "ai" so that the LangfuseExporter thinks it's an AI SDK trace
      enabled: true,
      export: {
        type: "custom",
        exporter: new LangfuseExporter({
            publicKey: process.env.LANGFUSE_PUBLIC_KEY,
            secretKey: process.env.LANGFUSE_SECRET_KEY,
            baseUrl: process.env.LANGFUSE_HOST,
        }),
      },
    },
  });
  ```

  > **Note**: The `telemetry.serviceName` field must be set to "ai" so that the LangfuseExporter thinks it's an AI SDK trace.

  ## Run mastra dev server
  Start the Mastra development server:

  ```bash
  npm run dev
  ```

  Head over to the developer playground with the provided URL and start chatting with your agent.


  ### View traces in Langfuse
  Head over to your [Langfuse dashboard](https://cloud.langfuse.com) and you'll see the traces from your agent interactions. You can analyze the prompts, completions, and other details of your AI interactions.

  Here's an example of a trace:

  ![Mastra trace in Langfuse UI](https://langfuse.com/images/docs/langfuse-mastra-trace.png)

</Steps>

## References

- [Project Repository](https://github.com/akuya-ekorot/langfuse-mastra)
- [Langfuse Documentation](https://langfuse.com/docs)
- [langfuse-vercel package](https://www.npmjs.com/package/langfuse-vercel)