# Memory Class Reference

The `Memory` class provides a robust system for managing conversation history and thread-based message storage in Mastra. It enables persistent storage of conversations, semantic search capabilities, and efficient message retrieval.

## Usage Example

```typescript copy showLineNumbers
import { Memory } from "@mastra/memory";
import { MastraStorageLibSql } from "@mastra/core/storage";

const memory = new Memory({
  storage: new MastraStorageLibSql({
    url: ":memory:",
  }),
});
```

## Parameters

<PropertiesTable
  content={[
    {
      name: "storage",
      type: "MastraStorage",
      description: "Storage implementation for persisting memory data",
      isOptional: false,
    },
    {
      name: "vector",
      type: "MastraVector",
      description: "Vector store for semantic search capabilities",
      isOptional: true,
    },
    {
      name: "embedding",
      type: "EmbeddingConfig",
      description: "Configuration for text embeddings",
      isOptional: true,
    },
    {
      name: "options",
      type: "MemoryConfig",
      description: "General memory configuration options",
      isOptional: true,
    },
  ]}
/>

### options

<PropertiesTable
  content={[
    {
      name: "lastMessages",
      type: "number | false",
      description: "Number of most recent messages to retrieve. Set to false to disable.",
      isOptional: true,
      defaultValue: "40",
    },
    {
      name: "semanticRecall",
      type: "boolean | SemanticRecallConfig",
      description: "Enable semantic search in message history. Automatically enabled when vector store is provided.",
      isOptional: true,
      defaultValue: "false (true if vector store provided)",
    },
    {
      name: "topK",
      type: "number",
      description: "Number of similar messages to retrieve when using semantic search",
      isOptional: true,
      defaultValue: "2",
    },
    {
      name: "messageRange",
      type: "number | { before: number; after: number }",
      description: "Range of messages to include around semantic search results",
      isOptional: true,
      defaultValue: "2",
    },
    {
      name: "workingMemory",
      type: "{ enabled: boolean; template: string }",
      description: "Configuration for working memory feature that allows persistent storage of user information across conversations",
      isOptional: true,
      defaultValue: "{ enabled: false, template: '<user><first_name></first_name><last_name></last_name>...</user>' }",
    },
  ]}
/>

### Working Memory

The working memory feature allows agents to maintain persistent information across conversations. When enabled, the Memory class will automatically manage XML-based working memory updates through the conversation stream.

If no template is provided, the Memory class uses a default template that includes fields for user details, preferences, goals, and other contextual information. See the [Agent Memory Guide](/docs/agents/agent-memory#working-memory) for detailed usage examples and best practices.

### embedding

<PropertiesTable
  content={[
    {
      name: "provider",
      type: "string",
      description: "The embedding provider to use (e.g., 'OPEN_AI')",
      isOptional: false,
    },
    {
      name: "model",
      type: "string",
      description: "The specific embedding model to use (e.g., 'text-embedding-3-small')",
      isOptional: false,
    },
    {
      name: "maxRetries",
      type: "number",
      description: "Maximum number of retry attempts for embedding generation",
      isOptional: true,
      defaultValue: "3",
    },
  ]}
/>

## Additional Notes

### Vector Search Configuration

When using vector search capabilities, ensure you configure both the vector store and appropriate search options. Here's an example (just using the in-memory store):

```typescript copy showLineNumbers
import { Memory } from "@mastra/memory";
import { MastraStorageLibSql } from "@mastra/core/storage";
import { LibSQLVector } from "@mastra/vector-libsql";

const memory = new Memory({
  storage: new MastraStorageLibSql({
    url: ":memory:",
  }),
  vector: new LibSQLVector({
    url: ":memory:",
  }),
  embedding: {
    provider: "OPEN_AI",
    model: "text-embedding-3-small",
    maxRetries: 3,
  },
  options: {
    semanticRecall: {
      topK: 5,
      messageRange: 2
    }
  }
});
```

### Related

- [createThread](/docs/reference/memory/createThread.mdx)
- [query](/docs/reference/memory/query.mdx)
