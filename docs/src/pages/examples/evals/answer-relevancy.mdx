---
title: "Example: Answer Relevancy Evaluation | Evals | Mastra Docs"
description: Example of using the Answer Relevancy metric to evaluate how well LLM responses align with provided context.
---

import { GithubLink } from "../../../components/github-link";

# Answer Relevancy Evaluation

This example demonstrates how to use Mastra's Answer Relevancy metric to evaluate the relevance of LLM-generated responses to given contexts.

## Overview

The example shows how to:

1. Set up a Mastra agent for generating responses
2. Configure the Answer Relevancy metric
3. Evaluate responses for different scenarios
4. Interpret the evaluation results

## Setup

### Environment Setup

Make sure to set up your environment variables:

```bash filename=".env"
OPENAI_API_KEY=your_api_key_here
```

### Dependencies

Import the necessary dependencies:

```typescript copy showLineNumbers filename="src/index.ts"
import { openai } from '@ai-sdk/openai';
import { Agent } from '@mastra/core/agent';
import { AnswerRelevancyMetric } from '@mastra/evals/llm';
```

## Agent Creation

Create a Mastra agent for generating responses:

```typescript copy showLineNumbers{6} filename="src/index.ts"
const agent = new Agent({
  name: 'Example Agent',
  instructions: 'You are a helpful assistant that provides concise and relevant answers.',
  model: openai('gpt-4o-mini'),
});
```

## Metric Configuration

Set up the Answer Relevancy metric with custom parameters:

```typescript copy showLineNumbers{13} filename="src/index.ts"
const metric = new AnswerRelevancyMetric(openai('gpt-4o-mini'), {
  uncertaintyWeight: 0.3, // Weight for 'unsure' verdicts
  scale: 1, // Scale for the final score
});
```

## Example Usage

### High Relevancy Example

Evaluate a response that closely matches the context:

```typescript copy showLineNumbers filename="src/index.ts"
const context1 = `
  The Great Wall of China is over 13,000 miles long.
  Construction began more than 2,000 years ago during the Warring States period.
  It was built to protect Chinese states from nomadic invasions.
`;
const query1 = 'What was the purpose of building the Great Wall of China?';
const response1 = await agent.generate(query1, { context: context1 });

const result1 = await metric.measure(context1, response1.text);
console.log('High Relevancy Result:', {
  score: result1.score,
  reason: result1.info.reason,
});
```

### Low Relevancy Example

Evaluate a response where the context doesn't match the query:

```typescript copy showLineNumbers filename="src/index.ts"
const context2 = `
  The pyramids of Egypt were built during the Old Kingdom period.
  They served as tombs for pharaohs and their consorts.
  The largest pyramid is the Great Pyramid of Giza.
`;
const query2 = 'What materials were used to build the Great Wall of China?';
const response2 = await agent.generate(query2, { context: context2 });

const result2 = await metric.measure(context2, response2.text);
console.log('Low Relevancy Result:', {
  score: result2.score,
  reason: result2.info.reason,
});
```

## Understanding the Results

The metric provides:

1. A score between 0 and 1 (scaled by the `scale` parameter)
   - 1.0: Perfect relevancy
   - 0.7-0.9: High relevancy
   - 0.4-0.6: Moderate relevancy
   - 0.1-0.3: Low relevancy
   - 0.0: No relevancy

2. Detailed reasoning explaining:
   - How well the response uses context information
   - Any irrelevant or unsupported information
   - Areas where the response could be improved

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/evals/answer-relevancy"
  }
/>
