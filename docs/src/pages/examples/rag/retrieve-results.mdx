# Retrieve Results

```tsx copy
import { MDocument, embed, PgVector } from "@mastra/rag";

const doc = MDocument.fromText("Your text content...");

const chunks = await doc.chunk();

const { embedding } = await embed(chunks[0], {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002",
});

const pgVector = new PgVector('postgresql://localhost:5432/mydb');

const results = await pgVector.query("embeddings", embedding, 10);

// Example structure:
// [
//   { text: "A chunk related to main points...", metadata: { title: "...", ... } },
//   ...
// ]
 
// Incorporate these results into your LLM prompt
const prompt = `
  Using the context below, answer the user's question.
 
  Context:
  ${results.map(r => r.text).join('\n')}
 
  User's question: What are the main points in the article?
`;
 
// Send `prompt` to your LLM model for a more informed and grounded response.
```

