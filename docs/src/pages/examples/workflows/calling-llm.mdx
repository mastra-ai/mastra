import { GithubLink } from '../../../components/github-link';

# Calling an LLM from a Workflow

This example demonstrates how to create a workflow that streams responses from an LLM, showing both real-time output and final text handling.

```ts showLineNumbers copy
import { Mastra, Step, Workflow } from '@mastra/core';
import { z } from 'zod';

const newWorkflow = new Workflow({
  name: 'pass message to the workflow',
  triggerSchema: z.object({
    message: z.string(),
  }),
});

const replyAsPenguin = new Step({
  id: 'reply',
  outputSchema: z.object({
    reply: z.string(),
  }),
  execute: async ({ context, mastra }) => {
    const penguinCharacter = mastra?.llm?.({
      provider: 'OPEN_AI',
      name: 'gpt-4o',
    });

    const res = await penguinCharacter?.stream(context.machineContext?.triggerData?.message);

    if (!res) {
      return { reply: '' };
    }

    for await (const chunk of res?.textStream) {
      process.stdout.write(chunk);
    }

    const text = await res.text;
    return { reply: text };
  },
});

newWorkflow.step(replyAsPenguin);
newWorkflow.commit();

const mastra = new Mastra({
  workflows: { newWorkflow },
});

await mastra
  .getWorkflow('newWorkflow')
  .execute({ triggerData: { message: 'Give me a speech as skipper from penguin of madagascar' } });
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/workflows/calling-llm-from-workflow'}
/>
