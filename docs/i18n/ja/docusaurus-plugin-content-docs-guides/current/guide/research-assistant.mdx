---
title: "ガイド：RAG を使って研究論文アシスタントを構築する | Mastra RAG ガイド"
description: RAG を用いて学術論文を分析し、質問に回答できる AI リサーチアシスタントを作成するためのガイド。
---

import Steps from "@site/src/components/Steps";
import StepItem from "@site/src/components/StepItem";


# RAG を用いた研究論文アシスタントの構築

このガイドでは、Retrieval Augmented Generation (RAG) を用いて、学術論文を分析し、その内容に関する特定の質問に答えられる AI 研究アシスタントを作成します。

例として、基礎的な Transformer の論文「Attention Is All You Need」(https://arxiv.org/html/1706.03762) を使用します。データベースにはローカルの LibSQL を用います。

例として、基礎的な Transformer の論文[「Attention Is All You Need」](https://arxiv.org/html/1706.03762) を使用します。データベースにはローカルの LibSQL を用います。

## 前提条件

- Node.js `v20.0` 以降がインストールされていること
- 対応する[モデルプロバイダー](/models)の API キー
- 既存の Mastra プロジェクト（新規プロジェクトのセットアップは[インストールガイド](/docs/getting-started/installation)を参照）

## RAG の仕組み

RAG がどのように動作するか、そして各コンポーネントをどのように実装するかを見ていきましょう。

### ナレッジストア／インデックス

- テキストをベクトル表現に変換する
- コンテンツを数値ベクトルとして表現する
- **実装**: OpenAI の `text-embedding-3-small` で埋め込みを生成し、LibSQLVector に保存する

### Retriever

- 類似検索によって関連するコンテンツを見つける
- クエリの埋め込みを保存済みベクトルと照合する
- **実装**: 保存済みの埋め込みに対して類似検索を行うために LibSQLVector を使用します

### ジェネレーター

- 取得したコンテンツを LLM で処理する
- 文脈に基づく応答を生成する
- **実装**: 取得コンテンツに基づいて回答を生成するために GPT-4o-mini を使用します

この実装では以下を行います:

1. Transformer 論文を埋め込みベクトルに変換する
2. 高速な検索のためにそれらを LibSQLVector に保存する
3. 類似度検索で関連するセクションを特定する
4. 取得したコンテキストに基づき正確な応答を生成する

## エージェントの作成

エージェントの動作を定義し、Mastra プロジェクトに接続して、ベクターストアを作成します。

<Steps>

<StepItem title="追加の依存関係をインストールする">

追加の依存関係をインストールする

[インストールガイド](/docs/getting-started/installation)の手順を実行した後、以下の追加依存関係をインストールします:

```bash copy
npm install @mastra/rag@latest ai@^4.0.0
```

:::info

Mastra は現在 AI SDK の v5 をサポートしていません（[サポートスレッド](https://github.com/mastra-ai/mastra/issues/5470)参照）。このガイドでは v4 を使用してください。

:::

</StepItem>

<StepItem title="エージェントを定義する">

ここでは RAG 対応のリサーチアシスタントを作成します。エージェントは以下を使用します:

- 論文内の関連コンテンツを見つけるために、ベクターストア上でセマンティック検索を行う [Vector Query Tool](/reference/tools/vector-query-tool)
- クエリの理解と応答生成のための GPT-4o-mini
- 論文の分析方法、取得したコンテンツの効果的な活用、制約の明示を指示するカスタムインストラクション

新しいファイル `src/mastra/agents/researchAgent.ts` を作成し、エージェントを定義します:

```ts copy title="src/mastra/agents/researchAgent.ts"
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
import { createVectorQueryTool } from "@mastra/rag";

// Create a tool for semantic search over the paper embeddings
const vectorQueryTool = createVectorQueryTool({
  vectorStoreName: "libSqlVector",
  indexName: "papers",
  model: openai.embedding("text-embedding-3-small"),
});

export const researchAgent = new Agent({
  name: "Research Assistant",
  instructions: `You are a helpful research assistant that analyzes academic papers and technical documents.
    Use the provided vector query tool to find relevant information from your knowledge base,
    and provide accurate, well-supported answers based on the retrieved content.
    Focus on the specific content available in the tool and acknowledge if you cannot find sufficient information to answer a question.
    Base your responses only on the content provided, not on general knowledge.`,
  model: openai("gpt-4o-mini"),
  tools: {
    vectorQueryTool,
  },
});
```

</StepItem>

<StepItem title="ベクターストアを作成する">

プロジェクトのルートで `pwd` コマンドを実行して絶対パスを取得します。パスは次のようになります:

```bash
> pwd
/Users/your-name/guides/research-assistant
```

`src/mastra/index.ts` ファイルで、既存の内容と設定に次を追加します:

```ts copy title="src/mastra/index.ts" {2, 4-6, 9}
import { Mastra } from "@mastra/core/mastra";
import { LibSQLVector } from "@mastra/libsql";

const libSqlVector = new LibSQLVector({
  connectionUrl: "file:/Users/your-name/guides/research-assistant/vector.db",
});

export const mastra = new Mastra({
  vectors: { libSqlVector },
});
```

`connectionUrl` には `pwd` で取得した絶対パスを使用してください。これにより、`vector.db` ファイルがプロジェクトのルートに作成されます。

:::note

このガイドではローカルの LibSQL ファイルへのハードコードされた絶対パスを使用していますが、本番環境では適しません。代わりにリモートの永続データベースを使用してください。

:::

</StepItem>

<StepItem title="Mastra にエージェントを登録する">

`src/mastra/index.ts` ファイルで、エージェントを Mastra に追加します:

```ts copy title="src/mastra/index.ts" {3, 10}
import { Mastra } from "@mastra/core/mastra";
import { LibSQLVector } from "@mastra/libsql";
import { researchAgent } from "./agents/researchAgent";

const libSqlVector = new LibSQLVector({
  connectionUrl: "file:/Users/your-name/guides/research-assistant/vector.db",
});

export const mastra = new Mastra({
  agents: { researchAgent },
  vectors: { libSqlVector },
});
```

</StepItem>

</Steps>

## ドキュメントの処理

以下の手順では、研究論文を取得し、より小さなチャンクに分割して埋め込みを生成し、そのチャンク化した情報をベクターデータベースに保存します。

<Steps>

<StepItem title="論文を読み込み、処理する">

このステップでは、URL を指定して研究論文を取得し、ドキュメントオブジェクトに変換してから、扱いやすい小さなチャンクに分割します。チャンク化することで、処理がより高速かつ効率的になります。

新しいファイル `src/store.ts` を作成し、次を追加します:

```ts copy title="src/store.ts"
import { MDocument } from "@mastra/rag";

// Load the paper
const paperUrl = "https://arxiv.org/html/1706.03762";
const response = await fetch(paperUrl);
const paperText = await response.text();

// Create document and chunk it
const doc = MDocument.fromText(paperText);
const chunks = await doc.chunk({
  strategy: "recursive",
  maxSize: 512,
  overlap: 50,
  separators: ["\n\n", "\n", " "],
});

console.log("Number of chunks:", chunks.length);
```

ターミナルでファイルを実行します:

```bash copy
npx bun src/store.ts
```

次の出力が表示されるはずです:

```bash
Number of chunks: 892
```

</StepItem>

<StepItem title="埋め込みを作成して保存する">

最後に、RAG 用に次の準備を行います。

1. 各テキストチャンクの埋め込みを生成する
2. 埋め込みを保持するベクターストアのインデックスを作成する
3. 埋め込みとメタデータ（元テキストとソース情報）の両方をベクターデータベースに保存する

:::note

このメタデータは、ベクターストアが関連する一致を見つけたときに実際のコンテンツを返すために不可欠です。

:::

これにより、エージェントは効率的に検索して関連情報を取得できます。

`src/store.ts` ファイルを開き、次を追加します:

```ts copy title="src/store.ts" {2-4, 20-99}
import { MDocument } from "@mastra/rag";
import { openai } from "@ai-sdk/openai";
import { embedMany } from "ai";
import { mastra } from "./mastra";

// Load the paper
const paperUrl = "https://arxiv.org/html/1706.03762";
const response = await fetch(paperUrl);
const paperText = await response.text();

// Create document and chunk it
const doc = MDocument.fromText(paperText);
const chunks = await doc.chunk({
  strategy: "recursive",
  maxSize: 512,
  overlap: 50,
  separators: ["\n\n", "\n", " "],
});

// Generate embeddings
const { embeddings } = await embedMany({
  model: openai.embedding("text-embedding-3-small"),
  values: chunks.map((chunk) => chunk.text),
});

// Get the vector store instance from Mastra
const vectorStore = mastra.getVector("libSqlVector");

// Create an index for paper chunks
await vectorStore.createIndex({
  indexName: "papers",
  dimension: 1536,
});

// Store embeddings
await vectorStore.upsert({
  indexName: "papers",
  vectors: embeddings,
  metadata: chunks.map((chunk) => ({
    text: chunk.text,
    source: "transformer-paper",
  })),
});
```

最後に、スクリプトを再度実行して埋め込みを保存します:

```bash copy
npx bun src/store.ts
```

処理が成功すれば、ターミナルには出力やエラーは表示されません。

</StepItem>

</Steps>

## アシスタントをテストする

ベクターデータベースにすべての埋め込みが登録されたので、さまざまな種類のクエリでリサーチアシスタントを試せます。

新しいファイル `src/ask-agent.ts` を作成し、いくつかの種類のクエリを追加します:

```ts title="src/ask-agent.ts" copy
import { mastra } from "./mastra";
const agent = mastra.getAgent("researchAgent");

// 概念に関する基本的なクエリ
const query1 =
  "ニューラルネットワークを用いたシーケンスモデリングでは、どのような問題に直面しますか?";
const response1 = await agent.generate(query1);
console.log("\nクエリ:", query1);
console.log("応答:", response1.text);
```

スクリプトを実行する:

```bash copy
npx bun src/ask-agent.ts
```

次のような出力が表示されます：

```bash
Query: ニューラルネットワークを用いた系列モデリングにはどのような問題がありますか?
Response: ニューラルネットワークを用いた系列モデリングには、以下のような主要な課題があります:
1. 特に長い系列において、学習中に勾配消失や勾配爆発が発生すること
2. 入力における長期的な依存関係の処理が困難であること
3. 逐次処理により計算効率が制限されること
4. 計算の並列化が困難で、その結果学習時間が長くなること
```

別の質問を試してみてください：

```ts title="src/ask-agent.ts" copy
import { mastra } from "./mastra";
const agent = mastra.getAgent("researchAgent");

// 特定の調査結果に関するクエリ
const query2 = "翻訳品質においてどのような改善が達成されましたか?";
const response2 = await agent.generate(query2);
console.log("\nクエリ:", query2);
console.log("応答:", response2.text);
```

出力：

```
Query: 翻訳品質においてどのような改善が達成されましたか?
Response: このモデルは翻訳品質において大幅な改善を示し、WMT 2014英独翻訳タスクにおいて、
従来報告されていたモデルと比較して2.0 BLEUポイント以上の改善を達成すると同時に、
トレーニングコストの削減も実現しました。
```


### アプリケーションを提供する

API 経由でリサーチアシスタントを公開するため、Mastra サーバーを起動します:

```bash
mastra dev
```

リサーチアシスタントの利用可能時間:

```
http://localhost:4111/api/agents/researchAgent/generate
```

curl でテストする：

```bash
curl -X POST http://localhost:4111/api/agents/researchAgent/generate \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      { "role": "user", "content": "モデルの並列化についての主な知見は何ですか？" }
    ]
  }'
```


## 高度な RAG の例

より高度な RAG 手法の例を見てみましょう:

- メタデータで結果を絞り込む [Filter RAG](/examples/rag/usage/filter-rag)
- 情報密度を最適化する [Cleanup RAG](/examples/rag/usage/cleanup-rag)
- ワークフローで複雑な推論クエリに対応する [Chain of Thought RAG](/examples/rag/usage/cot-rag)
- 結果の関連性を高める [Rerank RAG](/examples/rag/usage/cleanup-rag)