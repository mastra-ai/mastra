---
title: "リファレンス: Google Gemini Live Voice | Voice | Mastra ドキュメント"
description: "GeminiLiveVoice クラスのドキュメント。Google の Gemini Live API を用い、Gemini API と Vertex AI の両方に対応したリアルタイムのマルチモーダル音声対話を提供します。"
---

<div id="google-gemini-live-voice">
  # Google Gemini Live Voice
</div>

GeminiLiveVoice クラスは、Google の Gemini Live API を用いてリアルタイムの音声対話を実現します。双方向の音声ストリーミング、ツールの呼び出し、セッション管理に対応し、標準的な Google API と Vertex AI のいずれの認証方式も利用できます。

<div id="usage-example">
  ## 使い方の例
</div>

```typescript
import { GeminiLiveVoice } from "@mastra/voice-google-gemini-live";
import { playAudio, getMicrophoneStream } from "@mastra/node-audio";

// Gemini API で初期化（API キーを使用）
const voice = new GeminiLiveVoice({
  apiKey: process.env.GOOGLE_API_KEY, // Gemini API に必須
  model: "gemini-2.0-flash-exp",
  speaker: "Puck", // デフォルトのボイス
  debug: true,
});

// または Vertex AI で初期化（OAuth を使用）
const voiceWithVertexAI = new GeminiLiveVoice({
  vertexAI: true,
  project: "your-gcp-project",
  location: "us-central1",
  serviceAccountKeyFile: "/path/to/service-account.json",
  model: "gemini-2.0-flash-exp",
  speaker: "Puck",
});

// もしくは VoiceConfig パターンを使用（他プロバイダーとの一貫性のため推奨）
const voiceWithConfig = new GeminiLiveVoice({
  speechModel: {
    name: "gemini-2.0-flash-exp",
    apiKey: process.env.GOOGLE_API_KEY,
  },
  speaker: "Puck",
  realtimeConfig: {
    model: "gemini-2.0-flash-exp",
    apiKey: process.env.GOOGLE_API_KEY,
    options: {
      debug: true,
      sessionConfig: {
        interrupts: { enabled: true },
      },
    },
  },
});

// 接続を確立（他のメソッドを使用する前に必須）
await voice.connect();

// イベントリスナーを設定
voice.on("speaker", (audioStream) => {
  // オーディオストリームを処理（NodeJS.ReadableStream）
  playAudio(audioStream);
});

voice.on("writing", ({ text, role }) => {
  // 書き起こしテキストを処理
  console.log(`${role}: ${text}`);
});

voice.on("turnComplete", ({ timestamp }) => {
  // ターン完了を処理
  console.log("ターン完了時刻:", timestamp);
});

// テキスト読み上げ
await voice.speak("こんにちは。本日どのようにお手伝いできますか？", {
  speaker: "Charon", // 既定のボイスを上書き
  responseModalities: ["AUDIO", "TEXT"],
});

// 音声入力を処理
const microphoneStream = getMicrophoneStream();
await voice.send(microphoneStream);

// セッション設定を更新
await voice.updateSessionConfig({
  speaker: "Kore",
  instructions: "回答はより簡潔にしてください",
});

// 終了時は切断
await voice.disconnect();
// または同期ラッパーを使用
voice.close();
```


<div id="configuration">
  ## 設定
</div>

<div id="constructor-options">
  ### コンストラクターのオプション
</div>

<PropertiesTable
  content={[
    {
      name: "apiKey",
      type: "string",
      description:
        "Gemini API の認証に使用する Google API キー。Vertex AI を使用しない場合は必須です。",
      isOptional: true,
    },
    {
      name: "model",
      type: "GeminiVoiceModel",
      description: "リアルタイム音声対話に使用するモデル ID。",
      isOptional: true,
      defaultValue: "'gemini-2.0-flash-exp'",
    },
    {
      name: "speaker",
      type: "GeminiVoiceName",
      description: "音声合成のデフォルトのボイス ID。",
      isOptional: true,
      defaultValue: "'Puck'",
    },
    {
      name: "vertexAI",
      type: "boolean",
      description: "認証に Gemini API の代わりに Vertex AI を使用します。",
      isOptional: true,
      defaultValue: "false",
    },
    {
      name: "project",
      type: "string",
      description: "Google Cloud のプロジェクト ID（Vertex AI で必須）。",
      isOptional: true,
    },
    {
      name: "location",
      type: "string",
      description: "Vertex AI 用の Google Cloud リージョン。",
      isOptional: true,
      defaultValue: "'us-central1'",
    },
    {
      name: "serviceAccountKeyFile",
      type: "string",
      description:
        "Vertex AI の認証に使用するサービス アカウント JSON キー ファイルへのパス。",
      isOptional: true,
    },
    {
      name: "serviceAccountEmail",
      type: "string",
      description:
        "なりすまし用のサービス アカウントのメールアドレス（キー ファイルの代替）。",
      isOptional: true,
    },
    {
      name: "instructions",
      type: "string",
      description: "モデルへのシステム指示。",
      isOptional: true,
    },
    {
      name: "sessionConfig",
      type: "GeminiSessionConfig",
      description:
        "割り込みやコンテキスト設定を含むセッション設定。",
      isOptional: true,
    },
    {
      name: "debug",
      type: "boolean",
      description: "トラブルシューティング用にデバッグ ログを有効にします。",
      isOptional: true,
      defaultValue: "false",
    },
  ]}
/>

<div id="session-configuration">
  ### セッション設定
</div>

<PropertiesTable
  content={[
    {
      name: "interrupts",
      type: "object",
      description: "割り込み処理の構成。",
      isOptional: true,
    },
    {
      name: "interrupts.enabled",
      type: "boolean",
      description: "割り込み処理を有効にするかどうか。",
      isOptional: true,
      defaultValue: "true",
    },
    {
      name: "interrupts.allowUserInterruption",
      type: "boolean",
      description: "ユーザーがモデルの応答を中断できるようにするかどうか。",
      isOptional: true,
      defaultValue: "true",
    },
    {
      name: "contextCompression",
      type: "boolean",
      description: "自動コンテキスト圧縮を有効にするかどうか。",
      isOptional: true,
      defaultValue: "false",
    },
  ]}
/>

<div id="methods">
  ## メソッド
</div>

<div id="connect">
  ### connect()
</div>

Gemini Live API への接続を確立します。speak、listen、または send メソッドを使用する前に呼び出す必要があります。

<PropertiesTable
  content={[
    {
      name: "runtimeContext",
      type: "object",
      description: "接続時の任意の実行時コンテキスト。",
      isOptional: true,
    },
    {
      name: "returns",
      type: "Promise<void>",
      description: "接続が確立されると解決される Promise。",
    },
  ]}
/>

<div id="speak">
  ### speak()
</div>

テキストを音声に変換してモデルに送信します。入力は文字列または読み取り可能なストリームを受け付けます。

<PropertiesTable
  content={[
    {
      name: "input",
      type: "string | NodeJS.ReadableStream",
      description: "音声に変換するテキストまたはテキストのストリーム。",
      isOptional: false,
    },
    {
      name: "options",
      type: "GeminiLiveVoiceOptions",
      description: "任意の音声設定。",
      isOptional: true,
    },
    {
      name: "options.speaker",
      type: "GeminiVoiceName",
      description: "この音声リクエストに使用する音声 ID。",
      isOptional: true,
      defaultValue: "コンストラクタで指定された speaker の値",
    },
    {
      name: "options.languageCode",
      type: "string",
      description: "レスポンスの言語コード。",
      isOptional: true,
    },
    {
      name: "options.responseModalities",
      type: "('AUDIO' | 'TEXT')[]",
      description: "モデルから受け取るレスポンスのモダリティ。",
      isOptional: true,
      defaultValue: "['AUDIO', 'TEXT']",
    },
  ]}
/>

戻り値: `Promise<void>`（レスポンスは `speaker` および `writing` イベントで発行されます）

<div id="listen">
  ### listen()
</div>

音声認識のための音声入力を処理します。読み取り可能な音声データのストリームを受け取り、文字起こしされたテキストを返します。

<PropertiesTable
  content={[
    {
      name: "audioStream",
      type: "NodeJS.ReadableStream",
      description: "文字起こし対象の音声ストリーム。",
      isOptional: false,
    },
    {
      name: "options",
      type: "GeminiLiveVoiceOptions",
      description: "オプションのリスニング設定。",
      isOptional: true,
    },
  ]}
/>

戻り値: `Promise<string>` - 文字起こしされたテキスト

<div id="send">
  ### send()
</div>

ライブのマイク入力など、連続的な音声ストリーミングのシナリオで、音声データをリアルタイムに Gemini サービスへストリーミングします。

<PropertiesTable
  content={[
    {
      name: "audioData",
      type: "NodeJS.ReadableStream | Int16Array",
      description: "サービスに送信する音声ストリームまたはバッファ。",
      isOptional: false,
    },
  ]}
/>

Returns: `Promise<void>`

<div id="updatesessionconfig">
  ### updateSessionConfig()
</div>

セッション設定を動的に更新します。音声設定やスピーカー選択、その他のランタイム設定の変更に使用できます。

<PropertiesTable
  content={[
    {
      name: "config",
      type: "Partial<GeminiLiveVoiceConfig>",
      description: "適用する設定の更新内容。",
      isOptional: false,
    },
  ]}
/>

戻り値: `Promise<void>`

<div id="addtools">
  ### addTools()
</div>

音声インスタンスにツールのセットを追加します。ツールを使うと、モデルは会話中に追加のアクションを実行できます。GeminiLiveVoice を Agent に追加すると、Agent に設定されたツールは自動的に音声インターフェースでも利用可能になります。

<PropertiesTable
  content={[
    {
      name: "tools",
      type: "ToolsInput",
      description: "適用するツールの設定。",
      isOptional: false,
    },
  ]}
/>

Returns: `void`

<div id="addinstructions">
  ### addInstructions()
</div>

モデルに対するシステム指示を追加または更新します。

<PropertiesTable
  content={[
    {
      name: "instructions",
      type: "string",
      description: "設定するシステム指示。",
      isOptional: true,
    },
  ]}
/>

戻り値: `void`

<div id="answer">
  ### answer()
</div>

モデルからの応答を起動します。このメソッドは、エージェントと統合された場合に主に内部で使用されます。

<PropertiesTable
  content={[
    {
      name: "options",
      type: "Record<string, unknown>",
      description: "answer リクエスト用のオプションパラメータ。",
      isOptional: true,
    },
  ]}
/>

戻り値: `Promise<void>`

<div id="getspeakers">
  ### getSpeakers()
</div>

Gemini Live API で利用可能な音声ボイスの一覧を返します。

Returns: `Promise<Array<{ voiceId: string; description?: string }>>`

<div id="disconnect">
  ### disconnect()
</div>

Gemini Live セッションから切断し、リソースを解放します。クリーンアップを適切に行う非同期メソッドです。

戻り値: `Promise<void>`

<div id="close">
  ### close()
</div>

disconnect() の同期ラッパー。内部で await せずに disconnect() を呼び出します。

戻り値: `void`

<div id="on">
  ### on()
</div>

音声イベントのリスナーを登録します。

<PropertiesTable
  content={[
    {
      name: "event",
      type: "string",
      description: "リッスンするイベント名。",
      isOptional: false,
    },
    {
      name: "callback",
      type: "Function",
      description: "イベント発生時に呼び出される関数。",
      isOptional: false,
    },
  ]}
/>

戻り値: `void`

<div id="off">
  ### off()
</div>

以前に登録したイベントリスナーを削除します。

<PropertiesTable
  content={[
    {
      name: "event",
      type: "string",
      description: "リッスンを停止するイベント名。",
      isOptional: false,
    },
    {
      name: "callback",
      type: "Function",
      description: "削除するコールバック関数。",
      isOptional: false,
    },
  ]}
/>

戻り値: `void`

<div id="events">
  ## イベント
</div>

GeminiLiveVoice クラスは次のイベントを発行します:

<PropertiesTable
  content={[
    {
      name: "speaker",
      type: "event",
      description:
        "モデルから音声データを受信したときに発行されます。コールバックは NodeJS.ReadableStream を受け取ります。",
    },
    {
      name: "speaking",
      type: "event",
      description:
        "音声メタデータとともに発行されます。コールバックは { audioData?: Int16Array, sampleRate?: number } を受け取ります。",
    },
    {
      name: "writing",
      type: "event",
      description:
        "音声の書き起こしテキストが利用可能になったときに発行されます。コールバックは { text: string, role: 'assistant' | 'user' } を受け取ります。",
    },
    {
      name: "session",
      type: "event",
      description:
        "セッション状態が変化したときに発行されます。コールバックは { state: 'connecting' | 'connected' | 'disconnected' | 'disconnecting' | 'updated', config?: object } を受け取ります。",
    },
    {
      name: "turnComplete",
      type: "event",
      description:
        "会話のターンが完了したときに発行されます。コールバックは { timestamp: number } を受け取ります。",
    },
    {
      name: "toolCall",
      type: "event",
      description:
        "モデルがツールの呼び出しを要求したときに発行されます。コールバックは { name: string, args: object, id: string } を受け取ります。",
    },
    {
      name: "usage",
      type: "event",
      description:
        "トークン使用状況とともに発行されます。コールバックは { inputTokens: number, outputTokens: number, totalTokens: number, modality: string } を受け取ります。",
    },
    {
      name: "error",
      type: "event",
      description:
        "エラー発生時に発行されます。コールバックは { message: string, code?: string, details?: unknown } を受け取ります。",
    },

    {
      name: "interrupt",
      type: "event",
      description:
        "割り込みイベント。コールバックは { type: 'user' | 'model', timestamp: number } を受け取ります。",
    },

]}
/>

<div id="available-models">
  ## 利用可能なモデル
</div>

利用可能な Gemini Live モデルは次のとおりです:

- `gemini-2.0-flash-exp`（デフォルト）
- `gemini-2.0-flash-exp-image-generation`
- `gemini-2.0-flash-live-001`
- `gemini-live-2.5-flash-preview-native-audio`
- `gemini-2.5-flash-exp-native-audio-thinking-dialog`
- `gemini-live-2.5-flash-preview`
- `gemini-2.6.flash-preview-tts`

<div id="available-voices">
  ## 利用可能な音声
</div>

利用できる音声オプションは次のとおりです:

- `Puck`（デフォルト）: 会話調で親しみやすい
- `Charon`: 低く、威厳がある
- `Kore`: 中立的でプロフェッショナル
- `Fenrir`: 温かみがあり、親しみやすい

<div id="authentication-methods">
  ## 認証方式
</div>

<div id="gemini-api-development">
  ### Gemini API（開発）
</div>

[Google AI Studio](https://makersuite.google.com/app/apikey) の API キーを使う最も簡単な方法：

```typescript
const voice = new GeminiLiveVoice({
  apiKey: "あなたのAPIキー", // Gemini API に必要
  model: "gemini-2.0-flash-exp",
});
```


<div id="vertex-ai-production">
  ### Vertex AI（本番環境）
</div>

OAuth 認証と Google Cloud Platform を使用する本番環境向け:

```typescript
// サービス アカウント キー ファイルを使用
const voice = new GeminiLiveVoice({
  vertexAI: true,
  project: "your-gcp-project",
  location: "us-central1",
  serviceAccountKeyFile: "/path/to/service-account.json",
});

// アプリケーション デフォルト認証情報（ADC）を使用
const voice = new GeminiLiveVoice({
  vertexAI: true,
  project: "your-gcp-project",
  location: "us-central1",
});

// サービス アカウントの偽装（インパーソネーション）を使用
const voice = new GeminiLiveVoice({
  vertexAI: true,
  project: "your-gcp-project",
  location: "us-central1",
  serviceAccountEmail: "service-account@project.iam.gserviceaccount.com",
});
```


<div id="advanced-features">
  ## 高度な機能
</div>

<div id="session-management">
  ### セッション管理
</div>

Gemini Live API は、ネットワーク障害時のセッション再開に対応しています:

```typescript
voice.on("sessionHandle", ({ handle, expiresAt }) => {
  // セッション再開用にハンドルを保存する
  saveSessionHandle(handle, expiresAt);
});

// 以前のセッションを再開する
const voice = new GeminiLiveVoice({
  sessionConfig: {
    enableResumption: true,
    maxDuration: "2h",
  },
});
```


<div id="tool-calling">
  ### ツール呼び出し
</div>

会話中にモデルが関数を呼び出せるようにする:

```typescript
import { z } from "zod";

voice.addTools({
  weather: {
    description: "天気情報を取得",
    parameters: z.object({
      location: z.string(),
    }),
    execute: async ({ location }) => {
      const weather = await getWeather(location);
      return weather;
    },
  },
});

voice.on("toolCall", ({ name, args, id }) => {
  console.log(`ツール呼び出し: ${name} 引数:`, args);
});
```


<div id="notes">
  ## 注意事項
</div>

- Gemini Live API はリアルタイム通信に WebSocket を使用します
- 音声は入力が 16 kHz の PCM16、出力が 24 kHz の PCM16 として処理されます
- 他のメソッドを使用する前に、音声インスタンスは `connect()` で接続しておく必要があります
- 処理が完了したら、リソースを正しく解放するために必ず `close()` を呼び出してください
- Vertex AI の認証には、適切な IAM 権限（`aiplatform.user` ロール）が必要です
- セッション再開機能により、ネットワーク中断から復旧できます
- この API はテキストおよび音声のリアルタイム対話をサポートします