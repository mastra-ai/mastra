---
title: "Mastra の Voice | Voice | Mastra ドキュメント"
description: Mastra の音声機能の概要。テキスト読み上げ、音声認識、リアルタイムの音声同時変換を含みます。
---

import { AudioPlayback } from "@site/src/components/AudioPlayback";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";


<div id="voice-in-mastra">
  # Mastra の Voice
</div>

Mastra の Voice システムは、音声インタラクション向けの統一インターフェースを提供し、アプリケーションでのテキスト読み上げ（TTS）、音声認識（STT）、およびリアルタイム音声変換（STS）機能を実現します。

<div id="adding-voice-to-agents">
  ## エージェントに音声を追加する
</div>

エージェントに音声機能を統合する方法は、[「エージェントに音声を追加する」](/docs/agents/adding-voice)のドキュメントをご参照ください。本セクションでは、単一および複数の音声プロバイダーの活用方法に加え、リアルタイムでのインタラクションについても解説します。

```typescript
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
import { OpenAIVoice } from "@mastra/voice-openai";

// TTS用のOpenAI音声を初期化

const voiceAgent = new Agent({
  name: "音声エージェント",
  instructions:
    "あなたはユーザーのタスクをサポートする音声アシスタントです。",
  model: openai("gpt-4o"),
  voice: new OpenAIVoice(),
});
```

次に、以下の音声機能を利用できます。


<div id="text-to-speech-tts">
  ### 音声合成（TTS）
</div>

Mastra の TTS 機能を使って、エージェントの応答を自然な音声に変換できます。
OpenAI、ElevenLabs など、複数のプロバイダーから選べます。

詳細な設定や高度な機能については、[Text-to-Speech ガイド](./text-to-speech)をご覧ください。

<Tabs>
  <TabItem value="OpenAI" label="OpenAI">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { OpenAIVoice } from "@mastra/voice-openai";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new OpenAIVoice(),
    });

    const { text } = await voiceAgent.generate("空は何色ですか?");

    // テキストを音声に変換してオーディオストリームに出力
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "default", // オプション: スピーカーを指定
      responseFormat: "wav", // オプション: レスポンス形式を指定
    });

    playAudio(audioStream);
    ```

    OpenAI の音声プロバイダーについて詳しくは、[OpenAI Voice Reference](/reference/voice/openai)をご覧ください。
  </TabItem>

  <TabItem value="azure" label="Azure">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { AzureVoice } from "@mastra/voice-azure";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "ユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new AzureVoice(),
    });

    const { text } = await voiceAgent.generate("空は何色ですか?");

    // テキストを音声に変換してオーディオストリームに出力
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "en-US-JennyNeural", // オプション:スピーカーを指定
    });

    playAudio(audioStream);
    ```

    Azure 音声プロバイダーの詳細は、[Azure Voice Reference](/reference/voice/azure) をご覧ください。
  </TabItem>

  <TabItem value="ElevenLabs" label="ElevenLabs">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { ElevenLabsVoice } from "@mastra/voice-elevenlabs";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new ElevenLabsVoice(),
    });

    const { text } = await voiceAgent.generate("空は何色ですか?");

    // テキストを音声に変換してオーディオストリームに出力
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "default", // オプション:スピーカーを指定
    });

    playAudio(audioStream);
    ```

    ElevenLabs の音声プロバイダーの詳細は、[ElevenLabs Voice Reference](/reference/voice/elevenlabs) をご覧ください。
  </TabItem>

  <TabItem value="PlayAI" label="PlayAI">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { PlayAIVoice } from "@mastra/voice-playai";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new PlayAIVoice(),
    });

    const { text } = await voiceAgent.generate("空は何色ですか?");

    // テキストを音声に変換してオーディオストリームに出力
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "default", // オプション: スピーカーを指定
    });

    playAudio(audioStream);
    ```

    PlayAI の音声プロバイダーについて詳しくは、[PlayAI Voice Reference](/reference/voice/playai)をご覧ください。
  </TabItem>

  <TabItem value="Google" label="Google">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { GoogleVoice } from "@mastra/voice-google";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "ユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new GoogleVoice(),
    });

    const { text } = await voiceAgent.generate("空は何色ですか?");

    // テキストを音声に変換してオーディオストリームに出力
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "en-US-Studio-O", // オプション:スピーカーを指定
    });

    playAudio(audioStream);
    ```

    Google の音声プロバイダーについて詳しくは、[Google Voice リファレンス](/reference/voice/google)をご覧ください。
  </TabItem>

  <TabItem value="Cloudflare" label="Cloudflare">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { CloudflareVoice } from "@mastra/voice-cloudflare";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "Voice Agent",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new CloudflareVoice(),
    });

    const { text } = await voiceAgent.generate("空は何色ですか?");

    // テキストを音声に変換してオーディオストリームに出力
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "default", // オプション: スピーカーを指定
    });

    playAudio(audioStream);
    ```

    Cloudflare の音声プロバイダーについて詳しくは、[Cloudflare Voice Reference](/reference/voice/cloudflare) をご覧ください。
  </TabItem>

  <TabItem value="Deepgram" label="Deepgram">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { DeepgramVoice } from "@mastra/voice-deepgram";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new DeepgramVoice(),
    });

    const { text } = await voiceAgent.generate("空は何色ですか?");

    // テキストを音声に変換してオーディオストリームに出力
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "aura-english-us", // オプション:スピーカーを指定
    });

    playAudio(audioStream);
    ```

    Deepgram の音声プロバイダーの詳細については、[Deepgram Voice リファレンス](/reference/voice/deepgram)を参照してください。
  </TabItem>

  <TabItem value="Speechify（スピーチファイ）" label="Speechify">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { SpeechifyVoice } from "@mastra/voice-speechify";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new SpeechifyVoice(),
    });

    const { text } = await voiceAgent.generate("空は何色ですか?");

    // テキストを音声に変換してオーディオストリームに出力
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "matthew", // オプション: 話者を指定
    });

    playAudio(audioStream);
    ```

    Speechify の音声プロバイダーの詳細は、[Speechify Voice Reference](/reference/voice/speechify) をご覧ください。
  </TabItem>

  <TabItem value="sarvam" label="Sarvam">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { SarvamVoice } from "@mastra/voice-sarvam";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "Voice Agent",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new SarvamVoice(),
    });

    const { text } = await voiceAgent.generate("空は何色ですか?");

    // テキストを音声に変換してオーディオストリームに変換
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "default", // オプション: スピーカーを指定
    });

    playAudio(audioStream);
    ```

    Sarvam の音声プロバイダーについて詳しくは、[Sarvam Voice Reference](/reference/voice/sarvam)をご覧ください。
  </TabItem>

  <TabItem value="murf" label="Murf">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { MurfVoice } from "@mastra/voice-murf";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new MurfVoice(),
    });

    const { text } = await voiceAgent.generate("空は何色ですか?");

    // テキストを音声に変換してオーディオストリームに出力
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "default", // オプション: スピーカーを指定
    });

    playAudio(audioStream);
    ```

    Murf の音声プロバイダーについて詳しくは、[Murf Voice Reference](/reference/voice/murf) をご覧ください。
  </TabItem>
</Tabs>

<div id="speech-to-text-stt">
  ### 音声認識（STT）
</div>

OpenAI、ElevenLabs などの各種プロバイダーを利用して、話し言葉をテキストに変換します。詳細な設定オプションについては、[音声認識](./speech-to-text)を参照してください。

サンプル音声ファイルは[こちら](https://github.com/mastra-ai/realtime-voice-demo/raw/refs/heads/main/how_can_i_help_you.mp3)からダウンロードできます。

<br />

<AudioPlayback audio="https://github.com/mastra-ai/realtime-voice-demo/raw/refs/heads/main/how_can_i_help_you.mp3" />

<Tabs>
  <TabItem value="OpenAI" label="OpenAI">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { OpenAIVoice } from "@mastra/voice-openai";
    import { createReadStream } from "fs";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "ユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new OpenAIVoice(),
    });

    // URLから音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`ユーザーの発言: ${transcript}`);

    // トランスクリプトに基づいて応答を生成
    const { text } = await voiceAgent.generate(transcript);
    ```

    OpenAI の音声プロバイダーの詳細は、[OpenAI Voice Reference](/reference/voice/openai)をご覧ください。
  </TabItem>

  <TabItem value="アジュール" label="Azure">
    ```typescript
    import { createReadStream } from "fs";
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { AzureVoice } from "@mastra/voice-azure";
    import { createReadStream } from "fs";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new AzureVoice(),
    });

    // URLから音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`ユーザーの発言: ${transcript}`);

    // 文字起こし結果に基づいて応答を生成
    const { text } = await voiceAgent.generate(transcript);
    ```

    Azure の音声プロバイダーの詳細は、[Azure Voice Reference](/reference/voice/azure) をご覧ください。
  </TabItem>

  <TabItem value="ElevenLabs" label="ElevenLabs">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { ElevenLabsVoice } from "@mastra/voice-elevenlabs";
    import { createReadStream } from "fs";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new ElevenLabsVoice(),
    });

    // URLから音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`ユーザーの発言: ${transcript}`);

    // 文字起こし結果に基づいて応答を生成
    const { text } = await voiceAgent.generate(transcript);
    ```

    ElevenLabs の音声プロバイダーについて詳しくは、[ElevenLabs Voice Reference](/reference/voice/elevenlabs) をご覧ください。
  </TabItem>

  <TabItem value="google" label="Google">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { GoogleVoice } from "@mastra/voice-google";
    import { createReadStream } from "fs";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new GoogleVoice(),
    });

    // URLから音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`ユーザーの発言: ${transcript}`);

    // トランスクリプトに基づいてレスポンスを生成
    const { text } = await voiceAgent.generate(transcript);
    ```

    Google の音声プロバイダーについて詳しくは、[Google Voice リファレンス](/reference/voice/google)をご覧ください。
  </TabItem>

  <TabItem value="Cloudflare" label="Cloudflare">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { CloudflareVoice } from "@mastra/voice-cloudflare";
    import { createReadStream } from "fs";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "あなたはユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new CloudflareVoice(),
    });

    // URLから音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`ユーザーの発言: ${transcript}`);

    // トランスクリプトに基づいてレスポンスを生成
    const { text } = await voiceAgent.generate(transcript);
    ```

    Cloudflare の音声プロバイダーの詳細は、[Cloudflare Voice Reference](/reference/voice/cloudflare) をご覧ください。
  </TabItem>

  <TabItem value="Deepgram" label="Deepgram">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { DeepgramVoice } from "@mastra/voice-deepgram";
    import { createReadStream } from "fs";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "ユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new DeepgramVoice(),
    });

    // URLから音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`ユーザーの発言: ${transcript}`);

    // トランスクリプトに基づいてレスポンスを生成
    const { text } = await voiceAgent.generate(transcript);
    ```

    Deepgram の音声プロバイダーに関する詳細は、[Deepgram Voice Reference](/reference/voice/deepgram) をご覧ください。
  </TabItem>

  <TabItem value="sarvam" label="Sarvam">
    ```typescript
    import { Agent } from "@mastra/core/agent";
    import { openai } from "@ai-sdk/openai";
    import { SarvamVoice } from "@mastra/voice-sarvam";
    import { createReadStream } from "fs";

    const voiceAgent = new Agent({
      name: "音声エージェント",
      instructions:
        "ユーザーのタスクをサポートする音声アシスタントです。",
      model: openai("gpt-4o"),
      voice: new SarvamVoice(),
    });

    // URLから音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`ユーザーの発言: ${transcript}`);

    // トランスクリプトに基づいて応答を生成
    const { text } = await voiceAgent.generate(transcript);
    ```

    Sarvam の音声プロバイダーについての詳細は、[Sarvam Voice Reference](/reference/voice/sarvam) をご覧ください。
  </TabItem>
</Tabs>

<div id="speech-to-speech-sts">
  ### Speech to Speech (STS)
</div>

音声同士の会話体験を実現します。統合APIにより、ユーザーとAIエージェント間でリアルタイムの音声対話が可能になります。
詳細な設定オプションや高度な機能については、[Speech to Speech](./speech-to-speech)をご覧ください。

<Tabs>
  <TabItem value="openai" label="OpenAI">

```typescript
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
import { playAudio, getMicrophoneStream } from "@mastra/node-audio";
import { OpenAIRealtimeVoice } from "@mastra/voice-openai-realtime";

const voiceAgent = new Agent({
  name: "Voice Agent",
  instructions:
    "You are a voice assistant that can help users with their tasks.",
  model: openai("gpt-4o"),
  voice: new OpenAIRealtimeVoice(),
});

// Listen for agent audio responses
voiceAgent.voice.on("speaker", ({ audio }) => {
  playAudio(audio);
});

// Initiate the conversation
await voiceAgent.voice.speak("How can I help you today?");

// Send continuous audio from the microphone
const micStream = getMicrophoneStream();
await voiceAgent.voice.send(micStream);
```

OpenAI の音声プロバイダーに関する詳細は、[OpenAI Voice Reference](/reference/voice/openai-realtime)をご覧ください。

  </TabItem>
  <TabItem value="google" label="Google">

```typescript
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
import { playAudio, getMicrophoneStream } from "@mastra/node-audio";
import { GeminiLiveVoice } from "@mastra/voice-google-gemini-live";

const voiceAgent = new Agent({
  name: "Voice Agent",
  instructions:
    "You are a voice assistant that can help users with their tasks.",
  model: openai("gpt-4o"),
  voice: new GeminiLiveVoice({
    // Live API mode
    apiKey: process.env.GOOGLE_API_KEY,
    model: "gemini-2.0-flash-exp",
    speaker: "Puck",
    debug: true,
    // Vertex AI alternative:
    // vertexAI: true,
    // project: 'your-gcp-project',
    // location: 'us-central1',
    // serviceAccountKeyFile: '/path/to/service-account.json',
  }),
});

// Connect before using speak/send
await voiceAgent.voice.connect();

// Listen for agent audio responses
voiceAgent.voice.on("speaker", ({ audio }) => {
  playAudio(audio);
});

// Listen for text responses and transcriptions
voiceAgent.voice.on("writing", ({ text, role }) => {
  console.log(`${role}: ${text}`);
});

// Initiate the conversation
await voiceAgent.voice.speak("How can I help you today?");

// Send continuous audio from the microphone
const micStream = getMicrophoneStream();
await voiceAgent.voice.send(micStream);
```

Google Gemini Live の音声プロバイダーに関する詳細は、[Google Gemini Live Reference](/reference/voice/google-gemini-live)をご覧ください。

  </TabItem>
</Tabs>

<div id="voice-configuration">
  ## 音声の設定
</div>

各音声プロバイダーは、さまざまなモデルやオプションで構成できます。以下に、サポートされているすべてのプロバイダーの詳細な設定項目を示します。

<Tabs>
  <TabItem value="OpenAI" label="OpenAI">
    ```typescript
    // OpenAI音声設定
    const voice = new OpenAIVoice({
      speechModel: {
        name: "gpt-3.5-turbo", // モデル名の例
        apiKey: process.env.OPENAI_API_KEY,
        language: "en-US", // 言語コード
        voiceType: "neural", // 音声モデルのタイプ
      },
      listeningModel: {
        name: "whisper-1", // モデル名の例
        apiKey: process.env.OPENAI_API_KEY,
        language: "en-US", // 言語コード
        format: "wav", // 音声フォーマット
      },
      speaker: "alloy", // スピーカー名の例
    });
    ```

    OpenAI の音声プロバイダーの詳細については、[OpenAI Voice Reference](/reference/voice/openai) をご覧ください。
  </TabItem>

  <TabItem value="Azure" label="Azure">
    ```typescript
    // Azure音声設定
    const voice = new AzureVoice({
      speechModel: {
        name: "en-US-JennyNeural", // モデル名の例
        apiKey: process.env.AZURE_SPEECH_KEY,
        region: process.env.AZURE_SPEECH_REGION,
        language: "en-US", // 言語コード
        style: "cheerful", // 音声スタイル
        pitch: "+0Hz", // ピッチ調整
        rate: "1.0", // 発話速度
      },
      listeningModel: {
        name: "en-US", // モデル名の例
        apiKey: process.env.AZURE_SPEECH_KEY,
        region: process.env.AZURE_SPEECH_REGION,
        format: "simple", // 出力形式
      },
    });
    ```

    Azure の音声プロバイダーの詳細は、[Azure Voice Reference](/reference/voice/azure) をご覧ください。
  </TabItem>

  <TabItem value="ElevenLabs" label="ElevenLabs">
    ```typescript
    // ElevenLabs音声設定
    const voice = new ElevenLabsVoice({
      speechModel: {
        voiceId: "your-voice-id", // 音声IDの例
        model: "eleven_multilingual_v2", // モデル名の例
        apiKey: process.env.ELEVENLABS_API_KEY,
        language: "en", // 言語コード
        emotion: "neutral", // 感情設定
      },
      // ElevenLabsには個別のリスニングモデルがない場合があります
    });
    ```

    ElevenLabs の音声プロバイダーについての詳細は、[ElevenLabs Voice Reference](/reference/voice/elevenlabs) をご参照ください。
  </TabItem>

  <TabItem value="Play.ai" label="PlayAI">
    ```typescript
    // PlayAI音声設定
    const voice = new PlayAIVoice({
      speechModel: {
        name: "playai-voice", // モデル名の例
        speaker: "emma", // 話者名の例
        apiKey: process.env.PLAYAI_API_KEY,
        language: "en-US", // 言語コード
        speed: 1.0, // 読み上げ速度
      },
      // PlayAIには別個のリスニングモデルがない可能性があります
    });
    ```

    PlayAI の音声プロバイダーの詳細は、[PlayAI Voice Reference](/reference/voice/playai)をご覧ください。
  </TabItem>

  <TabItem value="Google" label="Google">
    ```typescript
    // Google Voice 設定
    const voice = new GoogleVoice({
      speechModel: {
        name: "en-US-Studio-O", // モデル名の例
        apiKey: process.env.GOOGLE_API_KEY,
        languageCode: "en-US", // 言語コード
        gender: "FEMALE", // 音声の性別
        speakingRate: 1.0, // 発話速度
      },
      listeningModel: {
        name: "en-US", // モデル名の例
        sampleRateHertz: 16000, // サンプリングレート
      },
    });
    ```

    Google 音声プロバイダーの詳細については、[Google Voice リファレンス](/reference/voice/google)をご覧ください。
  </TabItem>

  <TabItem value="Cloudflare" label="Cloudflare">
    ```typescript
    // Cloudflare音声設定
    const voice = new CloudflareVoice({
      speechModel: {
        name: "cloudflare-voice", // モデル名の例
        accountId: process.env.CLOUDFLARE_ACCOUNT_ID,
        apiToken: process.env.CLOUDFLARE_API_TOKEN,
        language: "en-US", // 言語コード
        format: "mp3", // 音声フォーマット
      },
      // Cloudflareには個別のリスニングモデルがない可能性があります
    });
    ```

    Cloudflare の音声プロバイダーについて詳しくは、[Cloudflare Voice Reference](/reference/voice/cloudflare) をご覧ください。
  </TabItem>

  <TabItem value="Deepgram" label="Deepgram">
    ```typescript
    // Deepgram音声設定
    const voice = new DeepgramVoice({
      speechModel: {
        name: "nova-2", // モデル名の例
        speaker: "aura-english-us", // 話者名の例
        apiKey: process.env.DEEPGRAM_API_KEY,
        language: "en-US", // 言語コード
        tone: "formal", // トーン設定
      },
      listeningModel: {
        name: "nova-2", // モデル名の例
        format: "flac", // オーディオ形式
      },
    });
    ```

    Deepgram の音声プロバイダーについて詳しくは、[Deepgram Voice リファレンス](/reference/voice/deepgram)をご覧ください。
  </TabItem>

  <TabItem value="Speechify" label="Speechify">
    ```typescript
    // Speechify 音声設定
    const voice = new SpeechifyVoice({
      speechModel: {
        name: "speechify-voice", // モデル名の例
        speaker: "matthew", // 話者名の例
        apiKey: process.env.SPEECHIFY_API_KEY,
        language: "en-US", // 言語コード
        speed: 1.0, // 読み上げ速度
      },
      // Speechify には別個のリスニングモデルがない場合があります
    });
    ```

    Speechify の音声プロバイダーについて詳しくは、[Speechify Voice Reference](/reference/voice/speechify) をご覧ください。
  </TabItem>

  <TabItem value="sarvam" label="Sarvam">
    ```typescript
    // Sarvam音声設定
    const voice = new SarvamVoice({
      speechModel: {
        name: "sarvam-voice", // モデル名の例
        apiKey: process.env.SARVAM_API_KEY,
        language: "en-IN", // 言語コード
        style: "conversational", // スタイル設定
      },
      // Sarvamには個別のリスニングモデルがない可能性があります
    });
    ```

    Sarvam のボイスプロバイダーについての詳細は、[Sarvam Voice Reference](/reference/voice/sarvam) をご覧ください。
  </TabItem>

  <TabItem value="murf" label="Murf">
    ```typescript
    // Murf音声設定
    const voice = new MurfVoice({
      speechModel: {
        name: "murf-voice", // モデル名の例
        apiKey: process.env.MURF_API_KEY,
        language: "en-US", // 言語コード
        emotion: "happy", // 感情設定
      },
      // Murfには個別のリスニングモデルがない場合があります
    });
    ```

    Murf の音声プロバイダーの詳細は、[Murf Voice Reference](/reference/voice/murf) をご覧ください。
  </TabItem>

  <TabItem value="openai-realtime" label="OpenAI リアルタイム">
    ```typescript
    // OpenAI リアルタイム音声設定
    const voice = new OpenAIRealtimeVoice({
      speechModel: {
        name: "gpt-3.5-turbo", // モデル名の例
        apiKey: process.env.OPENAI_API_KEY,
        language: "en-US", // 言語コード
      },
      listeningModel: {
        name: "whisper-1", // モデル名の例
        apiKey: process.env.OPENAI_API_KEY,
        format: "ogg", // 音声フォーマット
      },
      speaker: "alloy", // スピーカー名の例
    });
    ```

    OpenAI Realtime の音声プロバイダーについて詳しくは、[OpenAI Realtime Voice Reference](/reference/voice/openai-realtime)をご参照ください。
  </TabItem>

  <TabItem value="google-gemini-live" label="Google Gemini Live">
    ```typescript
    // Google Gemini Live 音声設定
    const voice = new GeminiLiveVoice({
      speechModel: {
        name: "gemini-2.0-flash-exp", // モデル名の例
        apiKey: process.env.GOOGLE_API_KEY,
      },
      speaker: "Puck", // スピーカー名の例
      // Google Gemini Live は音声認識と音声合成のモデルが統合されたリアルタイム双方向APIです
    });
    ```

    Google Gemini Live の音声プロバイダーの詳細は、[Google Gemini Live リファレンス](/reference/voice/google-gemini-live)をご覧ください。
  </TabItem>
</Tabs>

<div id="using-multiple-voice-providers">
  ### 複数の音声プロバイダーの利用
</div>

この例では、Mastra で 2 種類の音声プロバイダーを作成して使用する方法を示します。音声認識（STT）には OpenAI を、音声合成（TTS）には PlayAI を使用します。

まず、必要な設定を行ったうえで、各音声プロバイダーのインスタンスを作成します。

```typescript
import { OpenAIVoice } from "@mastra/voice-openai";
import { PlayAIVoice } from "@mastra/voice-playai";
import { CompositeVoice } from "@mastra/core/voice";
import { playAudio, getMicrophoneStream } from "@mastra/node-audio";

// STT 用に OpenAI の音声機能を初期化
const input = new OpenAIVoice({
  listeningModel: {
    name: "whisper-1",
    apiKey: process.env.OPENAI_API_KEY,
  },
});

// TTS 用に PlayAI の音声機能を初期化
const output = new PlayAIVoice({
  speechModel: {
    name: "playai-voice",
    apiKey: process.env.PLAYAI_API_KEY,
  },
});

// CompositeVoice を使ってプロバイダーを統合
const voice = new CompositeVoice({
  input,
  output,
});

// 統合した音声プロバイダーで音声対話を実装
const audioStream = getMicrophoneStream(); // この関数が音声入力を取得する想定
const transcript = await voice.listen(audioStream);

// 文字起こし結果をログ出力
console.log("文字起こし結果:", transcript);

// テキストを音声に変換
const responseAudio = await voice.speak(`あなたは次のように言いました: ${transcript}`, {
  speaker: "default", // 任意: 話者を指定
  responseFormat: "wav", // 任意: 応答フォーマットを指定
});

// 音声応答を再生
playAudio(responseAudio);
```

CompositeVoice の詳細は、[CompositeVoice リファレンス](/reference/voice/composite-voice)をご参照ください。


<div id="more-resources">
  ## 参考情報
</div>

- [CompositeVoice](/reference/voice/composite-voice)
- [MastraVoice](/reference/voice/mastra-voice)
- [OpenAI Voice](/reference/voice/openai)
- [OpenAI Realtime Voice](/reference/voice/openai-realtime)
- [Azure Voice](/reference/voice/azure)
- [Google Voice](/reference/voice/google)
- [Google Gemini Live Voice](/reference/voice/google-gemini-live)
- [Deepgram Voice](/reference/voice/deepgram)
- [PlayAI Voice](/reference/voice/playai)
- [音声のサンプル](/examples/voice/text-to-speech)