---
title: ドキュメントの分割と埋め込み | RAG | Mastra ドキュメント
description: 効率的な処理と検索のための、Mastra におけるドキュメントの分割と埋め込みに関するガイド。
---

<div id="chunking-and-embedding-documents">
  # ドキュメントの分割と埋め込み
</div>

処理の前に、コンテンツから MDocument インスタンスを作成します。さまざまな形式から初期化できます:

```ts showLineNumbers copy
const docFromText = MDocument.fromText("プレーンテキストの内容...");
const docFromHTML = MDocument.fromHTML("<html>HTMLの内容...</html>");
const docFromMarkdown = MDocument.fromMarkdown("# Markdownの内容...");
const docFromJSON = MDocument.fromJSON(`{ "key": "value" }`);
```


<div id="step-1-document-processing">
  ## ステップ 1: ドキュメント処理
</div>

`chunk` を使ってドキュメントを扱いやすいサイズに分割します。Mastra では、ドキュメントの種類に最適化された複数のチャンク分割戦略をサポートしています:

* `recursive`: コンテンツ構造に基づくスマートな分割
* `character`: シンプルな文字ベースの分割
* `token`: トークンを考慮した分割
* `markdown`: Markdown に配慮した分割
* `semantic-markdown`: 関連する見出し系統に基づく Markdown 分割
* `html`: HTML 構造に配慮した分割
* `json`: JSON 構造に配慮した分割
* `latex`: LaTeX 構造に配慮した分割
* `sentence`: 文単位での分割

**注:** 各戦略は、その分割手法に最適化された異なるパラメータを受け付けます。

`recursive` 戦略の使用例は次のとおりです:

```ts showLineNumbers copy
const chunks = await doc.chunk({
  strategy: "recursive",
  maxSize: 512,
  overlap: 50,
  separators: ["\n"],
  extract: {
    metadata: true, // メタデータを抽出する（オプション）
  },
});
```

文構造の保持が重要なテキストの場合は、`sentence` 戦略の使用例を次に示します。

```ts showLineNumbers copy
const chunks = await doc.chunk({
  strategy: "sentence",
  maxSize: 450,
  minSize: 50,
  overlap: 0,
  sentenceEnders: ["."],
  keepSeparator: true,
});
```

セクション同士の意味関係を保つことが重要な Markdown ドキュメント向けに、`semantic-markdown` 戦略の使用例を次に示します。

```ts showLineNumbers copy
const chunks = await doc.chunk({
  strategy: "semantic-markdown",
  joinThreshold: 500,
  modelName: "gpt-3.5-turbo",
});
```

**注:** メタデータ抽出では LLM を呼び出す場合があるため、API キーが設定されていることを確認してください。

チャンク化の戦略については、[チャンクに関するドキュメント](/reference/rag/chunk)でさらに詳しく解説しています。


<div id="step-2-embedding-generation">
  ## ステップ 2: 埋め込みの生成
</div>

お好みのプロバイダーを使って、チャンクを埋め込みベクトルに変換します。Mastra は、モデルルーターまたは AI SDK パッケージを通じて埋め込みモデルをサポートしています。

<div id="using-the-model-router-recommended">
  ### モデルルーターの使用（推奨）
</div>

最も簡単な方法は、`provider/model` という文字列を使って Mastra のモデルルーターを利用することです。

```ts showLineNumbers copy
import { ModelRouterEmbeddingModel } from "@mastra/core";
import { embedMany } from "ai";

const embeddingModel = new ModelRouterEmbeddingModel(
  "openai/text-embedding-3-small",
);

const { embeddings } = await embedMany({
  model: embeddingModel,
  values: chunks.map((chunk) => chunk.text),
});
```

対応している埋め込みモデル:

* **OpenAI**: `text-embedding-3-small`, `text-embedding-3-large`, `text-embedding-ada-002`
* **Google**: `gemini-embedding-001`, `text-embedding-004`

モデルルーターは、環境変数からの API キーの検出を自動で処理します。


<div id="using-ai-sdk-packages">
  ### AI SDK パッケージの利用
</div>

AI SDK の埋め込みモデルは、直接利用することもできます。

```ts showLineNumbers copy
import { openai } from "@ai-sdk/openai";
import { embedMany } from "ai";

const { embeddings } = await embedMany({
  model: openai.embedding("text-embedding-3-small"),
  values: chunks.map((chunk) => chunk.text),
});
```

埋め込み関数は、テキストの意味を表す数値配列（ベクトル）を返し、ベクトルデータベースでの類似検索にそのまま使用できます。


<div id="configuring-embedding-dimensions">
  ### 埋め込み次元の設定
</div>

埋め込みモデルは通常、固定の次元数を持つベクトル（例: OpenAI の `text-embedding-3-small` は 1536 次元）を出力します。
一部のモデルはこの次元数の削減に対応しており、次のような効果が期待できます:

* ベクターデータベースの保存容量を削減
* 類似検索の計算コストを削減

以下は対応モデルの例です:

OpenAI（text-embedding-3 系）:

```ts
const { embeddings } = await embedMany({
  model: openai.embedding("text-embedding-3-small", {
    dimensions: 256, // text-embedding-3 以降でのみ対応
  }),
  values: chunks.map((chunk) => chunk.text),
});
```

Google（text-embedding-004）：

```ts
const { embeddings } = await embedMany({
  model: google.textEmbeddingModel("text-embedding-004", {
    outputDimensionality: 256, // 末尾側の余分な成分を切り落とします
  }),
  values: chunks.map((chunk) => chunk.text),
});
```


<div id="vector-database-compatibility">
  ### ベクターデータベースの互換性
</div>

埋め込みを保存する際は、ベクターデータベースのインデックスを使用する埋め込みモデルの出力次元数に合わせて設定する必要があります。次元数が一致しない場合、エラーが発生したり、データが破損したりする恐れがあります。

<div id="example-complete-pipeline">
  ## 例：完全なパイプライン
</div>

以下は、両方のプロバイダーを用いたドキュメント処理と埋め込み生成の例です：

```ts showLineNumbers copy
import { embedMany } from "ai";
import { openai } from "@ai-sdk/openai";
import { cohere } from "@ai-sdk/cohere";

import { MDocument } from "@mastra/rag";

// ドキュメントを初期化
const doc = MDocument.fromText(`
  気候変動は世界の農業に深刻な課題を突きつけている。
  気温上昇と降水パターンの変化は作物収量に影響を及ぼす。
`);

// チャンクを作成
const chunks = await doc.chunk({
  strategy: "recursive",
  maxSize: 256,
  overlap: 50,
});

// OpenAI で埋め込みを生成
const { embeddings: openAIEmbeddings } = await embedMany({
  model: openai.embedding("text-embedding-3-small"),
  values: chunks.map((chunk) => chunk.text),
});

// または

// Cohere で埋め込みを生成
const { embeddings: cohereEmbeddings } = await embedMany({
  model: cohere.embedding("embed-english-v3.0"),
  values: chunks.map((chunk) => chunk.text),
});

// ベクターデータベースに埋め込みを保存
await vectorStore.upsert({
  indexName: "embeddings",
  vectors: embeddings,
});
```


##

さまざまなチャンク戦略や埋め込み設定の例については、次を参照してください：

- [チャンクのリファレンス](/reference/rag/chunk)
- [埋め込みのリファレンス](/reference/rag/embeddings)

ベクターデータベースと埋め込みの詳細については、次を参照してください：

- [ベクターデータベース](./vector-databases)
- [埋め込み API リファレンス](/reference/rag/embeddings)