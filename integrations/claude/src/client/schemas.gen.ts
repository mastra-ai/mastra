// This file is auto-generated by @hey-api/openapi-ts

export const ErrorSchema = {
    type: 'object',
    properties: {
        type: {
            type: 'string',
            nullable: false
        },
        message: {
            type: 'string',
            nullable: false
        }
    },
    required: ['type', 'message']
} as const;

export const ErrorResponseSchema = {
    type: 'object',
    properties: {
        error: {
            '$ref': '#/components/schemas/Error'
        }
    },
    required: ['error']
} as const;

export const CreateCompletionRequestSchema = {
    type: 'object',
    properties: {
        model: {
            description: `The model that will complete your prompt.
As we improve Claude, we develop new versions of it that you can query.
This parameter controls which version of Claude answers your request.
Right now we are offering two model families: Claude, and Claude Instant.
You can use them by setting model to "claude-2" or "claude-instant-1", respectively.
See models for additional details.
`,
            oneOf: [
                {
                    type: 'string'
                },
                {
                    type: 'string',
                    enum: ['claude-2', 'claude-2.0', 'claude-instant-1', 'claude-instant-1.1']
                }
            ],
            'x-oaiTypeLabel': 'string'
        },
        prompt: {
            description: `The prompt that you want Claude to complete.

For proper response generation you will need to format your prompt as follows:
\n\nHuman: \${userQuestion}\n\nAssistant:
See our comments on prompts for more context.
`,
            default: '<|endoftext|>',
            nullable: true,
            oneOf: [
                {
                    type: 'string',
                    default: '',
                    example: 'This is a test.'
                },
                {
                    type: 'array',
                    items: {
                        type: 'string',
                        default: '',
                        example: 'This is a test.'
                    }
                },
                {
                    type: 'array',
                    minItems: 1,
                    items: {
                        type: 'integer'
                    },
                    example: '[1212, 318, 257, 1332, 13]'
                },
                {
                    type: 'array',
                    minItems: 1,
                    items: {
                        type: 'array',
                        minItems: 1,
                        items: {
                            type: 'integer'
                        }
                    },
                    example: '[[1212, 318, 257, 1332, 13]]'
                }
            ]
        },
        max_tokens_to_sample: {
            type: 'integer',
            minimum: 1,
            default: 256,
            example: 256,
            nullable: true,
            description: `The maximum number of tokens to generate before stopping.

Note that our models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.
`
        },
        temperature: {
            type: 'number',
            minimum: 0,
            maximum: 1,
            default: 1,
            example: 1,
            nullable: true,
            description: `Amount of randomness injected into the response.

Defaults to 1. Ranges from 0 to 1. Use temp closer to 0 for analytical / multiple choice, and closer to 1 for creative and generative tasks.
`
        },
        top_p: {
            type: 'number',
            minimum: 0,
            maximum: 1,
            default: 1,
            example: 1,
            nullable: true,
            description: `Use nucleus sampling.

In nucleus sampling, we compute the cumulative distribution over all the options 
for each subsequent token in decreasing probability order and cut it off once 
it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.
`
        },
        top_k: {
            type: 'number',
            minimum: 0,
            default: 5,
            example: 5,
            nullable: true,
            description: `Only sample from the top K options for each subsequent token.

Used to remove "long tail" low probability responses. Learn more technical details here.
`
        },
        stream: {
            description: `Whether to incrementally stream the response using server-sent events.
See this guide to SSE events for details.type: boolean
`,
            nullable: true,
            default: false
        },
        stop_sequences: {
            description: `Sequences that will cause the model to stop generating completion text.
Our models stop on "\n\nHuman:", and may include additional built-in stop sequences in the future. By providing the stop_sequences parameter, you may include additional strings that will cause the model to stop generating.
`,
            default: null,
            nullable: true,
            oneOf: [
                {
                    type: 'string',
                    default: '<|endoftext|>',
                    example: `
`,
                    nullable: true
                },
                {
                    type: 'array',
                    minItems: 1,
                    maxItems: 4,
                    items: {
                        type: 'string',
                        example: '["\\n"]'
                    }
                }
            ]
        },
        metadata: {
            type: 'object',
            properties: {
                user_id: {
                    type: 'string',
                    example: '13803d75-b4b5-4c3e-b2a2-6f21399b021b',
                    description: `An external identifier for the user who is associated with the request.

This should be a uuid, hash value, or other opaque identifier. Anthropic may use this id to help detect abuse. 
Do not include any identifying information such as name, email address, or phone number.
`
                }
            },
            description: `An object describing metadata about the request.
`
        }
    },
    required: ['model', 'prompt', 'max_tokens_to_sample']
} as const;

export const CreateCompletionResponseSchema = {
    type: 'object',
    properties: {
        stop_reason: {
            type: 'string',
            enum: ['stop_sequence', 'max_tokens'],
            description: `The reason that we stopped sampling.

This may be one the following values:

"stop_sequence": we reached a stop sequence — either provided by you via the stop_sequences parameter, or a stop sequence built into the model
"max_tokens": we exceeded max_tokens_to_sample or the model's maximum
`
        },
        model: {
            type: 'string',
            description: `The model that performed the completion.
`
        },
        completion: {
            type: 'string',
            description: `The resulting completion up to and excluding the stop sequences.
`
        }
    },
    required: ['completion', 'stop_reason', 'model']
} as const;

export const CreateCompletionStreamResponseSchema = {
    type: 'object',
    properties: {
        stop_reason: {
            type: 'string',
            enum: ['stop_sequence', 'max_tokens'],
            description: `The reason that we stopped sampling.

This may be one the following values:

"stop_sequence": we reached a stop sequence — either provided by you via the stop_sequences parameter, or a stop sequence built into the model
"max_tokens": we exceeded max_tokens_to_sample or the model's maximum
`
        },
        model: {
            type: 'string',
            description: `The model that performed the completion.
`
        },
        completion: {
            type: 'string',
            description: `The resulting completion up to and excluding the stop sequences.
`
        },
        choices: {
            type: 'array',
            items: {
                type: 'object',
                properties: {
                    delta: {
                        '$ref': '#/components/schemas/CompletionStreamResponseDelta'
                    }
                }
            }
        }
    },
    required: ['completion', 'stop_reason', 'model']
} as const;

export const CompletionStreamResponseDeltaSchema = {
    type: 'object',
    properties: {
        completion: {
            type: 'string',
            description: 'The contents of the chunk message.',
            nullable: true
        },
        stop_reason: {
            type: 'string',
            enum: ['stop_sequence', 'max_tokens'],
            description: `The reason that we stopped sampling.

This may be one the following values:

"stop_sequence": we reached a stop sequence — either provided by you via the stop_sequences parameter, or a stop sequence built into the model
"max_tokens": we exceeded max_tokens_to_sample or the model's maximum
`,
            nullable: true
        },
        model: {
            type: 'string',
            description: `The model that performed the completion.
`
        }
    }
} as const;