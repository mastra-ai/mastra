# Frontend Integration

Integrating Mastra's memory system with frontend applications allows you to build conversational UIs that maintain context across user interactions. This section covers how to connect Mastra memory features with frontend frameworks and the AI SDK.

## Mastra Client and useChat

Mastra integrates smoothly with the AI SDK's `useChat` hook, which provides a simple way to build chat interfaces. However, there are important considerations for ensuring memory works correctly in frontend applications.

### Message Ordering Challenges

When using `useChat`, you must be careful to avoid message ordering issues. By default, `useChat` sends all messages in the conversation history with each request, which can cause:

1. Duplicate messages in memory
2. Incorrect message ordering
3. Inefficient token usage

### Solution: Send Only the Latest Message

To avoid these issues, you should configure `useChat` to send only the latest message:

```typescript
// React component with useChat
const ChatComponent = () => {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    api: "/api/chat",
    experimental_prepareRequestBody({ messages, id }) {
      // This sends only the latest message to the server
      return { message: messages.at(-1), id };
    },
  });

  return (
    <div>
      {/* Chat UI components */}
    </div>
  );
};
```

### Server-Side Implementation

On your server, handle the incoming message and pass it to the agent with the appropriate thread and resource IDs:

```typescript
// API route handler (Next.js example)
export async function POST(request: Request) {
  const { message, id } = await request.json();
  
  // Extract or generate thread and resource IDs
  const threadId = id || uuidv4();
  const resourceId = "user_123"; // Get from session, JWT, etc.
  
  // Stream the response with memory
  const stream = await myAgent.stream(message.content, {
    threadId,
    resourceId,
  });
  
  // Return as streaming response
  return stream.toDataStreamResponse();
}
```

## Framework-Specific Integration

### Next.js Integration

```typescript
// components/Chat.tsx
import { useChat } from "ai/react";

export function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    api: "/api/chat",
    experimental_prepareRequestBody({ messages, id }) {
      return { message: messages.at(-1), id };
    },
  });

  return (
    <div className="chat-container">
      <div className="messages">
        {messages.map((m) => (
          <div key={m.id} className={`message ${m.role}`}>
            {m.content}
          </div>
        ))}
      </div>
      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Say something..."
        />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}
```

```typescript
// app/api/chat/route.ts
import { Mastra } from "@mastra/core";
import { Agent } from "@mastra/core/agent";
import { Memory } from "@mastra/memory";
import { openai } from "@ai-sdk/openai";
import { auth } from "@/lib/auth";

// Configure agent with memory
const memory = new Memory();
const agent = new Agent({
  name: "ChatAgent",
  instructions: "You are a helpful chat assistant.",
  model: openai("gpt-4o"),
  memory,
});

export async function POST(request: Request) {
  // Get user information from session or JWT
  const user = await auth.getUser(request);
  const { message, id } = await request.json();
  
  // Use the message ID as thread ID or create a new one
  const threadId = id || crypto.randomUUID();
  const resourceId = `user_${user.id}`;
  
  // Stream the response with memory
  const stream = await agent.stream(message.content, {
    threadId,
    resourceId,
  });
  
  // Return as streaming response
  return stream.toDataStreamResponse();
}
```

### Svelte Integration

For frameworks that don't support `experimental_prepareRequestBody`, you can extract the latest message on the server side:

```typescript
// Server endpoint (e.g., src/routes/api/chat/+server.js in SvelteKit)
import { agent } from '$lib/mastra';
import { json } from '@sveltejs/kit';

export async function POST({ request }) {
  const { messages } = await request.json();
  
  // Extract the latest message
  const latestMessage = messages.at(-1);
  
  // Get user and thread info
  const threadId = crypto.randomUUID();
  const resourceId = "user_123"; // From auth system
  
  // Stream with memory
  const stream = await agent.stream(latestMessage.content, {
    threadId,
    resourceId,
  });
  
  return new Response(stream.toReadableStream());
}
```

## Working with Working Memory in Frontend

When using working memory, remember to mask the XML tags to prevent them from showing in the UI:

```typescript
import { maskStreamTags } from "@mastra/core/utils";

// In your streaming response handler
const { data } = await streamToResponse(
  maskStreamTags(stream.textStream, "working_memory"),
  {
    onStart() {
      // Show loading indicator
    },
    onToken(token) {
      // Add token to UI
    },
    onFinish() {
      // Hide loading indicator
    },
  }
);
```

## Thread Management UI

You can also build UI components to manage memory threads:

```typescript
// Example thread list component (React)
const ThreadList = () => {
  const [threads, setThreads] = useState([]);
  
  useEffect(() => {
    // Fetch threads for the current user
    async function loadThreads() {
      const response = await fetch("/api/threads");
      const data = await response.json();
      setThreads(data);
    }
    
    loadThreads();
  }, []);
  
  return (
    <div className="thread-list">
      <h2>Your Conversations</h2>
      <ul>
        {threads.map((thread) => (
          <li key={thread.id}>
            <a href={`/chat/${thread.id}`}>{thread.title}</a>
            <span>{new Date(thread.updatedAt).toLocaleString()}</span>
          </li>
        ))}
      </ul>
    </div>
  );
};
```

## Related Features

- **[Memory Threads](../4-memory-threads/index.md)**: Understanding the thread and resource model
- **[Working Memory](./3.3-working-memory.md)**: Using persistent memory with frontend applications
- **[Thread Admin UI](../4-memory-threads/4.4-admin-ui.md)**: Building admin interfaces for thread management 