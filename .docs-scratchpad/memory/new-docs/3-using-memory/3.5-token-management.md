# Token Management

Effective memory usage in Mastra requires careful management of the context window size to avoid token limit errors while still providing sufficient context for the agent. This section covers token optimization strategies and best practices.

## Understanding Token Limits

Each language model has a finite context window size, measured in tokens:

| Model | Context Window | Optimal Memory Usage |
|-------|---------------|---------------------|
| GPT-4o | 128,000 tokens | ~80,000 tokens for memory |
| GPT-4 | 8,192 tokens | ~5,000 tokens for memory |
| GPT-3.5 | 16,384 tokens | ~10,000 tokens for memory |
| Claude 3 Opus | 200,000 tokens | ~140,000 tokens for memory |
| Claude 3 Sonnet | 100,000 tokens | ~70,000 tokens for memory |

When the combined size of:
- System prompt
- Memory context
- Current message
- Function/tool definitions

Exceeds the model's token limit, you'll encounter "context too long" errors.

## Token Efficiency Strategies

Mastra's memory system offers several ways to optimize token usage:

### 1. Limit Last Messages

The most direct way to control token usage is by reducing the number of recent messages retrieved:

```typescript
const memory = new Memory({
  options: {
    lastMessages: 10, // Retrieve fewer messages (default is 40)
  },
});

// Or per-request
await agent.stream("Hello", {
  resourceId: "user_123",
  threadId: "thread_456",
  memoryOptions: {
    lastMessages: 5, // Even fewer for this specific request
  },
});
```

### 2. Optimize Semantic Recall

Configure semantic recall to be more selective:

```typescript
const memory = new Memory({
  options: {
    semanticRecall: {
      topK: 2, // Fewer semantic matches (default is 3)
      messageRange: 1, // Less context around matches (default is 2)
    },
  },
});
```

### 3. Streamline Working Memory

Keep your working memory template focused on essential information:

```typescript
const memory = new Memory({
  options: {
    workingMemory: {
      enabled: true,
      template: `<user>
  <name></name>
  <preferences></preferences>
</user>`, // Minimal template
    },
  },
});
```

### 4. Use Tool Call Filters

When using models that support tool calling, you can reduce tool descriptions in the context:

```typescript
const result = await agent.generate("Hello", {
  resourceId: "user_123",
  threadId: "thread_456",
  toolChoice: "none", // Don't include tools in context for this simple query
});
```

## Dynamic Token Management

For sophisticated applications, you can implement dynamic token management:

```typescript
async function getMemoryOptions(message, threadId) {
  // Estimate token count based on message content
  const estimatedTokens = message.length / 4; // Rough approximation
  
  // Adjust memory settings based on complexity
  if (estimatedTokens > 500) {
    // Complex query - use more focused memory retrieval
    return {
      lastMessages: 5,
      semanticRecall: {
        topK: 3,
        messageRange: 1,
      },
    };
  } else {
    // Simple query - use more extensive context
    return {
      lastMessages: 15,
      semanticRecall: {
        topK: 2,
        messageRange: 2,
      },
    };
  }
}

// Use dynamic options
const memoryOptions = await getMemoryOptions(userMessage, threadId);
await agent.stream(userMessage, {
  resourceId,
  threadId,
  memoryOptions,
});
```

## Model-Specific Considerations

Different models have different token efficiency characteristics:

### GPT Models

- System instructions consume tokens from the context window
- Function/tool definitions can be verbose and consume many tokens
- Consider using gpt-4-turbo for complex memory needs due to its larger context window

### Claude Models

- Claude models have larger context windows
- Function/tool definitions are generally more token-efficient
- Can handle more extensive memory retrieval

### Mistral and Other Models

- Adjust memory strategies based on context window size
- Test token usage with your specific model
- Consider model-specific compression techniques

## Monitoring Token Usage

To optimize token usage, monitor how many tokens are being consumed:

```typescript
import { countTokens } from "@mastra/core/utils";

// Monitor token usage for memory
const { messages } = await memory.query({
  threadId: "thread_123",
  selectBy: {
    last: 20,
    vectorSearchString: "project requirements",
  },
  debug: true, // Enable debug output
});

// Estimate token usage
const tokenCount = countTokens(JSON.stringify(messages));
console.log(`Memory context using approximately ${tokenCount} tokens`);
```

## Balancing Context and Efficiency

Finding the right balance between context richness and token efficiency is key:

| Approach | Pros | Cons |
|----------|------|------|
| **Minimal Context** (few messages) | Lower token usage, faster responses | Limited context, may miss important information |
| **Full Context** (many messages) | Rich understanding, better responses | Higher token usage, potential context limits |
| **Semantic Recall** (relevant messages) | Focused context, topic-relevant | Requires vector DB, may miss temporal context |
| **Working Memory** (persistent facts) | Compact, essential information | Requires maintenance, may lack specific details |

## Best Practices

1. **Start conservative**: Begin with fewer messages and increase as needed
2. **Combine strategies**: Use last messages + semantic recall + working memory together
3. **Adapt to content**: Use more context for complex discussions, less for simple queries
4. **Monitor and adjust**: Observe token usage and adjust settings based on real usage
5. **Consider model selection**: Choose models with appropriate context windows for your use case

## Related Features

- **[Last Messages](./3.1-last-messages.md)**: Managing recent conversation context
- **[Semantic Recall](./3.2-semantic-recall.md)**: Optimizing relevant message retrieval
- **[Working Memory](./3.3-working-memory.md)**: Efficient persistent information storage 