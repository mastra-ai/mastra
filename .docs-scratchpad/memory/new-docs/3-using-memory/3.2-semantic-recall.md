# Semantic Recall

While last messages provide immediate conversation context, semantic recall enables agents to find and retrieve relevant information from anywhere in the conversation history, regardless of when it was mentioned.

## What is Semantic Recall?

Semantic recall is:
- A vector-based search capability that finds conceptually similar messages
- A way to surface relevant context from earlier in conversations
- A solution for maintaining context in long-running conversations

Unlike simple retrieval of recent messages, semantic recall:
- Searches based on meaning rather than just recency
- Can identify relevant messages from hours, days, or weeks ago
- Helps agents maintain coherence even in lengthy or complex discussions

## How Semantic Recall Works

Semantic recall operates through these steps:

1. **Embedding Generation**: Each message is converted into a vector representation (embedding)
2. **Vector Storage**: These embeddings are stored in a vector database
3. **Similarity Search**: When a new message arrives, its embedding is compared to past messages
4. **Context Retrieval**: The most semantically similar messages are retrieved
5. **Context Window Inclusion**: These messages (and surrounding context) are included for the agent

```
┌─────────┐         ┌──────────┐         ┌──────────────┐         ┌───────────────┐
│ Messages │──────► │ Embedder │──────► │ Vector Store │ ◄─────► │ Context Window│
└─────────┘         └──────────┘         └──────────────┘         └───────────────┘
```

## Configuring Semantic Recall

Semantic recall is enabled by default when you configure memory with a vector database. You can customize its behavior:

```typescript
// Basic configuration
const memory = new Memory({
  options: {
    semanticRecall: {
      topK: 3,           // Number of similar messages to retrieve
      messageRange: 2,   // Context around each match (messages before/after)
    },
  },
});

// Advanced configuration with asymmetric context
const memory = new Memory({
  options: {
    semanticRecall: {
      topK: 5,           // Retrieve 5 matches
      messageRange: {    // Asymmetric context
        before: 1,       // 1 message before each match
        after: 3,        // 3 messages after each match
      },
    },
  },
});
```

### Per-Request Configuration

You can also override semantic recall settings for specific requests:

```typescript
await agent.stream("Tell me about the feature we discussed last week", {
  resourceId: "user_123",
  threadId: "thread_456",
  memoryOptions: {
    semanticRecall: {
      topK: 5,         // Find more matches for this specific query
      messageRange: 3, // More context around matches
    },
  },
});
```

### Disabling Semantic Recall

```typescript
// Disable globally
const memory = new Memory({
  options: {
    semanticRecall: false,
  },
});

// Disable for a specific request
await agent.stream("Just focus on what I'm asking now", {
  resourceId: "user_123",
  threadId: "thread_456",
  memoryOptions: {
    semanticRecall: false,
  },
});
```

## Embedding Performance

The performance of semantic recall depends significantly on the embedding model used. By default, Mastra uses FastEmbed with the "bge-small-en-v1.5" model, which offers a good balance of quality and speed.

### Embedding Model Considerations

Different embedding models have different characteristics:

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| FastEmbed (default) | ~130MB | Fast | Good | Most use cases, local development |
| OpenAI Embeddings | Cloud | Medium | Excellent | Production, multilingual content |
| Azure Embeddings | Cloud | Medium | Excellent | Enterprise, compliance requirements |

### Configuring Custom Embedders

```typescript
import { Memory } from "@mastra/memory";
import { openai } from "@ai-sdk/openai";

// Using OpenAI embeddings
const memory = new Memory({
  embedder: openai.embedding("text-embedding-3-small"),
});
```

### Embedding Performance Optimization

To optimize embedding performance:
- Cache embeddings to avoid recomputing them
- Batch embedding operations when possible
- Consider the trade-off between embedding quality and computational cost
- Use a vector database with efficient similarity search algorithms
- For high-volume applications, consider dedicated vector search services

## When to Use Semantic Recall

Semantic recall is most valuable for:
- Long-running conversations with many messages
- Support scenarios where users reference past discussions
- Knowledge-intensive applications where context might span multiple sessions
- Situations where specific details mentioned earlier are important

## Semantic Recall Example

```typescript
import { Agent } from "@mastra/core/agent";
import { Memory } from "@mastra/memory";
import { openai } from "@ai-sdk/openai";
import { PgVector, PostgresStore } from "@mastra/pg";

// Create memory with semantic recall
const memory = new Memory({
  storage: new PostgresStore({
    connectionString: process.env.DATABASE_URL!,
  }),
  vector: new PgVector(process.env.DATABASE_URL!),
  options: {
    semanticRecall: {
      topK: 3,
      messageRange: 2,
    },
  },
});

// Create agent with memory
const agent = new Agent({
  name: "ProjectManager",
  instructions: "You are a project management assistant.",
  model: openai("gpt-4o"),
  memory: memory,
});

// Initial conversation about project requirements
await agent.stream("Let's set up our new website project. We need React, Next.js, and TailwindCSS.", {
  resourceId: "user_123",
  threadId: "project_456",
});

// Much later in the conversation
await agent.stream("What tech stack did we decide to use for the project?", {
  resourceId: "user_123",
  threadId: "project_456",
});
// The agent will find and recall the message about React, Next.js and TailwindCSS
// even if many messages have occurred since then
```

## Related Features

- **[Last Messages](./3.1-last-messages.md)**: For recent conversation history
- **[Working Memory](./3.3-working-memory.md)**: For storing persistent information
- **[Token Management](./3.5-token-management.md)**: For optimizing token usage with memory 