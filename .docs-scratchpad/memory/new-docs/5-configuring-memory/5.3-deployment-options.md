# Deployment Options for Mastra Memory

When deploying applications that use Mastra's memory system, you need to consider both where your application will run and how memory storage will be configured. This guide covers deployment options for Mastra Memory on different platforms and best practices for each environment.

## Mastra Cloud

Mastra Cloud provides the simplest deployment option for memory-enabled applications, with built-in support for storage, thread management, and scaling.

### Benefits of Mastra Cloud for Memory

- **Zero configuration**: Memory storage is pre-configured and managed for you
- **Automatic scaling**: Thread storage scales with your application usage
- **Data durability**: Built-in backups and redundancy for memory data
- **Analytics**: Memory usage insights and conversation analytics
- **Thread management**: Admin UI for viewing and managing memory threads

### Using Memory with Mastra Cloud

```typescript
import { Memory } from "@mastra/memory";
import { Agent } from "@mastra/core/agent";

// When deploying to Mastra Cloud, you can use the default memory config
// All storage is automatically configured
const agent = new Agent({
  name: "SupportAgent",
  instructions: "You are a helpful support agent.",
  memory: new Memory(), // Uses Mastra Cloud's managed storage
});

export const mastra = new Mastra({
  agents: { supportAgent: agent }
});
```

### Mastra Cloud Memory Considerations

1. **Environment variables**: No additional environment variables needed for memory
2. **Storage limits**: Check your plan for thread storage limits
3. **Data residency**: Memory data is stored in the same region as your deployment

## Vercel Deployment

Deploying memory-enabled applications to Vercel requires configuring external storage services, as Vercel's serverless functions are ephemeral.

### Database Options for Vercel

Choose one of these storage options for memory:

1. **Vercel KV (Upstash Redis)**: Good for small to medium applications
2. **Vercel Postgres**: Best for applications with complex querying needs
3. **External databases**: Any PostgreSQL-compatible database (Neon, Supabase, etc.)

### Using Upstash with Vercel

```typescript
import { Memory } from "@mastra/memory";
import { UpstashStore, UpstashVector } from "@mastra/upstash";
import { openai } from "@ai-sdk/openai";

// Configure memory with Upstash for both storage and vector search
const memory = new Memory({
  storage: new UpstashStore({
    url: process.env.UPSTASH_URL!,
    token: process.env.UPSTASH_TOKEN!,
  }),
  vector: new UpstashVector({
    url: process.env.UPSTASH_URL!,
    token: process.env.UPSTASH_TOKEN!,
  }),
  embedder: openai.embedding("text-embedding-3-small"), // Cloud-based embedder
});

// Add memory to an agent
const agent = new Agent({
  name: "SupportAgent",
  memory,
  // Other agent options
});
```

### Using Vercel Postgres

```typescript
import { Memory } from "@mastra/memory";
import { PostgresStore, PgVector } from "@mastra/pg";
import { openai } from "@ai-sdk/openai";

// Configure memory with Postgres for both storage and vector search
const memory = new Memory({
  storage: new PostgresStore({
    connectionString: process.env.POSTGRES_URL!,
  }),
  vector: new PgVector({
    connectionString: process.env.POSTGRES_URL!,
  }),
  embedder: openai.embedding("text-embedding-3-small"),
});
```

### Vercel Memory Deployment Checklist

1. **Environment variables**: Configure database connection strings in Vercel
2. **Connection pooling**: Set appropriate connection limits for serverless
3. **Cold starts**: Be aware of potential latency on first access
4. **External embedder**: Always use a cloud-based embedder (not FastEmbed)
5. **Edge runtime**: If using Edge Runtime, ensure your database is compatible

## Cloudflare Deployment

Deploying memory to Cloudflare Workers presents unique challenges due to the restricted runtime environment. Here's how to configure memory for Cloudflare:

### Cloudflare Storage Options

1. **Cloudflare D1**: SQLite-compatible database directly integrated with Workers
2. **Upstash**: Redis-compatible external database with REST API
3. **External PostgreSQL**: Using connection pools designed for Workers

### Using D1 with Cloudflare Workers

```typescript
import { Memory } from "@mastra/memory";
import { D1Store } from "@mastra/d1"; // Coming soon
import { openai } from "@ai-sdk/openai";

// Configure memory with D1 for storage
const memory = new Memory({
  storage: new D1Store({ 
    binding: env.DB // Your D1 binding from worker environment
  }),
  // Since D1 doesn't support vector search yet, disable semantic recall
  options: {
    semanticRecall: false
  },
  // Alternatively, use an external vector store like Pinecone or Upstash
  // vector: new PineconeVector({ /* config */ }),
  embedder: openai.embedding("text-embedding-3-small"),
});
```

### Using Upstash with Cloudflare

```typescript
import { Memory } from "@mastra/memory";
import { UpstashStore, UpstashVector } from "@mastra/upstash";
import { openai } from "@ai-sdk/openai";

const memory = new Memory({
  storage: new UpstashStore({
    url: env.UPSTASH_URL,
    token: env.UPSTASH_TOKEN,
  }),
  vector: new UpstashVector({
    url: env.UPSTASH_URL,
    token: env.UPSTASH_TOKEN,
  }),
  embedder: openai.embedding("text-embedding-3-small"),
});

// Add memory to an agent
const agent = new Agent({
  name: "SupportAgent",
  memory,
  // Other agent options
});
```

### Cloudflare Memory Deployment Considerations

1. **Avoid default storage**: Never use the default memory storage on Cloudflare
2. **External embedder required**: Always use a cloud-based embedder model
3. **Avoid file paths**: Cloudflare Workers cannot access the filesystem
4. **REST APIs preferred**: Use services with REST APIs over direct connections
5. **CPU/memory limits**: Be mindful of Workers resource limits with large memory operations

## General Deployment Best Practices

Regardless of your deployment platform, follow these best practices:

1. **Database connection pooling**: Configure connection pools appropriate for your deployment environment
2. **Environment variables**: Store all database credentials as environment variables
3. **Error handling**: Implement robust error handling for database connection failures
4. **Monitoring**: Set up monitoring for memory storage and query performance
5. **Backups**: Ensure your memory database is backed up regularly
6. **Scaling**: Plan for increased thread and message volume as your application grows

## Troubleshooting Deployment Issues

### Common Issues and Solutions

#### File Path Errors on Serverless

**Issue**: Errors related to `__dirname` or file paths when using the default embedder

**Solution**: Use a cloud-based embedder like OpenAI or disable semantic recall:

```typescript
// Option 1: Use a cloud embedder
const memory = new Memory({
  embedder: openai.embedding("text-embedding-3-small"),
});

// Option 2: Disable semantic recall
const memory = new Memory({
  options: {
    semanticRecall: false
  }
});
```

#### Database Connection Issues

**Issue**: "Too many connections" errors in serverless environments

**Solution**: Configure appropriate connection pooling:

```typescript
const memory = new Memory({
  storage: new PostgresStore({
    connectionString: process.env.DATABASE_URL!,
    pool: {
      max: 10, // Adjust based on your serverless concurrency limits
    },
  }),
});
```

#### Memory Usage Growth

**Issue**: Rapidly growing memory storage size

**Solution**: Implement a message retention policy:

```typescript
const memory = new Memory({
  options: {
    lastMessages: 50, // Only keep the latest 50 messages per thread
  },
});
```

## Related Resources

- [Memory Configuration Guide](./5.1-database-adapters.md)
- [Memory Performance Optimization](./5.2-defaults-and-settings.md)
- [Serverless Deployment Guide](/docs/deployment/deployment) 